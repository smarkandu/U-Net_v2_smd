wandb: Currently logged in as: ianben. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /afs/crc.nd.edu/user/y/ypeng4/.netrc
wandb: WARNING Path /afs/crc.nd.edu/user/y/ypeng4/data/trained_models/Dataset122_ISIC2017/nnUNetTrainer__nnUNetPlans__2d/808641_FusedMBConv_8/fold_0/wandb/ wasn't writable, using system temp directory.
wandb: WARNING Path /afs/crc.nd.edu/user/y/ypeng4/data/trained_models/Dataset122_ISIC2017/nnUNetTrainer__nnUNetPlans__2d/808641_FusedMBConv_8/fold_0/wandb/ wasn't writable, using system temp directory
wandb: wandb version 0.15.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.1
wandb: Run data is saved locally in /tmp/808641.1.gpu/wandb/run-20230819_162054-ff348brl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run nnunet_808641_0_lr_0.005
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ianben/isic2017_2_0
wandb: üöÄ View run at https://wandb.ai/ianben/isic2017_2_0/runs/ff348brl
OrderedDict([('self', <Parameter "self">), ('plans', <Parameter "plans: dict">), ('configuration', <Parameter "configuration: str">), ('fold', <Parameter "fold: int">), ('dataset_json', <Parameter "dataset_json: dict">), ('unpack_dataset', <Parameter "unpack_dataset: bool = True">), ('device', <Parameter "device: torch.device = device(type='cuda')">), ('debug', <Parameter "debug=True">), ('job_id', <Parameter "job_id=None">)])
Using device: cuda:0
==========================initial_lr: 0.005===========================
===================debug: False===================
2023-08-19 16:20:57.489456: I am training on qa-p100-007.crc.nd.edu
2023-08-19 16:20:57.491529: output folder: /afs/crc.nd.edu/user/y/ypeng4/data/trained_models/Dataset122_ISIC2017/nnUNetTrainer__nnUNetPlans__2d/808641_FusedMBConv_8/fold_0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

model: UNet(
  (inc): inconv(
    (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (conv2): FusedMBConv(
      (conv3x3): ConvNormAct(
        (conv): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU()
      )
      (se_block): SEBlock(
        (squeeze): AdaptiveAvgPool2d(output_size=1)
        (excitation): Sequential(
          (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU()
          (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
          (3): Sigmoid()
        )
      )
      (pointwise): ConvNormAct(
        (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): Identity()
      )
      (drop_path): DropPath()
      (shortcut): Sequential()
    )
  )
  (down1): down_block(
    (conv): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (2): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (down2): down_block(
    (conv): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (2): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (down3): down_block(
    (conv): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (2): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (down4): down_block(
    (conv): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (2): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (down5): down_block(
    (conv): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (2): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (down6): down_block(
    (conv): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
      (2): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (up0_0): up_block(
    (conv_ch): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (conv): Sequential(
      (0): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(512, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (up0): up_block(
    (conv_ch): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (conv): Sequential(
      (0): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (up1): up_block(
    (conv_ch): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
    (conv): Sequential(
      (0): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (up2): up_block(
    (conv_ch): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
    (conv): Sequential(
      (0): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (up3): up_block(
    (conv_ch): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
    (conv): Sequential(
      (0): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (up4): up_block(
    (conv_ch): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    (conv): Sequential(
      (0): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential(
          (0): ConvNormAct(
            (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (norm): Identity()
            (act): Identity()
          )
        )
      )
      (1): FusedMBConv(
        (conv3x3): ConvNormAct(
          (conv): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (norm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (se_block): SEBlock(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
            (3): Sigmoid()
          )
        )
        (pointwise): ConvNormAct(
          (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): Identity()
        )
        (drop_path): DropPath()
        (shortcut): Sequential()
      )
    )
  )
  (outc): Conv2d(8, 2, kernel_size=(1, 1), stride=(1, 1))
  (seg_layers): ModuleList(
    (0): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
    (1): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))
    (2): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))
    (3): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))
    (4): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))
    (5): Conv2d(8, 2, kernel_size=(1, 1), stride=(1, 1))
  )
)
================================================UNet================================================

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 49, 'patch_size': [256, 256], 'median_image_size_in_voxels': [256.0, 256.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False, False], 'UNet_class_name': 'ResidualEncoderUNet', 'nnUNet_UNet': False, 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [6, 6], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset122_ISIC2017', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 256, 256], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 160.1475372314453, 'median': 164.0, 'min': 0.0, 'percentile_00_5': 34.0, 'percentile_99_5': 252.0, 'std': 41.12111282348633}, '1': {'max': 255.0, 'mean': 111.18875122070312, 'median': 113.0, 'min': 0.0, 'percentile_00_5': 10.0, 'percentile_99_5': 222.0, 'std': 42.475669860839844}, '2': {'max': 255.0, 'mean': 91.16386413574219, 'median': 90.0, 'min': 0.0, 'percentile_00_5': 5.0, 'percentile_99_5': 207.0, 'std': 42.03706359863281}}} 

2023-08-19 16:20:59.638144: unpacking dataset...
2023-08-19 16:21:06.088280: unpacking done...
2023-08-19 16:21:06.090463: do_dummy_2d_data_aug: False
2023-08-19 16:21:06.104322: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 16:21:06.105826: The split file contains 1 splits.
2023-08-19 16:21:06.106778: Desired fold for training: 0
2023-08-19 16:21:06.107548: This split has 1500 training and 650 validation cases.
==================batch size: 24==================
2023-08-19 16:21:07.058933: Unable to plot network architecture:
2023-08-19 16:21:07.061016: module 'torch.onnx' has no attribute '_optimize_trace'
2023-08-19 16:21:07.156599: 
2023-08-19 16:21:07.157853: Epoch 0
2023-08-19 16:21:07.158859: Current learning rate: 0.005
2023-08-19 16:21:07.159695: start training, 250
================num of epochs: 250================
using pin_memory on device 0
2023-08-19 16:28:03.124049: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 16:28:03.397122: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 16:28:03.399853: The split file contains 1 splits.
2023-08-19 16:28:03.400855: Desired fold for training: 0
2023-08-19 16:28:03.401765: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 16:31:36.052150: dsc: 51.22%
2023-08-19 16:31:36.053625: miou: 34.43%
2023-08-19 16:31:36.054506: acc: 88.87%, sen: 34.89%, spe: 99.73%
2023-08-19 16:31:36.056439: current best miou: 0.34425665415120676 at epoch: 0, (0, 0.34425665415120676, 0.5121888786462105)
2023-08-19 16:31:36.057384: current best dsc: 0.5121888786462105 at epoch: 0, (0, 0.34425665415120676, 0.5121888786462105)
2023-08-19 16:31:38.036358: finished real validation
using pin_memory on device 0
2023-08-19 16:32:04.545749: train_loss 10.7221
2023-08-19 16:32:04.547311: val_loss 0.7739
2023-08-19 16:32:04.549250: Pseudo dice [0.5427]
2023-08-19 16:32:04.550543: Epoch time: 657.39 s
2023-08-19 16:32:04.551511: Yayy! New best EMA pseudo Dice: 0.5427
2023-08-19 16:32:07.904942: 
2023-08-19 16:32:07.906249: Epoch 1
2023-08-19 16:32:07.907239: Current learning rate: 0.00498
2023-08-19 16:32:07.908402: start training, 250
================num of epochs: 250================
2023-08-19 16:39:03.786670: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 16:39:04.091151: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 16:39:04.094724: The split file contains 1 splits.
2023-08-19 16:39:04.101106: Desired fold for training: 0
2023-08-19 16:39:04.105151: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 16:42:42.546240: dsc: 81.12%
2023-08-19 16:42:42.548238: miou: 68.23%
2023-08-19 16:42:42.549758: acc: 93.92%, sen: 78.00%, spe: 97.12%
2023-08-19 16:42:42.552408: current best miou: 0.6823299223022486 at epoch: 1, (1, 0.6823299223022486, 0.8111725450005528)
2023-08-19 16:42:42.553795: current best dsc: 0.8111725450005528 at epoch: 1, (1, 0.6823299223022486, 0.8111725450005528)
2023-08-19 16:42:45.095012: finished real validation
2023-08-19 16:43:11.444824: train_loss -0.341
2023-08-19 16:43:11.446212: val_loss -0.396
2023-08-19 16:43:11.447819: Pseudo dice [0.7938]
2023-08-19 16:43:11.449170: Epoch time: 663.54 s
2023-08-19 16:43:11.450534: Yayy! New best EMA pseudo Dice: 0.5678
2023-08-19 16:43:15.100736: 
2023-08-19 16:43:15.102873: Epoch 2
2023-08-19 16:43:15.104071: Current learning rate: 0.00497
2023-08-19 16:43:15.105540: start training, 250
================num of epochs: 250================
2023-08-19 16:50:13.521475: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 16:50:13.824465: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 16:50:13.827270: The split file contains 1 splits.
2023-08-19 16:50:13.828595: Desired fold for training: 0
2023-08-19 16:50:13.829550: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 16:53:49.128362: dsc: 79.94%
2023-08-19 16:53:49.130262: miou: 66.58%
2023-08-19 16:53:49.131126: acc: 94.12%, sen: 69.99%, spe: 98.97%
2023-08-19 16:53:49.132863: current best miou: 0.6823299223022486 at epoch: 1, (1, 0.6823299223022486, 0.8111725450005528)
2023-08-19 16:53:49.134084: current best dsc: 0.8111725450005528 at epoch: 1, (1, 0.6823299223022486, 0.8111725450005528)
2023-08-19 16:53:49.134983: finished real validation
2023-08-19 16:54:15.320607: train_loss -0.615
2023-08-19 16:54:15.322377: val_loss -0.5546
2023-08-19 16:54:15.323781: Pseudo dice [0.7882]
2023-08-19 16:54:15.324829: Epoch time: 660.22 s
2023-08-19 16:54:15.326069: Yayy! New best EMA pseudo Dice: 0.5898
2023-08-19 16:54:18.666100: 
2023-08-19 16:54:18.667884: Epoch 3
2023-08-19 16:54:18.668977: Current learning rate: 0.00495
2023-08-19 16:54:18.670190: start training, 250
================num of epochs: 250================
2023-08-19 17:01:17.029629: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 17:01:17.338006: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 17:01:17.341174: The split file contains 1 splits.
2023-08-19 17:01:17.342237: Desired fold for training: 0
2023-08-19 17:01:17.343246: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 17:04:52.417594: dsc: 85.20%
2023-08-19 17:04:52.419433: miou: 74.21%
2023-08-19 17:04:52.420632: acc: 95.33%, sen: 80.24%, spe: 98.37%
2023-08-19 17:04:52.422508: current best miou: 0.7421053306995192 at epoch: 3, (3, 0.7421053306995192, 0.8519637907330628)
2023-08-19 17:04:52.423448: current best dsc: 0.8519637907330628 at epoch: 3, (3, 0.7421053306995192, 0.8519637907330628)
2023-08-19 17:04:54.352326: finished real validation
2023-08-19 17:05:20.359490: train_loss -0.7105
2023-08-19 17:05:20.361303: val_loss -0.6774
2023-08-19 17:05:20.362896: Pseudo dice [0.8559]
2023-08-19 17:05:20.363982: Epoch time: 661.7 s
2023-08-19 17:05:20.364776: Yayy! New best EMA pseudo Dice: 0.6164
2023-08-19 17:05:23.654239: 
2023-08-19 17:05:23.655850: Epoch 4
2023-08-19 17:05:23.657003: Current learning rate: 0.00494
2023-08-19 17:05:23.658336: start training, 250
================num of epochs: 250================
2023-08-19 17:12:21.096877: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 17:12:21.401885: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 17:12:21.404949: The split file contains 1 splits.
2023-08-19 17:12:21.405977: Desired fold for training: 0
2023-08-19 17:12:21.406783: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 17:15:55.822071: dsc: 85.46%
2023-08-19 17:15:55.823900: miou: 74.62%
2023-08-19 17:15:55.824802: acc: 95.47%, sen: 79.58%, spe: 98.66%
2023-08-19 17:15:55.827148: current best miou: 0.7461622496491543 at epoch: 4, (4, 0.7461622496491543, 0.8546310628340249)
2023-08-19 17:15:55.828078: current best dsc: 0.8546310628340249 at epoch: 4, (4, 0.7461622496491543, 0.8546310628340249)
2023-08-19 17:15:57.763212: finished real validation
2023-08-19 17:16:23.647396: train_loss -0.737
2023-08-19 17:16:23.649364: val_loss -0.6713
2023-08-19 17:16:23.650968: Pseudo dice [0.8533]
2023-08-19 17:16:23.652085: Epoch time: 660.0 s
2023-08-19 17:16:23.652927: Yayy! New best EMA pseudo Dice: 0.6401
2023-08-19 17:16:26.872859: 
2023-08-19 17:16:26.874633: Epoch 5
2023-08-19 17:16:26.875752: Current learning rate: 0.00492
2023-08-19 17:16:26.877134: start training, 250
================num of epochs: 250================
2023-08-19 17:23:22.868529: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 17:23:23.191440: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 17:23:23.194378: The split file contains 1 splits.
2023-08-19 17:23:23.195497: Desired fold for training: 0
2023-08-19 17:23:23.196479: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 17:26:58.866048: dsc: 84.40%
2023-08-19 17:26:58.868011: miou: 73.01%
2023-08-19 17:26:58.868997: acc: 95.07%, sen: 79.70%, spe: 98.16%
2023-08-19 17:26:58.871047: current best miou: 0.7461622496491543 at epoch: 4, (4, 0.7461622496491543, 0.8546310628340249)
2023-08-19 17:26:58.871976: current best dsc: 0.8546310628340249 at epoch: 4, (4, 0.7461622496491543, 0.8546310628340249)
2023-08-19 17:26:58.873003: finished real validation
2023-08-19 17:27:24.714574: train_loss -0.7641
2023-08-19 17:27:24.716519: val_loss -0.661
2023-08-19 17:27:24.718553: Pseudo dice [0.8499]
2023-08-19 17:27:24.719670: Epoch time: 657.84 s
2023-08-19 17:27:24.720640: Yayy! New best EMA pseudo Dice: 0.6611
2023-08-19 17:27:28.101679: 
2023-08-19 17:27:28.102926: Epoch 6
2023-08-19 17:27:28.104044: Current learning rate: 0.00491
2023-08-19 17:27:28.105526: start training, 250
================num of epochs: 250================
2023-08-19 17:34:24.534940: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 17:34:24.846223: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 17:34:24.849029: The split file contains 1 splits.
2023-08-19 17:34:24.850267: Desired fold for training: 0
2023-08-19 17:34:24.851428: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 17:38:00.830560: dsc: 85.46%
2023-08-19 17:38:00.832571: miou: 74.61%
2023-08-19 17:38:00.833640: acc: 95.44%, sen: 79.97%, spe: 98.56%
2023-08-19 17:38:00.835440: current best miou: 0.7461622496491543 at epoch: 4, (4, 0.7461622496491543, 0.8546310628340249)
2023-08-19 17:38:00.836487: current best dsc: 0.8546310628340249 at epoch: 4, (4, 0.7461622496491543, 0.8546310628340249)
2023-08-19 17:38:00.837385: finished real validation
2023-08-19 17:38:26.681548: train_loss -0.7838
2023-08-19 17:38:26.683448: val_loss -0.666
2023-08-19 17:38:26.685208: Pseudo dice [0.8469]
2023-08-19 17:38:26.686500: Epoch time: 658.58 s
2023-08-19 17:38:26.687508: Yayy! New best EMA pseudo Dice: 0.6797
2023-08-19 17:38:30.070094: 
2023-08-19 17:38:30.072155: Epoch 7
2023-08-19 17:38:30.073535: Current learning rate: 0.00489
2023-08-19 17:38:30.075126: start training, 250
================num of epochs: 250================
2023-08-19 17:45:26.508170: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 17:45:26.831631: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 17:45:26.834494: The split file contains 1 splits.
2023-08-19 17:45:26.835526: Desired fold for training: 0
2023-08-19 17:45:26.836524: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 17:49:03.563001: dsc: 85.83%
2023-08-19 17:49:03.564894: miou: 75.18%
2023-08-19 17:49:03.566061: acc: 95.52%, sen: 80.92%, spe: 98.46%
2023-08-19 17:49:03.567900: current best miou: 0.7517796734746917 at epoch: 7, (7, 0.7517796734746917, 0.8583039121392714)
2023-08-19 17:49:03.569073: current best dsc: 0.8583039121392714 at epoch: 7, (7, 0.7517796734746917, 0.8583039121392714)
2023-08-19 17:49:05.584223: finished real validation
2023-08-19 17:49:31.609313: train_loss -0.7847
2023-08-19 17:49:31.611213: val_loss -0.6864
2023-08-19 17:49:31.613031: Pseudo dice [0.8638]
2023-08-19 17:49:31.614113: Epoch time: 661.54 s
2023-08-19 17:49:31.615188: Yayy! New best EMA pseudo Dice: 0.6981
2023-08-19 17:49:34.899736: 
2023-08-19 17:49:34.902307: Epoch 8
2023-08-19 17:49:34.904612: Current learning rate: 0.00488
2023-08-19 17:49:34.907361: start training, 250
================num of epochs: 250================
2023-08-19 17:56:32.240385: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 17:56:32.564651: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 17:56:32.567496: The split file contains 1 splits.
2023-08-19 17:56:32.568522: Desired fold for training: 0
2023-08-19 17:56:32.569582: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 18:00:07.622908: dsc: 85.75%
2023-08-19 18:00:07.624831: miou: 75.05%
2023-08-19 18:00:07.625829: acc: 95.51%, sen: 80.64%, spe: 98.50%
2023-08-19 18:00:07.627626: current best miou: 0.7517796734746917 at epoch: 7, (7, 0.7517796734746917, 0.8583039121392714)
2023-08-19 18:00:07.628611: current best dsc: 0.8583039121392714 at epoch: 7, (7, 0.7517796734746917, 0.8583039121392714)
2023-08-19 18:00:07.629512: finished real validation
2023-08-19 18:00:33.593279: train_loss -0.7837
2023-08-19 18:00:33.595249: val_loss -0.6777
2023-08-19 18:00:33.597067: Pseudo dice [0.8594]
2023-08-19 18:00:33.598132: Epoch time: 658.7 s
2023-08-19 18:00:33.599140: Yayy! New best EMA pseudo Dice: 0.7142
2023-08-19 18:00:36.983775: 
2023-08-19 18:00:36.985413: Epoch 9
2023-08-19 18:00:36.986451: Current learning rate: 0.00486
2023-08-19 18:00:36.987743: start training, 250
================num of epochs: 250================
2023-08-19 18:07:34.027090: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 18:07:34.346420: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 18:07:34.349200: The split file contains 1 splits.
2023-08-19 18:07:34.350458: Desired fold for training: 0
2023-08-19 18:07:34.351667: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 18:11:11.826209: dsc: 86.44%
2023-08-19 18:11:11.828062: miou: 76.12%
2023-08-19 18:11:11.829083: acc: 95.63%, sen: 83.17%, spe: 98.14%
2023-08-19 18:11:11.830916: current best miou: 0.761246128377828 at epoch: 9, (9, 0.761246128377828, 0.8644403710672324)
2023-08-19 18:11:11.831969: current best dsc: 0.8644403710672324 at epoch: 9, (9, 0.761246128377828, 0.8644403710672324)
2023-08-19 18:11:13.792487: finished real validation
2023-08-19 18:11:39.881548: train_loss -0.7994
2023-08-19 18:11:39.883255: val_loss -0.692
2023-08-19 18:11:39.884902: Pseudo dice [0.8664]
2023-08-19 18:11:39.886207: Epoch time: 662.9 s
2023-08-19 18:11:39.887141: Yayy! New best EMA pseudo Dice: 0.7294
2023-08-19 18:11:43.128675: 
2023-08-19 18:11:43.130247: Epoch 10
2023-08-19 18:11:43.131303: Current learning rate: 0.00485
2023-08-19 18:11:43.132648: start training, 250
================num of epochs: 250================
2023-08-19 18:18:39.789619: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 18:18:40.115393: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 18:18:40.118510: The split file contains 1 splits.
2023-08-19 18:18:40.119667: Desired fold for training: 0
2023-08-19 18:18:40.120768: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 18:22:17.734448: dsc: 86.37%
2023-08-19 18:22:17.736311: miou: 76.01%
2023-08-19 18:22:17.737382: acc: 95.54%, sen: 84.29%, spe: 97.81%
2023-08-19 18:22:17.739222: current best miou: 0.761246128377828 at epoch: 9, (9, 0.761246128377828, 0.8644403710672324)
2023-08-19 18:22:17.740256: current best dsc: 0.8644403710672324 at epoch: 9, (9, 0.761246128377828, 0.8644403710672324)
2023-08-19 18:22:17.741205: finished real validation
2023-08-19 18:22:43.650178: train_loss -0.8046
2023-08-19 18:22:43.652256: val_loss -0.7029
2023-08-19 18:22:43.654115: Pseudo dice [0.8633]
2023-08-19 18:22:43.655415: Epoch time: 660.52 s
2023-08-19 18:22:43.656423: Yayy! New best EMA pseudo Dice: 0.7428
2023-08-19 18:22:46.943407: 
2023-08-19 18:22:46.944844: Epoch 11
2023-08-19 18:22:46.945944: Current learning rate: 0.00483
2023-08-19 18:22:46.947378: start training, 250
================num of epochs: 250================
2023-08-19 18:29:43.667927: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 18:29:43.986961: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 18:29:43.989814: The split file contains 1 splits.
2023-08-19 18:29:43.990959: Desired fold for training: 0
2023-08-19 18:29:43.991886: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 18:33:21.381419: dsc: 86.78%
2023-08-19 18:33:21.383412: miou: 76.65%
2023-08-19 18:33:21.384381: acc: 95.76%, sen: 83.06%, spe: 98.32%
2023-08-19 18:33:21.386424: current best miou: 0.7664683862750687 at epoch: 11, (11, 0.7664683862750687, 0.8677974564733781)
2023-08-19 18:33:21.387562: current best dsc: 0.8677974564733781 at epoch: 11, (11, 0.7664683862750687, 0.8677974564733781)
2023-08-19 18:33:23.480546: finished real validation
2023-08-19 18:33:49.511811: train_loss -0.8134
2023-08-19 18:33:49.513770: val_loss -0.7242
2023-08-19 18:33:49.515752: Pseudo dice [0.8722]
2023-08-19 18:33:49.516983: Epoch time: 662.57 s
2023-08-19 18:33:49.517968: Yayy! New best EMA pseudo Dice: 0.7558
2023-08-19 18:33:53.631029: 
2023-08-19 18:33:53.632681: Epoch 12
2023-08-19 18:33:53.633741: Current learning rate: 0.00482
2023-08-19 18:33:53.635101: start training, 250
================num of epochs: 250================
2023-08-19 18:40:50.184116: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 18:40:50.500655: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 18:40:50.503417: The split file contains 1 splits.
2023-08-19 18:40:50.504568: Desired fold for training: 0
2023-08-19 18:40:50.505602: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 18:44:28.350130: dsc: 86.08%
2023-08-19 18:44:28.352457: miou: 75.56%
2023-08-19 18:44:28.353549: acc: 95.60%, sen: 81.28%, spe: 98.48%
2023-08-19 18:44:28.355832: current best miou: 0.7664683862750687 at epoch: 11, (11, 0.7664683862750687, 0.8677974564733781)
2023-08-19 18:44:28.357273: current best dsc: 0.8677974564733781 at epoch: 11, (11, 0.7664683862750687, 0.8677974564733781)
2023-08-19 18:44:28.358269: finished real validation
2023-08-19 18:44:54.616083: train_loss -0.8168
2023-08-19 18:44:54.618093: val_loss -0.6917
2023-08-19 18:44:54.619922: Pseudo dice [0.8604]
2023-08-19 18:44:54.621212: Epoch time: 660.99 s
2023-08-19 18:44:54.622255: Yayy! New best EMA pseudo Dice: 0.7662
2023-08-19 18:44:58.041858: 
2023-08-19 18:44:58.043496: Epoch 13
2023-08-19 18:44:58.044583: Current learning rate: 0.0048
2023-08-19 18:44:58.046021: start training, 250
================num of epochs: 250================
2023-08-19 18:51:55.303757: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 18:51:55.621805: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 18:51:55.624970: The split file contains 1 splits.
2023-08-19 18:51:55.626262: Desired fold for training: 0
2023-08-19 18:51:55.627423: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 18:55:32.149389: dsc: 86.89%
2023-08-19 18:55:32.151110: miou: 76.82%
2023-08-19 18:55:32.152167: acc: 95.76%, sen: 83.95%, spe: 98.13%
2023-08-19 18:55:32.154113: current best miou: 0.7682330998719485 at epoch: 13, (13, 0.7682330998719485, 0.8689274054733872)
2023-08-19 18:55:32.155066: current best dsc: 0.8689274054733872 at epoch: 13, (13, 0.7682330998719485, 0.8689274054733872)
2023-08-19 18:55:34.116661: finished real validation
2023-08-19 18:56:00.017406: train_loss -0.8211
2023-08-19 18:56:00.019176: val_loss -0.7206
2023-08-19 18:56:00.020857: Pseudo dice [0.8688]
2023-08-19 18:56:00.021959: Epoch time: 661.98 s
2023-08-19 18:56:00.022973: Yayy! New best EMA pseudo Dice: 0.7765
2023-08-19 18:56:03.309984: 
2023-08-19 18:56:03.311665: Epoch 14
2023-08-19 18:56:03.312756: Current learning rate: 0.00479
2023-08-19 18:56:03.314176: start training, 250
================num of epochs: 250================
2023-08-19 19:03:00.358808: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 19:03:00.685379: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 19:03:00.688363: The split file contains 1 splits.
2023-08-19 19:03:00.689556: Desired fold for training: 0
2023-08-19 19:03:00.690962: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 19:06:38.428769: dsc: 86.75%
2023-08-19 19:06:38.430442: miou: 76.60%
2023-08-19 19:06:38.431531: acc: 95.80%, sen: 82.05%, spe: 98.57%
2023-08-19 19:06:38.433466: current best miou: 0.7682330998719485 at epoch: 13, (13, 0.7682330998719485, 0.8689274054733872)
2023-08-19 19:06:38.434379: current best dsc: 0.8689274054733872 at epoch: 13, (13, 0.7682330998719485, 0.8689274054733872)
2023-08-19 19:06:38.435328: finished real validation
2023-08-19 19:07:04.472053: train_loss -0.8254
2023-08-19 19:07:04.474088: val_loss -0.7041
2023-08-19 19:07:04.475744: Pseudo dice [0.8669]
2023-08-19 19:07:04.476884: Epoch time: 661.16 s
2023-08-19 19:07:04.477867: Yayy! New best EMA pseudo Dice: 0.7855
2023-08-19 19:07:07.840976: 
2023-08-19 19:07:07.842516: Epoch 15
2023-08-19 19:07:07.843581: Current learning rate: 0.00477
2023-08-19 19:07:07.844998: start training, 250
================num of epochs: 250================
2023-08-19 19:14:04.525293: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 19:14:04.845535: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 19:14:04.848521: The split file contains 1 splits.
2023-08-19 19:14:04.849768: Desired fold for training: 0
2023-08-19 19:14:04.850885: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 19:17:40.673833: dsc: 86.44%
2023-08-19 19:17:40.675485: miou: 76.12%
2023-08-19 19:17:40.676475: acc: 95.69%, sen: 82.00%, spe: 98.45%
2023-08-19 19:17:40.678335: current best miou: 0.7682330998719485 at epoch: 13, (13, 0.7682330998719485, 0.8689274054733872)
2023-08-19 19:17:40.679421: current best dsc: 0.8689274054733872 at epoch: 13, (13, 0.7682330998719485, 0.8689274054733872)
2023-08-19 19:17:40.680535: finished real validation
2023-08-19 19:18:06.435806: train_loss -0.8231
2023-08-19 19:18:06.441571: val_loss -0.6967
2023-08-19 19:18:06.444708: Pseudo dice [0.8645]
2023-08-19 19:18:06.450066: Epoch time: 658.6 s
2023-08-19 19:18:06.451141: Yayy! New best EMA pseudo Dice: 0.7934
2023-08-19 19:18:09.757258: 
2023-08-19 19:18:09.758647: Epoch 16
2023-08-19 19:18:09.759695: Current learning rate: 0.00476
2023-08-19 19:18:09.761085: start training, 250
================num of epochs: 250================
2023-08-19 19:25:06.111711: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 19:25:06.438087: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 19:25:06.441311: The split file contains 1 splits.
2023-08-19 19:25:06.442407: Desired fold for training: 0
2023-08-19 19:25:06.443499: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 19:28:44.297817: dsc: 86.89%
2023-08-19 19:28:44.304873: miou: 76.82%
2023-08-19 19:28:44.305944: acc: 95.80%, sen: 83.04%, spe: 98.37%
2023-08-19 19:28:44.308022: current best miou: 0.7682330998719485 at epoch: 13, (13, 0.7682330998719485, 0.8689274054733872)
2023-08-19 19:28:44.309145: current best dsc: 0.8689274054733872 at epoch: 13, (13, 0.7682330998719485, 0.8689274054733872)
2023-08-19 19:28:44.310152: finished real validation
2023-08-19 19:29:10.131379: train_loss -0.8216
2023-08-19 19:29:10.132946: val_loss -0.7192
2023-08-19 19:29:10.134438: Pseudo dice [0.8704]
2023-08-19 19:29:10.135818: Epoch time: 660.38 s
2023-08-19 19:29:10.136731: Yayy! New best EMA pseudo Dice: 0.8011
2023-08-19 19:29:13.464068: 
2023-08-19 19:29:13.465820: Epoch 17
2023-08-19 19:29:13.466913: Current learning rate: 0.00474
2023-08-19 19:29:13.468291: start training, 250
================num of epochs: 250================
2023-08-19 19:36:09.109547: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 19:36:09.423093: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 19:36:09.426111: The split file contains 1 splits.
2023-08-19 19:36:09.427441: Desired fold for training: 0
2023-08-19 19:36:09.428593: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 19:39:47.017294: dsc: 86.91%
2023-08-19 19:39:47.019027: miou: 76.84%
2023-08-19 19:39:47.020011: acc: 95.85%, sen: 82.25%, spe: 98.58%
2023-08-19 19:39:47.021915: current best miou: 0.7684349707854876 at epoch: 17, (17, 0.7684349707854876, 0.8690565200078249)
2023-08-19 19:39:47.022893: current best dsc: 0.8690565200078249 at epoch: 17, (17, 0.7684349707854876, 0.8690565200078249)
2023-08-19 19:39:49.071494: finished real validation
2023-08-19 19:40:15.036271: train_loss -0.8365
2023-08-19 19:40:15.038216: val_loss -0.72
2023-08-19 19:40:15.040004: Pseudo dice [0.8762]
2023-08-19 19:40:15.041142: Epoch time: 661.58 s
2023-08-19 19:40:15.042292: Yayy! New best EMA pseudo Dice: 0.8086
2023-08-19 19:40:18.330223: 
2023-08-19 19:40:18.331864: Epoch 18
2023-08-19 19:40:18.333049: Current learning rate: 0.00473
2023-08-19 19:40:18.334479: start training, 250
================num of epochs: 250================
2023-08-19 19:47:14.377603: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 19:47:14.700581: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 19:47:14.703462: The split file contains 1 splits.
2023-08-19 19:47:14.704661: Desired fold for training: 0
2023-08-19 19:47:14.705807: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 19:50:50.873875: dsc: 87.08%
2023-08-19 19:50:50.875684: miou: 77.11%
2023-08-19 19:50:50.876899: acc: 95.78%, sen: 84.82%, spe: 97.99%
2023-08-19 19:50:50.878784: current best miou: 0.7710981928375527 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 19:50:50.879851: current best dsc: 0.8707571335750086 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 19:50:53.029805: finished real validation
2023-08-19 19:51:18.870426: train_loss -0.8357
2023-08-19 19:51:18.872014: val_loss -0.7147
2023-08-19 19:51:18.873484: Pseudo dice [0.8741]
2023-08-19 19:51:18.874644: Epoch time: 660.54 s
2023-08-19 19:51:18.875712: Yayy! New best EMA pseudo Dice: 0.8152
2023-08-19 19:51:22.243980: 
2023-08-19 19:51:22.245555: Epoch 19
2023-08-19 19:51:22.246697: Current learning rate: 0.00471
2023-08-19 19:51:22.248035: start training, 250
================num of epochs: 250================
2023-08-19 19:58:16.239155: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 19:58:16.564031: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 19:58:16.567004: The split file contains 1 splits.
2023-08-19 19:58:16.568331: Desired fold for training: 0
2023-08-19 19:58:16.569411: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 20:01:55.705834: dsc: 86.10%
2023-08-19 20:01:55.708596: miou: 75.59%
2023-08-19 20:01:55.710682: acc: 95.65%, sen: 80.50%, spe: 98.69%
2023-08-19 20:01:55.713413: current best miou: 0.7710981928375527 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 20:01:55.715150: current best dsc: 0.8707571335750086 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 20:01:55.716604: finished real validation
2023-08-19 20:02:22.298242: train_loss -0.8395
2023-08-19 20:02:22.300225: val_loss -0.6666
2023-08-19 20:02:22.302087: Pseudo dice [0.8575]
2023-08-19 20:02:22.303467: Epoch time: 660.06 s
2023-08-19 20:02:22.304584: Yayy! New best EMA pseudo Dice: 0.8194
2023-08-19 20:02:28.180595: 
2023-08-19 20:02:28.182274: Epoch 20
2023-08-19 20:02:28.183491: Current learning rate: 0.0047
2023-08-19 20:02:28.185070: start training, 250
================num of epochs: 250================
2023-08-19 20:09:23.822697: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 20:09:24.128063: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 20:09:24.131597: The split file contains 1 splits.
2023-08-19 20:09:24.132918: Desired fold for training: 0
2023-08-19 20:09:24.134151: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 20:13:00.875156: dsc: 86.22%
2023-08-19 20:13:00.877418: miou: 75.79%
2023-08-19 20:13:00.878615: acc: 95.41%, sen: 85.80%, spe: 97.34%
2023-08-19 20:13:00.880821: current best miou: 0.7710981928375527 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 20:13:00.882070: current best dsc: 0.8707571335750086 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 20:13:00.883319: finished real validation
2023-08-19 20:13:26.485635: train_loss -0.8383
2023-08-19 20:13:26.487663: val_loss -0.6676
2023-08-19 20:13:26.489718: Pseudo dice [0.8462]
2023-08-19 20:13:26.491386: Epoch time: 658.31 s
2023-08-19 20:13:26.492657: Yayy! New best EMA pseudo Dice: 0.8221
2023-08-19 20:13:30.490776: 
2023-08-19 20:13:30.492393: Epoch 21
2023-08-19 20:13:30.493511: Current learning rate: 0.00468
2023-08-19 20:13:30.494954: start training, 250
================num of epochs: 250================
2023-08-19 20:20:24.657404: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 20:20:24.980673: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 20:20:24.983670: The split file contains 1 splits.
2023-08-19 20:20:24.984997: Desired fold for training: 0
2023-08-19 20:20:24.986257: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 20:24:02.190070: dsc: 86.60%
2023-08-19 20:24:02.193012: miou: 76.37%
2023-08-19 20:24:02.194219: acc: 95.71%, sen: 82.81%, spe: 98.30%
2023-08-19 20:24:02.196507: current best miou: 0.7710981928375527 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 20:24:02.197835: current best dsc: 0.8707571335750086 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 20:24:02.198845: finished real validation
2023-08-19 20:24:28.289704: train_loss -0.839
2023-08-19 20:24:28.291638: val_loss -0.7187
2023-08-19 20:24:28.293414: Pseudo dice [0.8741]
2023-08-19 20:24:28.294590: Epoch time: 657.8 s
2023-08-19 20:24:28.295616: Yayy! New best EMA pseudo Dice: 0.8273
2023-08-19 20:24:31.667926: 
2023-08-19 20:24:31.669375: Epoch 22
2023-08-19 20:24:31.670503: Current learning rate: 0.00467
2023-08-19 20:24:31.671907: start training, 250
================num of epochs: 250================
2023-08-19 20:31:26.088855: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 20:31:26.477741: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 20:31:26.481090: The split file contains 1 splits.
2023-08-19 20:31:26.482642: Desired fold for training: 0
2023-08-19 20:31:26.483914: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 20:35:41.720493: dsc: 86.98%
2023-08-19 20:35:41.722347: miou: 76.97%
2023-08-19 20:35:41.723567: acc: 95.86%, sen: 82.53%, spe: 98.55%
2023-08-19 20:35:41.725541: current best miou: 0.7710981928375527 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 20:35:41.726572: current best dsc: 0.8707571335750086 at epoch: 18, (18, 0.7710981928375527, 0.8707571335750086)
2023-08-19 20:35:41.727494: finished real validation
2023-08-19 20:36:07.391187: train_loss -0.8513
2023-08-19 20:36:07.392879: val_loss -0.6888
2023-08-19 20:36:07.394385: Pseudo dice [0.8659]
2023-08-19 20:36:07.395582: Epoch time: 695.73 s
2023-08-19 20:36:07.396595: Yayy! New best EMA pseudo Dice: 0.8312
2023-08-19 20:36:10.948002: 
2023-08-19 20:36:10.949548: Epoch 23
2023-08-19 20:36:10.951082: Current learning rate: 0.00465
2023-08-19 20:36:10.952560: start training, 250
================num of epochs: 250================
2023-08-19 20:43:05.924295: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 20:43:06.251402: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 20:43:06.254304: The split file contains 1 splits.
2023-08-19 20:43:06.255659: Desired fold for training: 0
2023-08-19 20:43:06.257166: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 20:46:43.969673: dsc: 87.54%
2023-08-19 20:46:45.296458: miou: 77.84%
2023-08-19 20:46:45.298124: acc: 95.89%, sen: 86.20%, spe: 97.84%
2023-08-19 20:46:45.300329: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 20:46:45.301399: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 20:46:47.186278: finished real validation
2023-08-19 20:47:12.826359: train_loss -0.8471
2023-08-19 20:47:12.828385: val_loss -0.6855
2023-08-19 20:47:12.829850: Pseudo dice [0.8686]
2023-08-19 20:47:12.831488: Epoch time: 661.88 s
2023-08-19 20:47:12.832822: Yayy! New best EMA pseudo Dice: 0.8349
2023-08-19 20:47:15.966787: 
2023-08-19 20:47:15.968441: Epoch 24
2023-08-19 20:47:15.969893: Current learning rate: 0.00464
2023-08-19 20:47:15.971309: start training, 250
================num of epochs: 250================
2023-08-19 20:54:10.232628: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 20:54:10.551109: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 20:54:10.554072: The split file contains 1 splits.
2023-08-19 20:54:10.555419: Desired fold for training: 0
2023-08-19 20:54:10.556606: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 20:57:48.507373: dsc: 86.43%
2023-08-19 20:57:48.509544: miou: 76.10%
2023-08-19 20:57:48.511009: acc: 95.71%, sen: 81.52%, spe: 98.57%
2023-08-19 20:57:48.513236: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 20:57:48.514282: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 20:57:48.515239: finished real validation
2023-08-19 20:58:14.404063: train_loss -0.8406
2023-08-19 20:58:14.405976: val_loss -0.6673
2023-08-19 20:58:14.407738: Pseudo dice [0.8513]
2023-08-19 20:58:14.409006: Epoch time: 658.44 s
2023-08-19 20:58:14.410283: Yayy! New best EMA pseudo Dice: 0.8365
2023-08-19 20:58:17.749721: 
2023-08-19 20:58:17.751476: Epoch 25
2023-08-19 20:58:17.752623: Current learning rate: 0.00462
2023-08-19 20:58:17.754002: start training, 250
================num of epochs: 250================
2023-08-19 21:05:10.799479: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 21:05:11.110294: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 21:05:11.113413: The split file contains 1 splits.
2023-08-19 21:05:11.114777: Desired fold for training: 0
2023-08-19 21:05:11.116091: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 21:08:51.420926: dsc: 86.08%
2023-08-19 21:08:51.423077: miou: 75.56%
2023-08-19 21:08:51.424351: acc: 95.64%, sen: 80.48%, spe: 98.69%
2023-08-19 21:08:51.426789: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:08:51.428023: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:08:51.429242: finished real validation
2023-08-19 21:09:16.434249: train_loss -0.8498
2023-08-19 21:09:16.436063: val_loss -0.71
2023-08-19 21:09:16.437857: Pseudo dice [0.8603]
2023-08-19 21:09:16.439091: Epoch time: 658.69 s
2023-08-19 21:09:16.440316: Yayy! New best EMA pseudo Dice: 0.8389
2023-08-19 21:09:19.639970: 
2023-08-19 21:09:19.641681: Epoch 26
2023-08-19 21:09:19.642719: Current learning rate: 0.00461
2023-08-19 21:09:19.644265: start training, 250
================num of epochs: 250================
2023-08-19 21:16:11.368839: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 21:16:11.699123: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 21:16:11.709465: The split file contains 1 splits.
2023-08-19 21:16:11.710951: Desired fold for training: 0
2023-08-19 21:16:11.716124: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 21:19:49.975721: dsc: 87.02%
2023-08-19 21:19:49.977160: miou: 77.02%
2023-08-19 21:19:49.978174: acc: 95.78%, sen: 84.49%, spe: 98.05%
2023-08-19 21:19:49.980035: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:19:49.981081: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:19:49.982073: finished real validation
2023-08-19 21:20:15.777205: train_loss -0.8536
2023-08-19 21:20:15.778861: val_loss -0.6927
2023-08-19 21:20:15.780755: Pseudo dice [0.859]
2023-08-19 21:20:15.782007: Epoch time: 656.14 s
2023-08-19 21:20:15.783240: Yayy! New best EMA pseudo Dice: 0.8409
2023-08-19 21:20:18.980879: 
2023-08-19 21:20:18.982510: Epoch 27
2023-08-19 21:20:18.983572: Current learning rate: 0.00459
2023-08-19 21:20:18.984960: start training, 250
================num of epochs: 250================
2023-08-19 21:27:11.984050: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 21:27:12.347347: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 21:27:12.350364: The split file contains 1 splits.
2023-08-19 21:27:12.351712: Desired fold for training: 0
2023-08-19 21:27:12.353083: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 21:30:50.358139: dsc: 85.06%
2023-08-19 21:30:50.374388: miou: 74.00%
2023-08-19 21:30:50.382956: acc: 95.36%, sen: 78.91%, spe: 98.67%
2023-08-19 21:30:50.393079: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:30:50.408984: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:30:50.417597: finished real validation
2023-08-19 21:31:16.106040: train_loss -0.8335
2023-08-19 21:31:16.107741: val_loss -0.6643
2023-08-19 21:31:16.109574: Pseudo dice [0.8501]
2023-08-19 21:31:16.110962: Epoch time: 657.13 s
2023-08-19 21:31:16.112398: Yayy! New best EMA pseudo Dice: 0.8418
2023-08-19 21:31:19.538584: 
2023-08-19 21:31:19.540244: Epoch 28
2023-08-19 21:31:19.541290: Current learning rate: 0.00458
2023-08-19 21:31:19.543059: start training, 250
================num of epochs: 250================
2023-08-19 21:38:11.071201: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 21:38:11.380317: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 21:38:11.382988: The split file contains 1 splits.
2023-08-19 21:38:11.384289: Desired fold for training: 0
2023-08-19 21:38:11.385529: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 21:41:48.999741: dsc: 85.40%
2023-08-19 21:41:49.007776: miou: 74.52%
2023-08-19 21:41:49.009388: acc: 95.50%, sen: 78.53%, spe: 98.92%
2023-08-19 21:41:49.012822: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:41:49.014646: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:41:49.016550: finished real validation
2023-08-19 21:42:14.850335: train_loss -0.8395
2023-08-19 21:42:14.857381: val_loss -0.6795
2023-08-19 21:42:14.860310: Pseudo dice [0.8543]
2023-08-19 21:42:14.862083: Epoch time: 655.31 s
2023-08-19 21:42:14.863327: Yayy! New best EMA pseudo Dice: 0.8431
2023-08-19 21:42:18.450031: 
2023-08-19 21:42:18.453683: Epoch 29
2023-08-19 21:42:18.457384: Current learning rate: 0.00456
2023-08-19 21:42:18.459921: start training, 250
================num of epochs: 250================
2023-08-19 21:49:11.168101: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 21:49:11.489405: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 21:49:11.492597: The split file contains 1 splits.
2023-08-19 21:49:11.494139: Desired fold for training: 0
2023-08-19 21:49:11.495500: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 21:52:50.141170: dsc: 87.24%
2023-08-19 21:52:50.142913: miou: 77.37%
2023-08-19 21:52:50.144113: acc: 95.89%, sen: 84.00%, spe: 98.28%
2023-08-19 21:52:50.146104: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:52:50.147308: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 21:52:50.148445: finished real validation
2023-08-19 21:53:15.863146: train_loss -0.8461
2023-08-19 21:53:15.865212: val_loss -0.6867
2023-08-19 21:53:15.866980: Pseudo dice [0.8613]
2023-08-19 21:53:15.868281: Epoch time: 657.42 s
2023-08-19 21:53:15.869376: Yayy! New best EMA pseudo Dice: 0.8449
2023-08-19 21:53:19.097479: 
2023-08-19 21:53:19.099148: Epoch 30
2023-08-19 21:53:19.100317: Current learning rate: 0.00455
2023-08-19 21:53:19.101785: start training, 250
================num of epochs: 250================
2023-08-19 22:00:12.806564: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 22:00:13.119940: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 22:00:13.123061: The split file contains 1 splits.
2023-08-19 22:00:13.124469: Desired fold for training: 0
2023-08-19 22:00:13.125792: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 22:03:51.578107: dsc: 86.63%
2023-08-19 22:03:51.579829: miou: 76.41%
2023-08-19 22:03:51.581104: acc: 95.66%, sen: 83.97%, spe: 98.01%
2023-08-19 22:03:51.583242: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:03:51.584457: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:03:51.585576: finished real validation
2023-08-19 22:04:17.579755: train_loss -0.8521
2023-08-19 22:04:17.581876: val_loss -0.6969
2023-08-19 22:04:17.583672: Pseudo dice [0.8618]
2023-08-19 22:04:17.584914: Epoch time: 658.48 s
2023-08-19 22:04:17.585987: Yayy! New best EMA pseudo Dice: 0.8466
2023-08-19 22:04:20.860459: 
2023-08-19 22:04:20.862121: Epoch 31
2023-08-19 22:04:20.863271: Current learning rate: 0.00453
2023-08-19 22:04:20.864745: start training, 250
================num of epochs: 250================
2023-08-19 22:11:16.843086: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 22:11:17.149002: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 22:11:17.152030: The split file contains 1 splits.
2023-08-19 22:11:17.153342: Desired fold for training: 0
2023-08-19 22:11:17.154437: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 22:14:56.286463: dsc: 86.59%
2023-08-19 22:14:56.288257: miou: 76.35%
2023-08-19 22:14:56.289359: acc: 95.65%, sen: 83.85%, spe: 98.02%
2023-08-19 22:14:56.291342: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:14:56.292463: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:14:56.293479: finished real validation
2023-08-19 22:15:22.361873: train_loss -0.8594
2023-08-19 22:15:22.363768: val_loss -0.6723
2023-08-19 22:15:22.365575: Pseudo dice [0.8601]
2023-08-19 22:15:22.366812: Epoch time: 661.5 s
2023-08-19 22:15:22.367905: Yayy! New best EMA pseudo Dice: 0.8479
2023-08-19 22:15:25.654361: 
2023-08-19 22:15:25.656134: Epoch 32
2023-08-19 22:15:25.657372: Current learning rate: 0.00452
2023-08-19 22:15:25.658987: start training, 250
================num of epochs: 250================
2023-08-19 22:22:20.330253: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 22:22:20.641536: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 22:22:20.644691: The split file contains 1 splits.
2023-08-19 22:22:20.646123: Desired fold for training: 0
2023-08-19 22:22:20.647389: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 22:25:57.875096: dsc: 86.81%
2023-08-19 22:25:57.877160: miou: 76.70%
2023-08-19 22:25:57.878347: acc: 95.67%, sen: 85.00%, spe: 97.82%
2023-08-19 22:25:57.880320: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:25:57.881498: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:25:57.882519: finished real validation
2023-08-19 22:26:23.945199: train_loss -0.8601
2023-08-19 22:26:23.946994: val_loss -0.7142
2023-08-19 22:26:23.948597: Pseudo dice [0.8739]
2023-08-19 22:26:23.949838: Epoch time: 658.29 s
2023-08-19 22:26:23.950980: Yayy! New best EMA pseudo Dice: 0.8505
2023-08-19 22:26:27.243295: 
2023-08-19 22:26:27.244989: Epoch 33
2023-08-19 22:26:27.246227: Current learning rate: 0.0045
2023-08-19 22:26:27.247704: start training, 250
================num of epochs: 250================
2023-08-19 22:33:22.203064: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 22:33:22.508417: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 22:33:22.512064: The split file contains 1 splits.
2023-08-19 22:33:22.513320: Desired fold for training: 0
2023-08-19 22:33:22.514512: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 22:37:02.071477: dsc: 87.29%
2023-08-19 22:37:02.073362: miou: 77.45%
2023-08-19 22:37:02.074472: acc: 95.94%, sen: 83.25%, spe: 98.49%
2023-08-19 22:37:02.076419: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:37:02.077535: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:37:02.078571: finished real validation
2023-08-19 22:37:27.852757: train_loss -0.8573
2023-08-19 22:37:27.854433: val_loss -0.7083
2023-08-19 22:37:27.855937: Pseudo dice [0.8689]
2023-08-19 22:37:27.857282: Epoch time: 660.61 s
2023-08-19 22:37:27.858446: Yayy! New best EMA pseudo Dice: 0.8524
2023-08-19 22:37:31.345254: 
2023-08-19 22:37:31.346943: Epoch 34
2023-08-19 22:37:31.348147: Current learning rate: 0.00449
2023-08-19 22:37:31.349646: start training, 250
================num of epochs: 250================
2023-08-19 22:44:25.296355: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 22:44:25.613334: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 22:44:25.616038: The split file contains 1 splits.
2023-08-19 22:44:25.617277: Desired fold for training: 0
2023-08-19 22:44:25.618423: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 22:48:04.009942: dsc: 86.75%
2023-08-19 22:48:04.011715: miou: 76.60%
2023-08-19 22:48:04.012843: acc: 95.83%, sen: 81.57%, spe: 98.69%
2023-08-19 22:48:04.014791: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:48:04.016043: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:48:04.017236: finished real validation
2023-08-19 22:48:29.950114: train_loss -0.8597
2023-08-19 22:48:29.952204: val_loss -0.7143
2023-08-19 22:48:29.953930: Pseudo dice [0.873]
2023-08-19 22:48:29.955372: Epoch time: 658.61 s
2023-08-19 22:48:29.956490: Yayy! New best EMA pseudo Dice: 0.8544
2023-08-19 22:48:33.340959: 
2023-08-19 22:48:33.342495: Epoch 35
2023-08-19 22:48:33.343600: Current learning rate: 0.00447
2023-08-19 22:48:33.345025: start training, 250
================num of epochs: 250================
2023-08-19 22:55:27.012382: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 22:55:27.331678: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 22:55:27.334958: The split file contains 1 splits.
2023-08-19 22:55:27.336973: Desired fold for training: 0
2023-08-19 22:55:27.338989: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 22:59:03.722749: dsc: 86.96%
2023-08-19 22:59:03.724505: miou: 76.93%
2023-08-19 22:59:03.725642: acc: 95.86%, sen: 82.41%, spe: 98.57%
2023-08-19 22:59:03.727661: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:59:03.728937: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 22:59:03.730039: finished real validation
2023-08-19 22:59:29.454055: train_loss -0.8632
2023-08-19 22:59:29.456052: val_loss -0.6922
2023-08-19 22:59:29.457861: Pseudo dice [0.8623]
2023-08-19 22:59:29.459235: Epoch time: 656.12 s
2023-08-19 22:59:29.460381: Yayy! New best EMA pseudo Dice: 0.8552
2023-08-19 22:59:32.732224: 
2023-08-19 22:59:32.733956: Epoch 36
2023-08-19 22:59:32.735123: Current learning rate: 0.00446
2023-08-19 22:59:32.736624: start training, 250
================num of epochs: 250================
2023-08-19 23:06:30.637712: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 23:06:30.944569: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 23:06:30.947838: The split file contains 1 splits.
2023-08-19 23:06:30.949188: Desired fold for training: 0
2023-08-19 23:06:30.950614: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 23:10:07.320683: dsc: 87.17%
2023-08-19 23:10:07.322699: miou: 77.26%
2023-08-19 23:10:07.323941: acc: 95.93%, sen: 82.54%, spe: 98.62%
2023-08-19 23:10:07.325919: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:10:07.327248: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:10:07.328366: finished real validation
2023-08-19 23:10:33.370121: train_loss -0.8589
2023-08-19 23:10:33.390975: val_loss -0.7112
2023-08-19 23:10:33.405331: Pseudo dice [0.8755]
2023-08-19 23:10:33.428982: Epoch time: 660.64 s
2023-08-19 23:10:33.430470: Yayy! New best EMA pseudo Dice: 0.8572
2023-08-19 23:10:36.783429: 
2023-08-19 23:10:36.804309: Epoch 37
2023-08-19 23:10:36.845077: Current learning rate: 0.00444
2023-08-19 23:10:36.859855: start training, 250
================num of epochs: 250================
2023-08-19 23:17:31.500493: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 23:17:31.817143: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 23:17:31.819980: The split file contains 1 splits.
2023-08-19 23:17:31.821276: Desired fold for training: 0
2023-08-19 23:17:31.822616: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 23:21:10.541585: dsc: 86.85%
2023-08-19 23:21:10.554800: miou: 76.76%
2023-08-19 23:21:10.577245: acc: 95.69%, sen: 84.93%, spe: 97.86%
2023-08-19 23:21:10.601372: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:21:10.622705: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:21:10.645694: finished real validation
2023-08-19 23:21:36.520510: train_loss -0.8626
2023-08-19 23:21:36.537149: val_loss -0.6693
2023-08-19 23:21:36.558920: Pseudo dice [0.8578]
2023-08-19 23:21:36.579633: Epoch time: 659.74 s
2023-08-19 23:21:36.600966: Yayy! New best EMA pseudo Dice: 0.8573
2023-08-19 23:21:39.882338: 
2023-08-19 23:21:39.888282: Epoch 38
2023-08-19 23:21:39.889542: Current learning rate: 0.00443
2023-08-19 23:21:39.891042: start training, 250
================num of epochs: 250================
2023-08-19 23:28:34.686888: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 23:28:34.994027: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 23:28:34.997095: The split file contains 1 splits.
2023-08-19 23:28:34.998352: Desired fold for training: 0
2023-08-19 23:28:34.999682: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 23:32:12.077656: dsc: 87.52%
2023-08-19 23:32:12.079710: miou: 77.82%
2023-08-19 23:32:12.080952: acc: 95.95%, sen: 84.77%, spe: 98.20%
2023-08-19 23:32:12.083247: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:32:12.084430: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:32:12.085498: finished real validation
2023-08-19 23:32:38.011634: train_loss -0.8631
2023-08-19 23:32:38.013550: val_loss -0.752
2023-08-19 23:32:38.015351: Pseudo dice [0.8837]
2023-08-19 23:32:38.017109: Epoch time: 658.13 s
2023-08-19 23:32:38.018284: Yayy! New best EMA pseudo Dice: 0.8599
2023-08-19 23:32:41.213810: 
2023-08-19 23:32:41.215633: Epoch 39
2023-08-19 23:32:41.216942: Current learning rate: 0.00441
2023-08-19 23:32:41.219350: start training, 250
================num of epochs: 250================
2023-08-19 23:39:36.525852: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 23:39:36.838287: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 23:39:36.841370: The split file contains 1 splits.
2023-08-19 23:39:36.842626: Desired fold for training: 0
2023-08-19 23:39:36.843848: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 23:43:16.643029: dsc: 87.42%
2023-08-19 23:43:16.644751: miou: 77.65%
2023-08-19 23:43:16.645955: acc: 95.98%, sen: 83.38%, spe: 98.52%
2023-08-19 23:43:16.648119: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:43:16.649301: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:43:16.650395: finished real validation
2023-08-19 23:43:42.538164: train_loss -0.8678
2023-08-19 23:43:42.540037: val_loss -0.7102
2023-08-19 23:43:42.541825: Pseudo dice [0.868]
2023-08-19 23:43:42.543094: Epoch time: 661.33 s
2023-08-19 23:43:42.544190: Yayy! New best EMA pseudo Dice: 0.8607
2023-08-19 23:43:45.952325: 
2023-08-19 23:43:45.954190: Epoch 40
2023-08-19 23:43:45.955435: Current learning rate: 0.0044
2023-08-19 23:43:45.957107: start training, 250
================num of epochs: 250================
2023-08-19 23:50:41.889960: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-19 23:50:42.206400: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-19 23:50:42.209746: The split file contains 1 splits.
2023-08-19 23:50:42.211103: Desired fold for training: 0
2023-08-19 23:50:42.212744: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-19 23:54:20.698438: dsc: 87.07%
2023-08-19 23:54:20.700275: miou: 77.10%
2023-08-19 23:54:20.701379: acc: 95.87%, sen: 83.01%, spe: 98.46%
2023-08-19 23:54:20.703341: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:54:20.704488: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-19 23:54:20.705565: finished real validation
2023-08-19 23:54:46.529051: train_loss -0.8658
2023-08-19 23:54:46.530882: val_loss -0.7119
2023-08-19 23:54:46.532463: Pseudo dice [0.8657]
2023-08-19 23:54:46.533721: Epoch time: 660.58 s
2023-08-19 23:54:46.534908: Yayy! New best EMA pseudo Dice: 0.8612
2023-08-19 23:54:49.817278: 
2023-08-19 23:54:49.819127: Epoch 41
2023-08-19 23:54:49.820354: Current learning rate: 0.00438
2023-08-19 23:54:49.821940: start training, 250
================num of epochs: 250================
2023-08-20 00:01:45.217710: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 00:01:45.534099: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 00:01:45.537019: The split file contains 1 splits.
2023-08-20 00:01:45.538353: Desired fold for training: 0
2023-08-20 00:01:45.539566: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 00:05:23.666880: dsc: 87.22%
2023-08-20 00:05:23.668910: miou: 77.33%
2023-08-20 00:05:23.670108: acc: 95.92%, sen: 83.17%, spe: 98.48%
2023-08-20 00:05:23.672306: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:05:23.673451: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:05:23.674505: finished real validation
2023-08-20 00:05:49.631396: train_loss -0.8667
2023-08-20 00:05:49.633425: val_loss -0.7391
2023-08-20 00:05:49.635173: Pseudo dice [0.8822]
2023-08-20 00:05:49.636454: Epoch time: 659.82 s
2023-08-20 00:05:49.637586: Yayy! New best EMA pseudo Dice: 0.8633
2023-08-20 00:05:52.961301: 
2023-08-20 00:05:52.963240: Epoch 42
2023-08-20 00:05:52.964508: Current learning rate: 0.00437
2023-08-20 00:05:52.966235: start training, 250
================num of epochs: 250================
2023-08-20 00:12:48.267397: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 00:12:48.581907: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 00:12:48.585005: The split file contains 1 splits.
2023-08-20 00:12:48.586353: Desired fold for training: 0
2023-08-20 00:12:48.587644: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 00:16:25.868263: dsc: 86.62%
2023-08-20 00:16:25.870001: miou: 76.40%
2023-08-20 00:16:25.871078: acc: 95.79%, sen: 81.46%, spe: 98.67%
2023-08-20 00:16:25.873082: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:16:25.874290: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:16:25.875383: finished real validation
2023-08-20 00:16:51.666689: train_loss -0.8735
2023-08-20 00:17:05.634954: val_loss -0.7114
2023-08-20 00:17:05.638405: Pseudo dice [0.8697]
2023-08-20 00:17:05.639863: Epoch time: 658.71 s
2023-08-20 00:17:05.641087: Yayy! New best EMA pseudo Dice: 0.864
2023-08-20 00:17:08.910460: 
2023-08-20 00:17:08.912467: Epoch 43
2023-08-20 00:17:08.913804: Current learning rate: 0.00435
2023-08-20 00:17:08.915598: start training, 250
================num of epochs: 250================
2023-08-20 00:24:03.961641: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 00:24:04.277740: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 00:24:04.280884: The split file contains 1 splits.
2023-08-20 00:24:04.282134: Desired fold for training: 0
2023-08-20 00:24:04.283509: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 00:27:41.138526: dsc: 87.07%
2023-08-20 00:27:41.141381: miou: 77.09%
2023-08-20 00:27:41.142623: acc: 95.81%, sen: 84.12%, spe: 98.17%
2023-08-20 00:27:41.144636: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:27:41.145721: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:27:41.146837: finished real validation
2023-08-20 00:28:07.041650: train_loss -0.8687
2023-08-20 00:28:07.043463: val_loss -0.6858
2023-08-20 00:28:07.045058: Pseudo dice [0.8604]
2023-08-20 00:28:07.046392: Epoch time: 658.13 s
2023-08-20 00:28:08.334663: 
2023-08-20 00:28:08.336338: Epoch 44
2023-08-20 00:28:08.337579: Current learning rate: 0.00433
2023-08-20 00:28:08.339078: start training, 250
================num of epochs: 250================
2023-08-20 00:35:03.272561: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 00:35:03.652231: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 00:35:03.655411: The split file contains 1 splits.
2023-08-20 00:35:03.657012: Desired fold for training: 0
2023-08-20 00:35:03.658507: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 00:39:30.439466: dsc: 87.43%
2023-08-20 00:39:30.441593: miou: 77.66%
2023-08-20 00:39:30.443678: acc: 95.83%, sen: 86.51%, spe: 97.71%
2023-08-20 00:39:30.446168: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:39:30.448063: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:39:30.449342: finished real validation
2023-08-20 00:39:56.390053: train_loss -0.8681
2023-08-20 00:39:56.392163: val_loss -0.7071
2023-08-20 00:39:56.394127: Pseudo dice [0.8713]
2023-08-20 00:39:56.395475: Epoch time: 708.06 s
2023-08-20 00:39:56.396642: Yayy! New best EMA pseudo Dice: 0.8644
2023-08-20 00:39:59.699330: 
2023-08-20 00:39:59.701160: Epoch 45
2023-08-20 00:39:59.702393: Current learning rate: 0.00432
2023-08-20 00:39:59.703998: start training, 250
================num of epochs: 250================
2023-08-20 00:46:56.360054: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 00:46:56.662306: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 00:46:56.665917: The split file contains 1 splits.
2023-08-20 00:46:56.667329: Desired fold for training: 0
2023-08-20 00:46:56.668822: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 00:50:34.584448: dsc: 87.15%
2023-08-20 00:50:34.605817: miou: 77.22%
2023-08-20 00:50:34.623680: acc: 95.83%, sen: 84.29%, spe: 98.16%
2023-08-20 00:50:34.651264: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:50:34.668169: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 00:50:34.685855: finished real validation
2023-08-20 00:51:00.569948: train_loss -0.8712
2023-08-20 00:51:00.571934: val_loss -0.7003
2023-08-20 00:51:00.573768: Pseudo dice [0.8706]
2023-08-20 00:51:00.575147: Epoch time: 660.87 s
2023-08-20 00:51:00.576389: Yayy! New best EMA pseudo Dice: 0.865
2023-08-20 00:51:04.042434: 
2023-08-20 00:51:04.044502: Epoch 46
2023-08-20 00:51:04.045712: Current learning rate: 0.0043
2023-08-20 00:51:04.047622: start training, 250
================num of epochs: 250================
2023-08-20 00:57:59.624524: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 00:57:59.939920: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 00:57:59.943884: The split file contains 1 splits.
2023-08-20 00:57:59.945387: Desired fold for training: 0
2023-08-20 00:57:59.946819: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 01:01:39.721989: dsc: 86.94%
2023-08-20 01:01:39.724148: miou: 76.89%
2023-08-20 01:01:39.726153: acc: 95.83%, sen: 82.87%, spe: 98.44%
2023-08-20 01:01:39.728217: current best miou: 0.7783904555083915 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 01:01:39.729745: current best dsc: 0.8753875765554214 at epoch: 23, (23, 0.7783904555083915, 0.8753875765554214)
2023-08-20 01:01:39.731260: finished real validation
2023-08-20 01:02:05.752456: train_loss -0.8724
2023-08-20 01:02:05.754511: val_loss -0.7279
2023-08-20 01:02:05.756088: Pseudo dice [0.8698]
2023-08-20 01:02:05.757674: Epoch time: 661.71 s
2023-08-20 01:02:05.759153: Yayy! New best EMA pseudo Dice: 0.8655
2023-08-20 01:02:09.064675: 
2023-08-20 01:02:09.066793: Epoch 47
2023-08-20 01:02:09.069230: Current learning rate: 0.00429
2023-08-20 01:02:09.070980: start training, 250
================num of epochs: 250================
2023-08-20 01:09:04.869309: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 01:09:05.193188: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 01:09:05.196418: The split file contains 1 splits.
2023-08-20 01:09:05.197740: Desired fold for training: 0
2023-08-20 01:09:05.199135: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 01:12:43.819510: dsc: 87.58%
2023-08-20 01:12:43.821662: miou: 77.91%
2023-08-20 01:12:43.823122: acc: 95.95%, sen: 85.21%, spe: 98.11%
2023-08-20 01:12:43.825333: current best miou: 0.779069990947944 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:12:43.826504: current best dsc: 0.8758171346961242 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:12:45.810073: finished real validation
2023-08-20 01:13:11.836034: train_loss -0.8748
2023-08-20 01:13:11.838104: val_loss -0.7493
2023-08-20 01:13:11.839934: Pseudo dice [0.8895]
2023-08-20 01:13:11.841401: Epoch time: 662.77 s
2023-08-20 01:13:11.842622: Yayy! New best EMA pseudo Dice: 0.8679
2023-08-20 01:13:15.083311: 
2023-08-20 01:13:15.084961: Epoch 48
2023-08-20 01:13:15.086216: Current learning rate: 0.00427
2023-08-20 01:13:15.087807: start training, 250
================num of epochs: 250================
2023-08-20 01:20:10.165416: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 01:20:10.473624: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 01:20:10.476901: The split file contains 1 splits.
2023-08-20 01:20:10.478584: Desired fold for training: 0
2023-08-20 01:20:10.479998: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 01:23:47.170686: dsc: 87.00%
2023-08-20 01:23:47.172683: miou: 77.00%
2023-08-20 01:23:47.173853: acc: 95.87%, sen: 82.58%, spe: 98.54%
2023-08-20 01:23:47.175852: current best miou: 0.779069990947944 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:23:47.177079: current best dsc: 0.8758171346961242 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:23:47.178277: finished real validation
2023-08-20 01:24:13.240133: train_loss -0.8715
2023-08-20 01:24:13.242199: val_loss -0.651
2023-08-20 01:24:13.244014: Pseudo dice [0.8554]
2023-08-20 01:24:13.245343: Epoch time: 658.16 s
2023-08-20 01:24:14.568643: 
2023-08-20 01:24:14.570397: Epoch 49
2023-08-20 01:24:14.571689: Current learning rate: 0.00426
2023-08-20 01:24:14.573364: start training, 250
================num of epochs: 250================
2023-08-20 01:31:11.914720: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 01:31:12.230863: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 01:31:12.234070: The split file contains 1 splits.
2023-08-20 01:31:12.235536: Desired fold for training: 0
2023-08-20 01:31:12.236792: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 01:34:49.813121: dsc: 87.07%
2023-08-20 01:34:49.814867: miou: 77.10%
2023-08-20 01:34:49.815959: acc: 95.90%, sen: 82.45%, spe: 98.60%
2023-08-20 01:34:49.817997: current best miou: 0.779069990947944 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:34:49.819243: current best dsc: 0.8758171346961242 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:34:49.820326: finished real validation
2023-08-20 01:35:15.732188: train_loss -0.8752
2023-08-20 01:35:15.734378: val_loss -0.7264
2023-08-20 01:35:15.736157: Pseudo dice [0.8722]
2023-08-20 01:35:15.737563: Epoch time: 661.17 s
2023-08-20 01:35:18.893270: 
2023-08-20 01:35:18.895029: Epoch 50
2023-08-20 01:35:18.896200: Current learning rate: 0.00424
2023-08-20 01:35:18.897759: start training, 250
================num of epochs: 250================
2023-08-20 01:42:15.338135: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 01:42:15.648991: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 01:42:15.652386: The split file contains 1 splits.
2023-08-20 01:42:15.653726: Desired fold for training: 0
2023-08-20 01:42:15.654966: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 01:45:50.501368: dsc: 87.19%
2023-08-20 01:45:50.503604: miou: 77.28%
2023-08-20 01:45:50.504826: acc: 95.87%, sen: 83.95%, spe: 98.26%
2023-08-20 01:45:50.507103: current best miou: 0.779069990947944 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:45:50.508440: current best dsc: 0.8758171346961242 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:45:50.509578: finished real validation
2023-08-20 01:46:16.271337: train_loss -0.8757
2023-08-20 01:46:16.273365: val_loss -0.6995
2023-08-20 01:46:16.275043: Pseudo dice [0.8686]
2023-08-20 01:46:16.276541: Epoch time: 657.38 s
2023-08-20 01:46:17.596643: 
2023-08-20 01:46:17.598456: Epoch 51
2023-08-20 01:46:17.599704: Current learning rate: 0.00423
2023-08-20 01:46:17.601434: start training, 250
================num of epochs: 250================
2023-08-20 01:53:13.571355: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 01:53:13.883931: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 01:53:13.887111: The split file contains 1 splits.
2023-08-20 01:53:13.888562: Desired fold for training: 0
2023-08-20 01:53:13.889941: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 01:56:51.111331: dsc: 87.48%
2023-08-20 01:56:51.113203: miou: 77.75%
2023-08-20 01:56:51.114394: acc: 95.93%, sen: 84.87%, spe: 98.16%
2023-08-20 01:56:51.116426: current best miou: 0.779069990947944 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:56:51.117535: current best dsc: 0.8758171346961242 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 01:56:51.118598: finished real validation
2023-08-20 01:57:17.009215: train_loss -0.877
2023-08-20 01:57:17.010849: val_loss -0.7176
2023-08-20 01:57:17.012470: Pseudo dice [0.8803]
2023-08-20 01:57:17.013765: Epoch time: 659.42 s
2023-08-20 01:57:17.014973: Yayy! New best EMA pseudo Dice: 0.8686
2023-08-20 01:57:20.350288: 
2023-08-20 01:57:20.352051: Epoch 52
2023-08-20 01:57:20.353285: Current learning rate: 0.00421
2023-08-20 01:57:20.354812: start training, 250
================num of epochs: 250================
2023-08-20 02:04:16.343266: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 02:04:16.658102: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 02:04:16.661271: The split file contains 1 splits.
2023-08-20 02:04:16.662650: Desired fold for training: 0
2023-08-20 02:04:16.664091: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 02:07:53.736882: dsc: 86.61%
2023-08-20 02:07:53.738687: miou: 76.39%
2023-08-20 02:07:53.739858: acc: 95.66%, sen: 83.77%, spe: 98.06%
2023-08-20 02:07:53.741949: current best miou: 0.779069990947944 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 02:07:53.743304: current best dsc: 0.8758171346961242 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 02:07:53.744455: finished real validation
2023-08-20 02:08:19.656074: train_loss -0.8791
2023-08-20 02:08:19.658204: val_loss -0.6987
2023-08-20 02:08:19.660095: Pseudo dice [0.8618]
2023-08-20 02:08:19.661534: Epoch time: 659.31 s
2023-08-20 02:08:20.989458: 
2023-08-20 02:08:20.991159: Epoch 53
2023-08-20 02:08:20.992396: Current learning rate: 0.0042
2023-08-20 02:08:20.993991: start training, 250
================num of epochs: 250================
2023-08-20 02:15:16.658657: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 02:15:16.962252: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 02:15:16.965655: The split file contains 1 splits.
2023-08-20 02:15:16.967108: Desired fold for training: 0
2023-08-20 02:15:16.968509: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 02:18:54.152121: dsc: 87.05%
2023-08-20 02:18:54.155712: miou: 77.06%
2023-08-20 02:18:54.156899: acc: 95.81%, sen: 83.97%, spe: 98.20%
2023-08-20 02:18:54.159021: current best miou: 0.779069990947944 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 02:18:54.160297: current best dsc: 0.8758171346961242 at epoch: 47, (47, 0.779069990947944, 0.8758171346961242)
2023-08-20 02:18:54.161506: finished real validation
2023-08-20 02:19:20.143146: train_loss -0.878
2023-08-20 02:19:20.145082: val_loss -0.684
2023-08-20 02:19:20.146996: Pseudo dice [0.8625]
2023-08-20 02:19:20.148385: Epoch time: 659.16 s
2023-08-20 02:19:21.472597: 
2023-08-20 02:19:21.474349: Epoch 54
2023-08-20 02:19:21.475651: Current learning rate: 0.00418
2023-08-20 02:19:21.477310: start training, 250
================num of epochs: 250================
2023-08-20 02:26:15.988561: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 02:26:16.465766: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 02:26:16.469012: The split file contains 1 splits.
2023-08-20 02:26:16.470388: Desired fold for training: 0
2023-08-20 02:26:16.471807: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 02:29:52.208364: dsc: 87.59%
2023-08-20 02:29:52.210231: miou: 77.93%
2023-08-20 02:29:52.211360: acc: 96.00%, sen: 84.22%, spe: 98.38%
2023-08-20 02:29:52.213425: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 02:29:52.214554: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 02:29:54.572806: finished real validation
2023-08-20 02:30:20.336931: train_loss -0.8799
2023-08-20 02:30:20.338950: val_loss -0.7176
2023-08-20 02:30:20.340861: Pseudo dice [0.8779]
2023-08-20 02:30:20.342363: Epoch time: 658.87 s
2023-08-20 02:30:21.665474: 
2023-08-20 02:30:21.667205: Epoch 55
2023-08-20 02:30:21.668458: Current learning rate: 0.00417
2023-08-20 02:30:21.670120: start training, 250
================num of epochs: 250================
2023-08-20 02:37:17.654550: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 02:37:17.971192: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 02:37:17.974474: The split file contains 1 splits.
2023-08-20 02:37:17.976007: Desired fold for training: 0
2023-08-20 02:37:17.977347: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 02:40:53.273070: dsc: 86.89%
2023-08-20 02:40:53.275151: miou: 76.82%
2023-08-20 02:40:53.276352: acc: 95.81%, sen: 82.94%, spe: 98.40%
2023-08-20 02:40:53.278446: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 02:40:53.279667: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 02:40:53.280907: finished real validation
2023-08-20 02:41:19.190508: train_loss -0.8826
2023-08-20 02:41:19.192684: val_loss -0.7184
2023-08-20 02:41:19.194582: Pseudo dice [0.8752]
2023-08-20 02:41:19.195877: Epoch time: 657.53 s
2023-08-20 02:41:19.197087: Yayy! New best EMA pseudo Dice: 0.8691
2023-08-20 02:41:22.663454: 
2023-08-20 02:41:22.665145: Epoch 56
2023-08-20 02:41:22.666506: Current learning rate: 0.00415
2023-08-20 02:41:22.668084: start training, 250
================num of epochs: 250================
2023-08-20 02:48:19.232317: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 02:48:19.539917: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 02:48:19.543256: The split file contains 1 splits.
2023-08-20 02:48:19.544653: Desired fold for training: 0
2023-08-20 02:48:19.546204: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 02:51:56.682121: dsc: 87.24%
2023-08-20 02:51:56.683908: miou: 77.37%
2023-08-20 02:51:56.685195: acc: 95.86%, sen: 84.42%, spe: 98.17%
2023-08-20 02:51:56.687468: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 02:51:56.688714: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 02:51:56.689895: finished real validation
2023-08-20 02:52:22.641235: train_loss -0.8841
2023-08-20 02:52:22.643240: val_loss -0.6934
2023-08-20 02:52:22.645094: Pseudo dice [0.867]
2023-08-20 02:52:22.646539: Epoch time: 659.98 s
2023-08-20 02:52:23.945753: 
2023-08-20 02:52:23.947512: Epoch 57
2023-08-20 02:52:23.948790: Current learning rate: 0.00414
2023-08-20 02:52:23.950463: start training, 250
================num of epochs: 250================
2023-08-20 02:59:19.193149: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 02:59:19.510355: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 02:59:19.513528: The split file contains 1 splits.
2023-08-20 02:59:19.514881: Desired fold for training: 0
2023-08-20 02:59:19.516106: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 03:02:57.888131: dsc: 87.09%
2023-08-20 03:02:57.889901: miou: 77.13%
2023-08-20 03:02:57.891126: acc: 95.84%, sen: 83.75%, spe: 98.27%
2023-08-20 03:02:57.893324: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:02:57.894574: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:02:57.895749: finished real validation
2023-08-20 03:03:23.909466: train_loss -0.885
2023-08-20 03:03:23.911588: val_loss -0.6598
2023-08-20 03:03:23.913237: Pseudo dice [0.8511]
2023-08-20 03:03:23.914567: Epoch time: 659.97 s
2023-08-20 03:03:25.214671: 
2023-08-20 03:03:25.216489: Epoch 58
2023-08-20 03:03:25.217788: Current learning rate: 0.00412
2023-08-20 03:03:25.219403: start training, 250
================num of epochs: 250================
2023-08-20 03:10:22.382840: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 03:10:22.688666: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 03:10:22.691919: The split file contains 1 splits.
2023-08-20 03:10:22.693484: Desired fold for training: 0
2023-08-20 03:10:22.694798: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 03:13:58.098634: dsc: 86.33%
2023-08-20 03:13:58.100341: miou: 75.96%
2023-08-20 03:13:58.101501: acc: 95.68%, sen: 81.46%, spe: 98.54%
2023-08-20 03:13:58.103543: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:13:58.104836: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:13:58.106052: finished real validation
2023-08-20 03:14:24.177525: train_loss -0.882
2023-08-20 03:14:24.179428: val_loss -0.6756
2023-08-20 03:14:24.181083: Pseudo dice [0.8628]
2023-08-20 03:14:24.182595: Epoch time: 658.97 s
2023-08-20 03:14:25.514469: 
2023-08-20 03:14:25.516117: Epoch 59
2023-08-20 03:14:25.517457: Current learning rate: 0.00411
2023-08-20 03:14:25.519069: start training, 250
================num of epochs: 250================
2023-08-20 03:21:21.910246: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 03:21:22.224854: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 03:21:22.228074: The split file contains 1 splits.
2023-08-20 03:21:22.229558: Desired fold for training: 0
2023-08-20 03:21:22.230946: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 03:24:59.433828: dsc: 87.14%
2023-08-20 03:24:59.435556: miou: 77.21%
2023-08-20 03:24:59.436746: acc: 95.86%, sen: 83.84%, spe: 98.27%
2023-08-20 03:24:59.438884: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:24:59.440094: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:24:59.441297: finished real validation
2023-08-20 03:25:25.417958: train_loss -0.8816
2023-08-20 03:25:25.420409: val_loss -0.7348
2023-08-20 03:25:25.422541: Pseudo dice [0.8764]
2023-08-20 03:25:25.423937: Epoch time: 659.91 s
2023-08-20 03:25:26.755561: 
2023-08-20 03:25:26.760293: Epoch 60
2023-08-20 03:25:26.763130: Current learning rate: 0.00409
2023-08-20 03:25:26.764829: start training, 250
================num of epochs: 250================
2023-08-20 03:32:23.571963: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 03:32:23.882211: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 03:32:23.885708: The split file contains 1 splits.
2023-08-20 03:32:23.887224: Desired fold for training: 0
2023-08-20 03:32:23.888577: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 03:36:00.299536: dsc: 87.34%
2023-08-20 03:36:00.301369: miou: 77.53%
2023-08-20 03:36:00.302575: acc: 95.92%, sen: 84.14%, spe: 98.28%
2023-08-20 03:36:00.304728: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:36:00.306068: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:36:00.307253: finished real validation
2023-08-20 03:36:26.149826: train_loss -0.8801
2023-08-20 03:36:26.151576: val_loss -0.7259
2023-08-20 03:36:26.153234: Pseudo dice [0.8775]
2023-08-20 03:36:26.154710: Epoch time: 659.4 s
2023-08-20 03:36:27.479135: 
2023-08-20 03:36:27.480845: Epoch 61
2023-08-20 03:36:27.482131: Current learning rate: 0.00407
2023-08-20 03:36:27.483849: start training, 250
================num of epochs: 250================
2023-08-20 03:43:23.089284: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 03:43:23.394150: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 03:43:23.397403: The split file contains 1 splits.
2023-08-20 03:43:23.398815: Desired fold for training: 0
2023-08-20 03:43:23.400124: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 03:47:01.378332: dsc: 87.55%
2023-08-20 03:47:01.380173: miou: 77.86%
2023-08-20 03:47:01.381448: acc: 95.98%, sen: 84.30%, spe: 98.33%
2023-08-20 03:47:01.383652: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:47:01.384908: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:47:01.386221: finished real validation
2023-08-20 03:47:27.278288: train_loss -0.8809
2023-08-20 03:47:27.280316: val_loss -0.7239
2023-08-20 03:47:27.282132: Pseudo dice [0.8794]
2023-08-20 03:47:27.283567: Epoch time: 659.8 s
2023-08-20 03:47:27.284833: Yayy! New best EMA pseudo Dice: 0.8697
2023-08-20 03:47:30.629877: 
2023-08-20 03:47:30.631767: Epoch 62
2023-08-20 03:47:30.633166: Current learning rate: 0.00406
2023-08-20 03:47:30.634840: start training, 250
================num of epochs: 250================
2023-08-20 03:54:25.953203: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 03:54:26.265625: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 03:54:26.269081: The split file contains 1 splits.
2023-08-20 03:54:26.270644: Desired fold for training: 0
2023-08-20 03:54:26.271978: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 03:58:02.710245: dsc: 86.83%
2023-08-20 03:58:02.712383: miou: 76.73%
2023-08-20 03:58:02.713521: acc: 95.75%, sen: 83.73%, spe: 98.16%
2023-08-20 03:58:02.715893: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:58:02.717256: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 03:58:02.718506: finished real validation
2023-08-20 03:58:28.653975: train_loss -0.8852
2023-08-20 03:58:28.656039: val_loss -0.6897
2023-08-20 03:58:28.657664: Pseudo dice [0.8693]
2023-08-20 03:58:28.658981: Epoch time: 658.03 s
2023-08-20 03:58:29.975189: 
2023-08-20 03:58:29.976939: Epoch 63
2023-08-20 03:58:29.978222: Current learning rate: 0.00404
2023-08-20 03:58:29.979844: start training, 250
================num of epochs: 250================
2023-08-20 04:05:25.488087: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 04:05:25.806759: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 04:05:25.809935: The split file contains 1 splits.
2023-08-20 04:05:25.811425: Desired fold for training: 0
2023-08-20 04:05:25.812785: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 04:09:03.016766: dsc: 87.05%
2023-08-20 04:09:03.018778: miou: 77.07%
2023-08-20 04:09:03.020031: acc: 95.79%, sen: 84.48%, spe: 98.07%
2023-08-20 04:09:03.022281: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:09:03.023623: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:09:03.024876: finished real validation
2023-08-20 04:09:28.853031: train_loss -0.8857
2023-08-20 04:09:28.854886: val_loss -0.6581
2023-08-20 04:09:28.856637: Pseudo dice [0.8586]
2023-08-20 04:09:28.858035: Epoch time: 658.88 s
2023-08-20 04:09:30.203361: 
2023-08-20 04:09:30.205226: Epoch 64
2023-08-20 04:09:30.206587: Current learning rate: 0.00403
2023-08-20 04:09:30.208373: start training, 250
================num of epochs: 250================
2023-08-20 04:16:25.523680: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 04:16:25.841820: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 04:16:25.845126: The split file contains 1 splits.
2023-08-20 04:16:25.846458: Desired fold for training: 0
2023-08-20 04:16:25.847927: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 04:20:02.446793: dsc: 87.48%
2023-08-20 04:20:02.448634: miou: 77.74%
2023-08-20 04:20:02.449908: acc: 95.88%, sen: 85.95%, spe: 97.88%
2023-08-20 04:20:02.451946: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:20:02.453138: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:20:02.454407: finished real validation
2023-08-20 04:20:28.379570: train_loss -0.8871
2023-08-20 04:20:28.381435: val_loss -0.7006
2023-08-20 04:20:28.383347: Pseudo dice [0.8729]
2023-08-20 04:20:28.384825: Epoch time: 658.18 s
2023-08-20 04:20:29.875711: 
2023-08-20 04:20:29.877745: Epoch 65
2023-08-20 04:20:29.879146: Current learning rate: 0.00401
2023-08-20 04:20:29.880806: start training, 250
================num of epochs: 250================
2023-08-20 04:27:25.443128: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 04:27:25.759858: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 04:27:25.763264: The split file contains 1 splits.
2023-08-20 04:27:25.764758: Desired fold for training: 0
2023-08-20 04:27:25.766104: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 04:31:03.413373: dsc: 87.06%
2023-08-20 04:31:03.416370: miou: 77.08%
2023-08-20 04:31:03.417528: acc: 95.79%, sen: 84.56%, spe: 98.05%
2023-08-20 04:31:03.419684: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:31:03.420982: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:31:03.422238: finished real validation
2023-08-20 04:31:29.209923: train_loss -0.8838
2023-08-20 04:31:29.211751: val_loss -0.7132
2023-08-20 04:31:29.213390: Pseudo dice [0.8735]
2023-08-20 04:31:29.214757: Epoch time: 659.34 s
2023-08-20 04:31:30.535747: 
2023-08-20 04:31:30.537546: Epoch 66
2023-08-20 04:31:30.538863: Current learning rate: 0.004
2023-08-20 04:31:30.540465: start training, 250
================num of epochs: 250================
2023-08-20 04:38:26.786976: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 04:38:27.171380: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 04:38:27.174532: The split file contains 1 splits.
2023-08-20 04:38:27.176139: Desired fold for training: 0
2023-08-20 04:38:27.177695: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 04:42:44.144938: dsc: 86.81%
2023-08-20 04:42:44.146923: miou: 76.69%
2023-08-20 04:42:44.148319: acc: 95.80%, sen: 82.43%, spe: 98.49%
2023-08-20 04:42:44.150624: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:42:44.151867: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:42:44.153143: finished real validation
2023-08-20 04:43:10.310095: train_loss -0.8839
2023-08-20 04:43:10.312232: val_loss -0.697
2023-08-20 04:43:10.314157: Pseudo dice [0.8646]
2023-08-20 04:43:10.315710: Epoch time: 699.78 s
2023-08-20 04:43:11.648002: 
2023-08-20 04:43:11.649843: Epoch 67
2023-08-20 04:43:11.651243: Current learning rate: 0.00398
2023-08-20 04:43:11.653004: start training, 250
================num of epochs: 250================
2023-08-20 04:50:06.892656: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 04:50:07.207968: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 04:50:07.211229: The split file contains 1 splits.
2023-08-20 04:50:07.212766: Desired fold for training: 0
2023-08-20 04:50:07.214169: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 04:53:44.171565: dsc: 87.30%
2023-08-20 04:53:44.173371: miou: 77.46%
2023-08-20 04:53:44.174583: acc: 96.00%, sen: 82.15%, spe: 98.78%
2023-08-20 04:53:44.176788: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:53:44.178173: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 04:53:44.179444: finished real validation
2023-08-20 04:54:10.201395: train_loss -0.8865
2023-08-20 04:54:53.593681: val_loss -0.7371
2023-08-20 04:54:53.596285: Pseudo dice [0.8815]
2023-08-20 04:54:53.597731: Epoch time: 658.56 s
2023-08-20 04:54:53.599238: Yayy! New best EMA pseudo Dice: 0.8702
2023-08-20 04:54:57.016973: 
2023-08-20 04:54:57.021999: Epoch 68
2023-08-20 04:54:57.026916: Current learning rate: 0.00397
2023-08-20 04:54:57.032757: start training, 250
================num of epochs: 250================
2023-08-20 05:01:52.051990: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 05:01:52.361855: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 05:01:52.365271: The split file contains 1 splits.
2023-08-20 05:01:52.366795: Desired fold for training: 0
2023-08-20 05:01:52.368351: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 05:05:29.007685: dsc: 87.11%
2023-08-20 05:05:29.009492: miou: 77.16%
2023-08-20 05:05:29.010690: acc: 95.86%, sen: 83.60%, spe: 98.32%
2023-08-20 05:05:29.012911: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:05:29.014259: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:05:29.015512: finished real validation
2023-08-20 05:05:54.846659: train_loss -0.8881
2023-08-20 05:05:54.848701: val_loss -0.6746
2023-08-20 05:05:54.850812: Pseudo dice [0.868]
2023-08-20 05:05:54.852309: Epoch time: 657.83 s
2023-08-20 05:05:56.192887: 
2023-08-20 05:05:56.194591: Epoch 69
2023-08-20 05:05:56.195943: Current learning rate: 0.00395
2023-08-20 05:05:56.197702: start training, 250
================num of epochs: 250================
2023-08-20 05:12:51.903090: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 05:12:52.224759: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 05:12:52.228166: The split file contains 1 splits.
2023-08-20 05:12:52.229672: Desired fold for training: 0
2023-08-20 05:12:52.231220: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 05:16:30.684107: dsc: 87.28%
2023-08-20 05:16:30.685974: miou: 77.43%
2023-08-20 05:16:30.687237: acc: 95.77%, sen: 86.58%, spe: 97.62%
2023-08-20 05:16:30.689434: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:16:30.691058: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:16:30.692313: finished real validation
2023-08-20 05:16:57.304619: train_loss -0.8881
2023-08-20 05:16:57.306954: val_loss -0.7372
2023-08-20 05:16:57.308907: Pseudo dice [0.8802]
2023-08-20 05:16:57.310374: Epoch time: 661.11 s
2023-08-20 05:16:57.311645: Yayy! New best EMA pseudo Dice: 0.871
2023-08-20 05:17:00.645893: 
2023-08-20 05:17:00.647707: Epoch 70
2023-08-20 05:17:00.649037: Current learning rate: 0.00394
2023-08-20 05:17:00.650707: start training, 250
================num of epochs: 250================
2023-08-20 05:23:56.244971: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 05:23:56.563008: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 05:23:56.566257: The split file contains 1 splits.
2023-08-20 05:23:56.567709: Desired fold for training: 0
2023-08-20 05:23:56.569029: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 05:27:33.795471: dsc: 86.25%
2023-08-20 05:27:33.797159: miou: 75.82%
2023-08-20 05:27:33.798608: acc: 95.69%, sen: 80.69%, spe: 98.71%
2023-08-20 05:27:33.800765: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:27:33.802158: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:27:33.803415: finished real validation
2023-08-20 05:27:59.476522: train_loss -0.8859
2023-08-20 05:27:59.478586: val_loss -0.7022
2023-08-20 05:27:59.480486: Pseudo dice [0.8601]
2023-08-20 05:27:59.481920: Epoch time: 658.83 s
2023-08-20 05:28:00.809523: 
2023-08-20 05:28:00.811125: Epoch 71
2023-08-20 05:28:00.812424: Current learning rate: 0.00392
2023-08-20 05:28:00.814129: start training, 250
================num of epochs: 250================
2023-08-20 05:34:56.672768: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 05:34:56.974555: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 05:34:56.978190: The split file contains 1 splits.
2023-08-20 05:34:56.979609: Desired fold for training: 0
2023-08-20 05:34:56.980947: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 05:38:34.852096: dsc: 87.23%
2023-08-20 05:38:34.853943: miou: 77.36%
2023-08-20 05:38:34.855201: acc: 95.88%, sen: 84.08%, spe: 98.25%
2023-08-20 05:38:34.857327: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:38:34.858555: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:38:34.859777: finished real validation
2023-08-20 05:39:00.580229: train_loss -0.8875
2023-08-20 05:39:00.582078: val_loss -0.6947
2023-08-20 05:39:00.583714: Pseudo dice [0.878]
2023-08-20 05:39:00.585143: Epoch time: 659.77 s
2023-08-20 05:39:01.911680: 
2023-08-20 05:39:01.913474: Epoch 72
2023-08-20 05:39:01.914732: Current learning rate: 0.00391
2023-08-20 05:39:01.916416: start training, 250
================num of epochs: 250================
2023-08-20 05:45:56.909394: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 05:45:57.220743: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 05:45:57.223948: The split file contains 1 splits.
2023-08-20 05:45:57.225487: Desired fold for training: 0
2023-08-20 05:45:57.226845: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 05:49:35.968396: dsc: 87.59%
2023-08-20 05:49:35.970233: miou: 77.92%
2023-08-20 05:49:35.971537: acc: 95.93%, sen: 85.81%, spe: 97.96%
2023-08-20 05:49:35.973701: current best miou: 0.7792752711407567 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:49:35.975016: current best dsc: 0.8759468349617826 at epoch: 54, (54, 0.7792752711407567, 0.8759468349617826)
2023-08-20 05:49:35.976305: finished real validation
2023-08-20 05:50:01.507162: train_loss -0.8926
2023-08-20 05:50:01.509336: val_loss -0.6818
2023-08-20 05:50:01.511231: Pseudo dice [0.8695]
2023-08-20 05:50:01.512848: Epoch time: 659.6 s
2023-08-20 05:50:02.847451: 
2023-08-20 05:50:02.849302: Epoch 73
2023-08-20 05:50:02.850712: Current learning rate: 0.00389
2023-08-20 05:50:02.852502: start training, 250
================num of epochs: 250================
2023-08-20 05:56:57.797653: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 05:56:58.114555: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 05:56:58.117893: The split file contains 1 splits.
2023-08-20 05:56:58.119411: Desired fold for training: 0
2023-08-20 05:56:58.120780: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 06:00:33.286700: dsc: 87.80%
2023-08-20 06:00:33.289015: miou: 78.25%
2023-08-20 06:00:33.290461: acc: 96.04%, sen: 85.14%, spe: 98.23%
2023-08-20 06:00:33.292998: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:00:33.294360: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:00:35.328765: finished real validation
2023-08-20 06:01:01.185315: train_loss -0.8921
2023-08-20 06:01:01.187457: val_loss -0.7205
2023-08-20 06:01:01.189483: Pseudo dice [0.8722]
2023-08-20 06:01:01.190936: Epoch time: 658.34 s
2023-08-20 06:01:02.529509: 
2023-08-20 06:01:02.531707: Epoch 74
2023-08-20 06:01:02.533603: Current learning rate: 0.00387
2023-08-20 06:01:02.535342: start training, 250
================num of epochs: 250================
2023-08-20 06:07:57.403290: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 06:07:57.718512: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 06:07:57.721670: The split file contains 1 splits.
2023-08-20 06:07:57.723127: Desired fold for training: 0
2023-08-20 06:07:57.724451: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 06:11:33.467564: dsc: 87.33%
2023-08-20 06:11:33.469373: miou: 77.51%
2023-08-20 06:11:33.470666: acc: 95.91%, sen: 84.09%, spe: 98.29%
2023-08-20 06:11:33.472780: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:11:33.474098: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:11:33.475318: finished real validation
2023-08-20 06:11:59.262562: train_loss -0.8924
2023-08-20 06:11:59.264514: val_loss -0.6832
2023-08-20 06:11:59.266147: Pseudo dice [0.8708]
2023-08-20 06:11:59.267551: Epoch time: 656.74 s
2023-08-20 06:12:00.598460: 
2023-08-20 06:12:00.600301: Epoch 75
2023-08-20 06:12:00.601615: Current learning rate: 0.00386
2023-08-20 06:12:00.603240: start training, 250
================num of epochs: 250================
2023-08-20 06:18:56.846637: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 06:18:57.161803: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 06:18:57.165002: The split file contains 1 splits.
2023-08-20 06:18:57.166529: Desired fold for training: 0
2023-08-20 06:18:57.168001: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 06:22:32.690468: dsc: 87.05%
2023-08-20 06:22:32.693371: miou: 77.07%
2023-08-20 06:22:32.695601: acc: 95.79%, sen: 84.57%, spe: 98.04%
2023-08-20 06:22:32.698882: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:22:32.701045: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:22:32.703288: finished real validation
2023-08-20 06:22:58.460415: train_loss -0.8924
2023-08-20 06:22:58.462540: val_loss -0.6631
2023-08-20 06:22:58.464413: Pseudo dice [0.8696]
2023-08-20 06:22:58.465822: Epoch time: 657.86 s
2023-08-20 06:22:59.797789: 
2023-08-20 06:22:59.799568: Epoch 76
2023-08-20 06:22:59.800894: Current learning rate: 0.00384
2023-08-20 06:22:59.802608: start training, 250
================num of epochs: 250================
2023-08-20 06:29:54.979830: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 06:29:55.274680: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 06:29:55.277923: The split file contains 1 splits.
2023-08-20 06:29:55.279395: Desired fold for training: 0
2023-08-20 06:29:55.280879: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 06:33:33.169028: dsc: 87.09%
2023-08-20 06:33:33.170838: miou: 77.13%
2023-08-20 06:33:33.172156: acc: 95.79%, sen: 84.69%, spe: 98.03%
2023-08-20 06:33:33.174490: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:33:33.175817: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:33:33.177089: finished real validation
2023-08-20 06:33:59.182979: train_loss -0.8927
2023-08-20 06:33:59.185034: val_loss -0.685
2023-08-20 06:33:59.187020: Pseudo dice [0.8659]
2023-08-20 06:33:59.188521: Epoch time: 659.39 s
2023-08-20 06:34:00.530497: 
2023-08-20 06:34:00.532212: Epoch 77
2023-08-20 06:34:00.533576: Current learning rate: 0.00383
2023-08-20 06:34:00.535402: start training, 250
================num of epochs: 250================
2023-08-20 06:40:55.483747: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 06:40:55.782426: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 06:40:55.785738: The split file contains 1 splits.
2023-08-20 06:40:55.787518: Desired fold for training: 0
2023-08-20 06:40:55.789055: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 06:44:33.816043: dsc: 86.94%
2023-08-20 06:44:33.818158: miou: 76.89%
2023-08-20 06:44:33.819773: acc: 95.84%, sen: 82.58%, spe: 98.51%
2023-08-20 06:44:33.822024: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:44:33.823361: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:44:33.824663: finished real validation
2023-08-20 06:44:59.841126: train_loss -0.8926
2023-08-20 06:44:59.843080: val_loss -0.6504
2023-08-20 06:44:59.844828: Pseudo dice [0.8523]
2023-08-20 06:44:59.846386: Epoch time: 659.31 s
2023-08-20 06:45:01.188788: 
2023-08-20 06:45:01.190589: Epoch 78
2023-08-20 06:45:01.192003: Current learning rate: 0.00381
2023-08-20 06:45:01.193775: start training, 250
================num of epochs: 250================
2023-08-20 06:51:55.737311: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 06:51:56.050614: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 06:51:56.053866: The split file contains 1 splits.
2023-08-20 06:51:56.055344: Desired fold for training: 0
2023-08-20 06:51:56.056703: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 06:55:37.442357: dsc: 87.04%
2023-08-20 06:55:37.444251: miou: 77.06%
2023-08-20 06:55:37.445549: acc: 95.76%, sen: 85.04%, spe: 97.92%
2023-08-20 06:55:37.447732: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:55:37.449126: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 06:55:37.450497: finished real validation
2023-08-20 06:56:03.343345: train_loss -0.8921
2023-08-20 06:56:03.345516: val_loss -0.6903
2023-08-20 06:56:03.347647: Pseudo dice [0.8656]
2023-08-20 06:56:03.349218: Epoch time: 662.16 s
2023-08-20 06:56:04.704354: 
2023-08-20 06:56:04.706205: Epoch 79
2023-08-20 06:56:04.707689: Current learning rate: 0.0038
2023-08-20 06:56:04.709456: start training, 250
================num of epochs: 250================
2023-08-20 07:03:00.371698: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 07:03:00.692799: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 07:03:00.695953: The split file contains 1 splits.
2023-08-20 07:03:00.697496: Desired fold for training: 0
2023-08-20 07:03:00.698947: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 07:06:37.269345: dsc: 87.10%
2023-08-20 07:06:37.271395: miou: 77.15%
2023-08-20 07:06:37.272913: acc: 95.80%, sen: 84.70%, spe: 98.03%
2023-08-20 07:06:37.275045: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:06:37.276409: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:06:37.277861: finished real validation
2023-08-20 07:07:03.041554: train_loss -0.8932
2023-08-20 07:07:03.043713: val_loss -0.697
2023-08-20 07:07:03.045659: Pseudo dice [0.8726]
2023-08-20 07:07:03.047251: Epoch time: 658.34 s
2023-08-20 07:07:04.399895: 
2023-08-20 07:07:04.401932: Epoch 80
2023-08-20 07:07:04.403294: Current learning rate: 0.00378
2023-08-20 07:07:04.405016: start training, 250
================num of epochs: 250================
2023-08-20 07:13:59.976713: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 07:14:00.287754: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 07:14:00.291014: The split file contains 1 splits.
2023-08-20 07:14:00.292736: Desired fold for training: 0
2023-08-20 07:14:00.294257: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 07:17:36.405636: dsc: 87.44%
2023-08-20 07:17:36.407652: miou: 77.69%
2023-08-20 07:17:36.409043: acc: 95.95%, sen: 84.10%, spe: 98.34%
2023-08-20 07:17:36.411428: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:17:36.413045: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:17:36.414404: finished real validation
2023-08-20 07:18:02.238757: train_loss -0.895
2023-08-20 07:18:02.240944: val_loss -0.6852
2023-08-20 07:18:02.242935: Pseudo dice [0.8602]
2023-08-20 07:18:02.244406: Epoch time: 657.84 s
2023-08-20 07:18:03.605796: 
2023-08-20 07:18:03.607774: Epoch 81
2023-08-20 07:18:03.609216: Current learning rate: 0.00377
2023-08-20 07:18:03.611012: start training, 250
================num of epochs: 250================
2023-08-20 07:24:58.901653: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 07:24:59.210521: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 07:24:59.213707: The split file contains 1 splits.
2023-08-20 07:24:59.215426: Desired fold for training: 0
2023-08-20 07:24:59.217073: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 07:28:37.482954: dsc: 86.91%
2023-08-20 07:28:37.485018: miou: 76.85%
2023-08-20 07:28:37.486252: acc: 95.85%, sen: 82.27%, spe: 98.58%
2023-08-20 07:28:37.488445: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:28:37.489815: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:28:37.491076: finished real validation
2023-08-20 07:29:03.488555: train_loss -0.8931
2023-08-20 07:29:03.490642: val_loss -0.7143
2023-08-20 07:29:03.492732: Pseudo dice [0.8711]
2023-08-20 07:29:03.494285: Epoch time: 659.89 s
2023-08-20 07:29:04.841458: 
2023-08-20 07:29:04.843273: Epoch 82
2023-08-20 07:29:04.844640: Current learning rate: 0.00375
2023-08-20 07:29:04.846411: start training, 250
================num of epochs: 250================
2023-08-20 07:36:00.705060: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 07:36:01.022837: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 07:36:01.026445: The split file contains 1 splits.
2023-08-20 07:36:01.028032: Desired fold for training: 0
2023-08-20 07:36:01.029535: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 07:39:37.249138: dsc: 87.49%
2023-08-20 07:39:37.250947: miou: 77.76%
2023-08-20 07:39:37.252276: acc: 96.01%, sen: 83.30%, spe: 98.57%
2023-08-20 07:39:37.254392: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:39:37.255797: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:39:37.257189: finished real validation
2023-08-20 07:40:03.000553: train_loss -0.894
2023-08-20 07:40:03.002705: val_loss -0.7118
2023-08-20 07:40:03.004758: Pseudo dice [0.8682]
2023-08-20 07:40:03.006315: Epoch time: 658.16 s
2023-08-20 07:40:04.306722: 
2023-08-20 07:40:04.308570: Epoch 83
2023-08-20 07:40:04.309942: Current learning rate: 0.00374
2023-08-20 07:40:04.311692: start training, 250
================num of epochs: 250================
2023-08-20 07:46:59.903817: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 07:47:00.220426: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 07:47:00.223737: The split file contains 1 splits.
2023-08-20 07:47:00.225370: Desired fold for training: 0
2023-08-20 07:47:00.227010: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 07:50:38.375010: dsc: 86.99%
2023-08-20 07:50:38.377040: miou: 76.98%
2023-08-20 07:50:38.378448: acc: 95.83%, sen: 83.26%, spe: 98.36%
2023-08-20 07:50:38.380747: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:50:38.382125: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 07:50:38.383418: finished real validation
2023-08-20 07:51:04.506426: train_loss -0.893
2023-08-20 07:51:04.508481: val_loss -0.7018
2023-08-20 07:51:04.510268: Pseudo dice [0.864]
2023-08-20 07:51:04.511757: Epoch time: 660.2 s
2023-08-20 07:51:05.809014: 
2023-08-20 07:51:05.810986: Epoch 84
2023-08-20 07:51:05.812434: Current learning rate: 0.00372
2023-08-20 07:51:05.814227: start training, 250
================num of epochs: 250================
2023-08-20 07:58:01.354946: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 07:58:01.664207: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 07:58:01.667506: The split file contains 1 splits.
2023-08-20 07:58:01.669204: Desired fold for training: 0
2023-08-20 07:58:01.670672: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 08:01:37.175943: dsc: 87.56%
2023-08-20 08:01:37.177953: miou: 77.87%
2023-08-20 08:01:37.179289: acc: 95.89%, sen: 86.27%, spe: 97.83%
2023-08-20 08:01:37.181590: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:01:37.182876: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:01:37.184207: finished real validation
2023-08-20 08:02:03.096419: train_loss -0.8926
2023-08-20 08:02:03.098687: val_loss -0.7087
2023-08-20 08:02:03.100710: Pseudo dice [0.8701]
2023-08-20 08:02:03.102270: Epoch time: 657.29 s
2023-08-20 08:02:04.403032: 
2023-08-20 08:02:04.405051: Epoch 85
2023-08-20 08:02:04.406499: Current learning rate: 0.0037
2023-08-20 08:02:04.408408: start training, 250
================num of epochs: 250================
2023-08-20 08:08:59.119596: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 08:08:59.431109: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 08:08:59.434381: The split file contains 1 splits.
2023-08-20 08:08:59.435945: Desired fold for training: 0
2023-08-20 08:08:59.437325: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 08:12:37.212513: dsc: 87.09%
2023-08-20 08:12:37.214402: miou: 77.13%
2023-08-20 08:12:37.215691: acc: 95.83%, sen: 84.08%, spe: 98.19%
2023-08-20 08:12:37.218007: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:12:37.219359: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:12:37.220677: finished real validation
2023-08-20 08:13:03.192299: train_loss -0.8967
2023-08-20 08:13:03.194169: val_loss -0.7073
2023-08-20 08:13:03.196001: Pseudo dice [0.8721]
2023-08-20 08:13:03.197584: Epoch time: 658.79 s
2023-08-20 08:13:04.488374: 
2023-08-20 08:13:04.490302: Epoch 86
2023-08-20 08:13:04.491748: Current learning rate: 0.00369
2023-08-20 08:13:04.493481: start training, 250
================num of epochs: 250================
2023-08-20 08:19:59.759217: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 08:20:00.072217: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 08:20:00.075483: The split file contains 1 splits.
2023-08-20 08:20:00.076946: Desired fold for training: 0
2023-08-20 08:20:00.078388: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 08:23:36.572253: dsc: 87.43%
2023-08-20 08:23:36.574239: miou: 77.67%
2023-08-20 08:23:36.575671: acc: 95.89%, sen: 85.35%, spe: 98.01%
2023-08-20 08:23:36.577926: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:23:36.579258: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:23:36.580547: finished real validation
2023-08-20 08:24:02.511548: train_loss -0.8964
2023-08-20 08:24:02.513487: val_loss -0.708
2023-08-20 08:24:02.515248: Pseudo dice [0.8703]
2023-08-20 08:24:02.516701: Epoch time: 658.03 s
2023-08-20 08:24:03.813926: 
2023-08-20 08:24:03.815842: Epoch 87
2023-08-20 08:24:03.817630: Current learning rate: 0.00367
2023-08-20 08:24:03.819691: start training, 250
================num of epochs: 250================
2023-08-20 08:30:59.016150: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 08:30:59.320772: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 08:30:59.324269: The split file contains 1 splits.
2023-08-20 08:30:59.325935: Desired fold for training: 0
2023-08-20 08:30:59.327502: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 08:34:36.065408: dsc: 87.52%
2023-08-20 08:34:36.068262: miou: 77.82%
2023-08-20 08:34:36.069776: acc: 95.91%, sen: 85.72%, spe: 97.96%
2023-08-20 08:34:36.072189: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:34:36.074008: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:34:36.075324: finished real validation
2023-08-20 08:35:01.984143: train_loss -0.8962
2023-08-20 08:35:01.986388: val_loss -0.6988
2023-08-20 08:35:01.988448: Pseudo dice [0.8754]
2023-08-20 08:35:01.990382: Epoch time: 658.17 s
2023-08-20 08:35:03.287374: 
2023-08-20 08:35:03.289238: Epoch 88
2023-08-20 08:35:03.291334: Current learning rate: 0.00366
2023-08-20 08:35:03.293741: start training, 250
================num of epochs: 250================
2023-08-20 08:41:59.215786: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 08:41:59.601686: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 08:41:59.605005: The split file contains 1 splits.
2023-08-20 08:41:59.607398: Desired fold for training: 0
2023-08-20 08:41:59.609436: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 08:46:19.855625: dsc: 87.30%
2023-08-20 08:46:19.899158: miou: 77.46%
2023-08-20 08:46:19.901089: acc: 95.91%, sen: 83.92%, spe: 98.32%
2023-08-20 08:46:19.903551: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:46:19.905343: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:46:19.907169: finished real validation
2023-08-20 08:46:45.962629: train_loss -0.8976
2023-08-20 08:46:45.964827: val_loss -0.7231
2023-08-20 08:46:45.967147: Pseudo dice [0.8802]
2023-08-20 08:46:45.968691: Epoch time: 702.68 s
2023-08-20 08:46:47.278171: 
2023-08-20 08:46:47.280381: Epoch 89
2023-08-20 08:46:47.281888: Current learning rate: 0.00364
2023-08-20 08:46:47.284218: start training, 250
================num of epochs: 250================
2023-08-20 08:53:42.587913: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 08:53:42.906956: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 08:53:42.910998: The split file contains 1 splits.
2023-08-20 08:53:42.912879: Desired fold for training: 0
2023-08-20 08:53:42.915171: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 08:57:21.325820: dsc: 87.11%
2023-08-20 08:57:21.328454: miou: 77.17%
2023-08-20 08:57:21.330042: acc: 95.88%, sen: 83.15%, spe: 98.44%
2023-08-20 08:57:21.333107: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:57:21.334465: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 08:57:21.335968: finished real validation
2023-08-20 08:57:47.279441: train_loss -0.8972
2023-08-20 08:57:47.281734: val_loss -0.6853
2023-08-20 08:57:47.284139: Pseudo dice [0.8714]
2023-08-20 08:57:47.285764: Epoch time: 660.0 s
2023-08-20 08:57:48.599006: 
2023-08-20 08:57:48.601027: Epoch 90
2023-08-20 08:57:48.602410: Current learning rate: 0.00363
2023-08-20 08:57:48.604167: start training, 250
================num of epochs: 250================
2023-08-20 09:04:45.190561: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 09:04:45.529530: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 09:04:45.533116: The split file contains 1 splits.
2023-08-20 09:04:45.534832: Desired fold for training: 0
2023-08-20 09:04:45.536537: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 09:08:23.985547: dsc: 87.63%
2023-08-20 09:08:23.987653: miou: 77.98%
2023-08-20 09:08:23.989034: acc: 95.94%, sen: 85.83%, spe: 97.97%
2023-08-20 09:08:23.991643: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:08:23.993088: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:08:23.994516: finished real validation
2023-08-20 09:08:49.936090: train_loss -0.8984
2023-08-20 09:08:49.939097: val_loss -0.6919
2023-08-20 09:08:49.941688: Pseudo dice [0.8646]
2023-08-20 09:08:49.943547: Epoch time: 661.34 s
2023-08-20 09:08:51.260132: 
2023-08-20 09:08:51.298340: Epoch 91
2023-08-20 09:08:51.322567: Current learning rate: 0.00361
2023-08-20 09:08:51.346542: start training, 250
================num of epochs: 250================
2023-08-20 09:15:47.703421: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 09:15:48.014942: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 09:15:48.018351: The split file contains 1 splits.
2023-08-20 09:15:48.019964: Desired fold for training: 0
2023-08-20 09:15:48.021499: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 09:19:27.118306: dsc: 87.49%
2023-08-20 09:19:27.120422: miou: 77.76%
2023-08-20 09:19:27.121784: acc: 95.87%, sen: 86.15%, spe: 97.83%
2023-08-20 09:19:27.124457: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:19:27.126289: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:19:27.127775: finished real validation
2023-08-20 09:19:53.079989: train_loss -0.8975
2023-08-20 09:19:53.082248: val_loss -0.6805
2023-08-20 09:19:53.084147: Pseudo dice [0.8693]
2023-08-20 09:19:53.085650: Epoch time: 661.82 s
2023-08-20 09:19:54.395811: 
2023-08-20 09:19:54.397676: Epoch 92
2023-08-20 09:19:54.399140: Current learning rate: 0.0036
2023-08-20 09:19:54.400887: start training, 250
================num of epochs: 250================
2023-08-20 09:26:50.230537: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 09:26:50.544195: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 09:26:50.547880: The split file contains 1 splits.
2023-08-20 09:26:50.549718: Desired fold for training: 0
2023-08-20 09:26:50.551279: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 09:30:27.148132: dsc: 86.90%
2023-08-20 09:30:27.170473: miou: 76.84%
2023-08-20 09:30:27.191644: acc: 95.78%, sen: 83.57%, spe: 98.24%
2023-08-20 09:30:27.216724: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:30:27.239794: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:30:27.264283: finished real validation
2023-08-20 09:30:53.251122: train_loss -0.8996
2023-08-20 09:30:53.253437: val_loss -0.6839
2023-08-20 09:30:53.255557: Pseudo dice [0.8643]
2023-08-20 09:30:53.257372: Epoch time: 658.86 s
2023-08-20 09:30:54.565033: 
2023-08-20 09:30:54.566900: Epoch 93
2023-08-20 09:30:54.568429: Current learning rate: 0.00358
2023-08-20 09:30:54.570124: start training, 250
================num of epochs: 250================
2023-08-20 09:37:51.058732: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 09:37:51.371228: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 09:37:51.375077: The split file contains 1 splits.
2023-08-20 09:37:51.376730: Desired fold for training: 0
2023-08-20 09:37:51.378595: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 09:41:27.976043: dsc: 87.11%
2023-08-20 09:41:27.992891: miou: 77.17%
2023-08-20 09:41:27.994904: acc: 95.82%, sen: 84.36%, spe: 98.13%
2023-08-20 09:41:27.997993: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:41:27.999591: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:41:28.001126: finished real validation
2023-08-20 09:41:53.889996: train_loss -0.8968
2023-08-20 09:41:53.892021: val_loss -0.7277
2023-08-20 09:41:53.894080: Pseudo dice [0.8762]
2023-08-20 09:41:53.895664: Epoch time: 659.33 s
2023-08-20 09:41:55.189072: 
2023-08-20 09:41:55.190943: Epoch 94
2023-08-20 09:41:55.192474: Current learning rate: 0.00356
2023-08-20 09:41:55.194340: start training, 250
================num of epochs: 250================
2023-08-20 09:48:52.306700: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 09:48:52.676685: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 09:48:52.680251: The split file contains 1 splits.
2023-08-20 09:48:52.681782: Desired fold for training: 0
2023-08-20 09:48:52.683374: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 09:52:30.112093: dsc: 87.28%
2023-08-20 09:52:30.114088: miou: 77.44%
2023-08-20 09:52:30.115498: acc: 95.84%, sen: 85.26%, spe: 97.97%
2023-08-20 09:52:30.118022: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:52:30.119400: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 09:52:30.121150: finished real validation
2023-08-20 09:52:55.990750: train_loss -0.8964
2023-08-20 09:52:55.992975: val_loss -0.6946
2023-08-20 09:52:55.994952: Pseudo dice [0.8725]
2023-08-20 09:52:55.996957: Epoch time: 660.8 s
2023-08-20 09:52:57.291517: 
2023-08-20 09:52:57.293254: Epoch 95
2023-08-20 09:52:57.294681: Current learning rate: 0.00355
2023-08-20 09:52:57.296489: start training, 250
================num of epochs: 250================
2023-08-20 09:59:53.713644: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 09:59:54.116626: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 09:59:54.119888: The split file contains 1 splits.
2023-08-20 09:59:54.121463: Desired fold for training: 0
2023-08-20 09:59:54.123013: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 10:03:30.506462: dsc: 87.28%
2023-08-20 10:03:30.546596: miou: 77.43%
2023-08-20 10:03:30.548062: acc: 95.81%, sen: 85.88%, spe: 97.80%
2023-08-20 10:03:30.550554: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:03:30.551922: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:03:30.553187: finished real validation
2023-08-20 10:03:56.640859: train_loss -0.8986
2023-08-20 10:03:56.642957: val_loss -0.7052
2023-08-20 10:03:56.644899: Pseudo dice [0.8706]
2023-08-20 10:03:56.646505: Epoch time: 659.35 s
2023-08-20 10:03:57.942632: 
2023-08-20 10:03:57.944479: Epoch 96
2023-08-20 10:03:57.945899: Current learning rate: 0.00353
2023-08-20 10:03:57.947989: start training, 250
================num of epochs: 250================
2023-08-20 10:10:54.021156: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 10:10:54.325158: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 10:10:54.328429: The split file contains 1 splits.
2023-08-20 10:10:54.330144: Desired fold for training: 0
2023-08-20 10:10:54.331832: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 10:14:31.003057: dsc: 87.50%
2023-08-20 10:14:31.006329: miou: 77.78%
2023-08-20 10:14:31.008261: acc: 95.93%, sen: 85.13%, spe: 98.10%
2023-08-20 10:14:31.010409: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:14:31.012568: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:14:31.016510: finished real validation
2023-08-20 10:14:57.075652: train_loss -0.8995
2023-08-20 10:14:57.078091: val_loss -0.7165
2023-08-20 10:14:57.080813: Pseudo dice [0.8765]
2023-08-20 10:14:57.084814: Epoch time: 659.14 s
2023-08-20 10:14:58.454923: 
2023-08-20 10:14:58.456924: Epoch 97
2023-08-20 10:14:58.458628: Current learning rate: 0.00352
2023-08-20 10:14:58.460547: start training, 250
================num of epochs: 250================
2023-08-20 10:21:55.064743: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 10:21:55.376157: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 10:21:55.379657: The split file contains 1 splits.
2023-08-20 10:21:55.382135: Desired fold for training: 0
2023-08-20 10:21:55.384553: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 10:25:33.901452: dsc: 87.62%
2023-08-20 10:25:33.905059: miou: 77.97%
2023-08-20 10:25:33.908889: acc: 96.00%, sen: 84.40%, spe: 98.34%
2023-08-20 10:25:33.916766: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:25:33.920507: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:25:33.923534: finished real validation
2023-08-20 10:25:59.990382: train_loss -0.8981
2023-08-20 10:25:59.993861: val_loss -0.7054
2023-08-20 10:25:59.997925: Pseudo dice [0.8724]
2023-08-20 10:26:00.002455: Epoch time: 661.54 s
2023-08-20 10:26:01.330926: 
2023-08-20 10:26:01.334944: Epoch 98
2023-08-20 10:26:01.336992: Current learning rate: 0.0035
2023-08-20 10:26:01.340065: start training, 250
================num of epochs: 250================
2023-08-20 10:32:57.678956: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 10:32:57.994976: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 10:32:57.998257: The split file contains 1 splits.
2023-08-20 10:32:57.999825: Desired fold for training: 0
2023-08-20 10:32:58.001415: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 10:36:35.501715: dsc: 86.43%
2023-08-20 10:36:35.503979: miou: 76.10%
2023-08-20 10:36:35.505388: acc: 95.67%, sen: 82.36%, spe: 98.35%
2023-08-20 10:36:35.507678: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:36:35.509067: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:36:35.510467: finished real validation
2023-08-20 10:37:01.663023: train_loss -0.8981
2023-08-20 10:37:01.665350: val_loss -0.6812
2023-08-20 10:37:01.667485: Pseudo dice [0.8653]
2023-08-20 10:37:01.669034: Epoch time: 660.33 s
2023-08-20 10:37:03.002689: 
2023-08-20 10:37:03.004568: Epoch 99
2023-08-20 10:37:03.006122: Current learning rate: 0.00349
2023-08-20 10:37:03.007990: start training, 250
================num of epochs: 250================
2023-08-20 10:43:59.382072: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 10:43:59.704587: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 10:43:59.707949: The split file contains 1 splits.
2023-08-20 10:43:59.709649: Desired fold for training: 0
2023-08-20 10:43:59.711201: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 10:47:36.879921: dsc: 87.63%
2023-08-20 10:47:36.882360: miou: 77.98%
2023-08-20 10:47:36.883839: acc: 96.03%, sen: 84.01%, spe: 98.45%
2023-08-20 10:47:36.886306: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:47:36.887712: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:47:36.889093: finished real validation
2023-08-20 10:48:02.918507: train_loss -0.9022
2023-08-20 10:48:02.920765: val_loss -0.7296
2023-08-20 10:48:02.922796: Pseudo dice [0.8785]
2023-08-20 10:48:02.924372: Epoch time: 659.92 s
2023-08-20 10:48:05.340816: Yayy! New best EMA pseudo Dice: 0.8712
2023-08-20 10:48:08.639004: 
2023-08-20 10:48:08.641214: Epoch 100
2023-08-20 10:48:08.642686: Current learning rate: 0.00347
2023-08-20 10:48:08.644602: start training, 250
================num of epochs: 250================
2023-08-20 10:55:05.713325: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 10:55:06.022796: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 10:55:06.026260: The split file contains 1 splits.
2023-08-20 10:55:06.028134: Desired fold for training: 0
2023-08-20 10:55:06.029796: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 10:58:43.913113: dsc: 87.54%
2023-08-20 10:58:43.915265: miou: 77.84%
2023-08-20 10:58:43.916594: acc: 95.91%, sen: 85.85%, spe: 97.93%
2023-08-20 10:58:43.918856: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:58:43.920306: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 10:58:43.921648: finished real validation
2023-08-20 10:59:09.944265: train_loss -0.9035
2023-08-20 10:59:09.946680: val_loss -0.7134
2023-08-20 10:59:09.948807: Pseudo dice [0.8771]
2023-08-20 10:59:09.950371: Epoch time: 661.31 s
2023-08-20 10:59:09.951792: Yayy! New best EMA pseudo Dice: 0.8718
2023-08-20 10:59:13.468857: 
2023-08-20 10:59:13.470782: Epoch 101
2023-08-20 10:59:13.472275: Current learning rate: 0.00346
2023-08-20 10:59:13.474102: start training, 250
================num of epochs: 250================
2023-08-20 11:06:10.144946: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 11:06:10.457291: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 11:06:10.460865: The split file contains 1 splits.
2023-08-20 11:06:10.462575: Desired fold for training: 0
2023-08-20 11:06:10.464216: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 11:09:47.848809: dsc: 87.29%
2023-08-20 11:09:47.850954: miou: 77.45%
2023-08-20 11:09:47.852387: acc: 95.96%, sen: 82.87%, spe: 98.59%
2023-08-20 11:09:47.854762: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 11:09:47.856282: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 11:09:47.857700: finished real validation
2023-08-20 11:10:14.080053: train_loss -0.9064
2023-08-20 11:10:14.082356: val_loss -0.6582
2023-08-20 11:10:14.084497: Pseudo dice [0.8623]
2023-08-20 11:10:14.086084: Epoch time: 660.61 s
2023-08-20 11:10:15.421966: 
2023-08-20 11:10:15.423976: Epoch 102
2023-08-20 11:10:15.425491: Current learning rate: 0.00344
2023-08-20 11:10:15.427434: start training, 250
================num of epochs: 250================
2023-08-20 11:17:11.337791: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 11:17:11.650884: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 11:17:11.654053: The split file contains 1 splits.
2023-08-20 11:17:11.655522: Desired fold for training: 0
2023-08-20 11:17:11.657072: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 11:20:47.543669: dsc: 87.24%
2023-08-20 11:20:47.545791: miou: 77.36%
2023-08-20 11:20:47.547201: acc: 95.90%, sen: 83.64%, spe: 98.37%
2023-08-20 11:20:47.549474: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 11:20:47.550861: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 11:20:47.552197: finished real validation
2023-08-20 11:21:13.365426: train_loss -0.9013
2023-08-20 11:21:13.367491: val_loss -0.6959
2023-08-20 11:21:13.369329: Pseudo dice [0.8734]
2023-08-20 11:21:13.370822: Epoch time: 657.95 s
2023-08-20 11:21:14.681698: 
2023-08-20 11:21:14.683627: Epoch 103
2023-08-20 11:21:14.685120: Current learning rate: 0.00342
2023-08-20 11:21:14.686925: start training, 250
================num of epochs: 250================
2023-08-20 11:28:09.473721: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 11:28:09.792455: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 11:28:09.795928: The split file contains 1 splits.
2023-08-20 11:28:09.797494: Desired fold for training: 0
2023-08-20 11:28:09.799051: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 11:31:47.461188: dsc: 87.40%
2023-08-20 11:31:47.463223: miou: 77.61%
2023-08-20 11:31:47.464604: acc: 95.98%, sen: 83.15%, spe: 98.57%
2023-08-20 11:31:47.466924: current best miou: 0.782516959887512 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 11:31:47.468353: current best dsc: 0.8779910401939668 at epoch: 73, (73, 0.782516959887512, 0.8779910401939668)
2023-08-20 11:31:47.469737: finished real validation
2023-08-20 11:32:13.297845: train_loss -0.9032
2023-08-20 11:32:13.300256: val_loss -0.7161
2023-08-20 11:32:13.302386: Pseudo dice [0.8823]
2023-08-20 11:32:13.303916: Epoch time: 658.62 s
2023-08-20 11:32:13.305345: Yayy! New best EMA pseudo Dice: 0.8723
2023-08-20 11:32:16.581373: 
2023-08-20 11:32:16.583387: Epoch 104
2023-08-20 11:32:16.584879: Current learning rate: 0.00341
2023-08-20 11:32:16.586823: start training, 250
================num of epochs: 250================
2023-08-20 11:39:11.870327: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 11:39:12.189077: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 11:39:12.192634: The split file contains 1 splits.
2023-08-20 11:39:12.194731: Desired fold for training: 0
2023-08-20 11:39:12.197548: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 11:42:48.345376: dsc: 87.86%
2023-08-20 11:42:48.347588: miou: 78.34%
2023-08-20 11:42:48.349090: acc: 96.00%, sen: 86.37%, spe: 97.94%
2023-08-20 11:42:48.351491: current best miou: 0.7834271122114771 at epoch: 104, (104, 0.7834271122114771, 0.8785636450709953)
2023-08-20 11:42:48.353004: current best dsc: 0.8785636450709953 at epoch: 104, (104, 0.7834271122114771, 0.8785636450709953)
2023-08-20 11:42:50.431852: finished real validation
2023-08-20 11:43:16.237012: train_loss -0.9032
2023-08-20 11:43:16.239291: val_loss -0.7186
2023-08-20 11:43:16.241436: Pseudo dice [0.8821]
2023-08-20 11:43:16.243046: Epoch time: 659.66 s
2023-08-20 11:43:16.244487: Yayy! New best EMA pseudo Dice: 0.8732
2023-08-20 11:43:19.528120: 
2023-08-20 11:43:19.529977: Epoch 105
2023-08-20 11:43:19.531532: Current learning rate: 0.00339
2023-08-20 11:43:19.533389: start training, 250
================num of epochs: 250================
2023-08-20 11:50:15.125823: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 11:50:15.436357: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 11:50:15.440036: The split file contains 1 splits.
2023-08-20 11:50:15.441917: Desired fold for training: 0
2023-08-20 11:50:15.443674: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 11:53:53.271055: dsc: 87.56%
2023-08-20 11:53:53.273498: miou: 77.87%
2023-08-20 11:53:53.275211: acc: 95.95%, sen: 85.11%, spe: 98.13%
2023-08-20 11:53:53.278324: current best miou: 0.7834271122114771 at epoch: 104, (104, 0.7834271122114771, 0.8785636450709953)
2023-08-20 11:53:53.280154: current best dsc: 0.8785636450709953 at epoch: 104, (104, 0.7834271122114771, 0.8785636450709953)
2023-08-20 11:53:53.282151: finished real validation
2023-08-20 11:54:19.523340: train_loss -0.9039
2023-08-20 11:54:19.525757: val_loss -0.6906
2023-08-20 11:54:19.527942: Pseudo dice [0.8675]
2023-08-20 11:54:19.529622: Epoch time: 660.0 s
2023-08-20 11:54:20.852692: 
2023-08-20 11:54:20.854635: Epoch 106
2023-08-20 11:54:20.856243: Current learning rate: 0.00338
2023-08-20 11:54:20.858145: start training, 250
================num of epochs: 250================
2023-08-20 12:01:29.435997: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 12:01:29.748031: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 12:01:29.751727: The split file contains 1 splits.
2023-08-20 12:01:29.753482: Desired fold for training: 0
2023-08-20 12:01:29.755032: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 12:05:20.206577: dsc: 88.00%
2023-08-20 12:05:20.208966: miou: 78.58%
2023-08-20 12:05:20.210544: acc: 96.11%, sen: 85.12%, spe: 98.32%
2023-08-20 12:05:20.213139: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:05:20.214665: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:05:22.243448: finished real validation
2023-08-20 12:05:49.642956: train_loss -0.9022
2023-08-20 12:05:49.645338: val_loss -0.7225
2023-08-20 12:05:49.647497: Pseudo dice [0.8817]
2023-08-20 12:05:49.649166: Epoch time: 688.79 s
2023-08-20 12:05:49.650659: Yayy! New best EMA pseudo Dice: 0.8736
2023-08-20 12:05:53.070385: 
2023-08-20 12:05:53.072473: Epoch 107
2023-08-20 12:05:53.074136: Current learning rate: 0.00336
2023-08-20 12:05:53.076173: start training, 250
================num of epochs: 250================
2023-08-20 12:13:05.774002: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 12:13:06.098072: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 12:13:06.101448: The split file contains 1 splits.
2023-08-20 12:13:06.103188: Desired fold for training: 0
2023-08-20 12:13:06.104731: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 12:16:55.926374: dsc: 87.54%
2023-08-20 12:16:55.928642: miou: 77.84%
2023-08-20 12:16:55.930133: acc: 95.97%, sen: 84.57%, spe: 98.26%
2023-08-20 12:16:55.932647: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:16:55.934266: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:16:55.935694: finished real validation
2023-08-20 12:17:23.367007: train_loss -0.9058
2023-08-20 12:17:23.369462: val_loss -0.6874
2023-08-20 12:17:23.372025: Pseudo dice [0.8732]
2023-08-20 12:17:23.373703: Epoch time: 690.3 s
2023-08-20 12:17:24.772986: 
2023-08-20 12:17:24.775053: Epoch 108
2023-08-20 12:17:24.776586: Current learning rate: 0.00335
2023-08-20 12:17:24.778549: start training, 250
================num of epochs: 250================
2023-08-20 12:24:37.711211: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 12:24:38.036273: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 12:24:38.039348: The split file contains 1 splits.
2023-08-20 12:24:38.041435: Desired fold for training: 0
2023-08-20 12:24:38.043090: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 12:28:26.087574: dsc: 87.32%
2023-08-20 12:28:26.090001: miou: 77.50%
2023-08-20 12:28:26.091404: acc: 95.89%, sen: 84.55%, spe: 98.17%
2023-08-20 12:28:26.093686: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:28:26.095252: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:28:26.096642: finished real validation
2023-08-20 12:28:53.512726: train_loss -0.9046
2023-08-20 12:28:53.515022: val_loss -0.718
2023-08-20 12:28:53.517068: Pseudo dice [0.8847]
2023-08-20 12:28:53.518635: Epoch time: 688.74 s
2023-08-20 12:28:53.520068: Yayy! New best EMA pseudo Dice: 0.8746
2023-08-20 12:28:57.039718: 
2023-08-20 12:28:57.041589: Epoch 109
2023-08-20 12:28:57.042986: Current learning rate: 0.00333
2023-08-20 12:28:57.044803: start training, 250
================num of epochs: 250================
2023-08-20 12:36:09.509693: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 12:36:09.839828: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 12:36:09.843118: The split file contains 1 splits.
2023-08-20 12:36:09.844727: Desired fold for training: 0
2023-08-20 12:36:09.846361: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 12:40:00.685406: dsc: 87.97%
2023-08-20 12:40:00.687640: miou: 78.53%
2023-08-20 12:40:00.689055: acc: 96.09%, sen: 85.28%, spe: 98.27%
2023-08-20 12:40:00.691434: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:40:00.692892: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:40:00.694360: finished real validation
2023-08-20 12:40:28.118087: train_loss -0.9047
2023-08-20 12:40:29.262829: val_loss -0.7653
2023-08-20 12:40:29.265137: Pseudo dice [0.8929]
2023-08-20 12:40:29.267138: Epoch time: 691.08 s
2023-08-20 12:40:29.268596: Yayy! New best EMA pseudo Dice: 0.8765
2023-08-20 12:40:32.658901: 
2023-08-20 12:40:32.661011: Epoch 110
2023-08-20 12:40:32.662614: Current learning rate: 0.00331
2023-08-20 12:40:32.664474: start training, 250
================num of epochs: 250================
2023-08-20 12:47:46.662993: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 12:47:46.995650: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 12:47:46.999065: The split file contains 1 splits.
2023-08-20 12:47:47.000951: Desired fold for training: 0
2023-08-20 12:47:47.002639: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 12:52:15.410844: dsc: 87.81%
2023-08-20 12:52:15.413208: miou: 78.26%
2023-08-20 12:52:15.414614: acc: 96.07%, sen: 84.56%, spe: 98.38%
2023-08-20 12:52:15.416998: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:52:15.418473: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 12:52:15.419921: finished real validation
2023-08-20 12:52:42.717028: train_loss -0.9055
2023-08-20 12:52:42.719005: val_loss -0.7103
2023-08-20 12:52:42.720824: Pseudo dice [0.8755]
2023-08-20 12:52:42.722497: Epoch time: 730.06 s
2023-08-20 12:52:44.112695: 
2023-08-20 12:52:44.114583: Epoch 111
2023-08-20 12:52:44.116137: Current learning rate: 0.0033
2023-08-20 12:52:44.117985: start training, 250
================num of epochs: 250================
2023-08-20 12:59:56.023463: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 12:59:56.353050: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 12:59:56.356587: The split file contains 1 splits.
2023-08-20 12:59:56.358275: Desired fold for training: 0
2023-08-20 12:59:56.359774: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 13:04:01.804024: dsc: 87.53%
2023-08-20 13:04:01.806383: miou: 77.83%
2023-08-20 13:04:01.807888: acc: 95.91%, sen: 85.70%, spe: 97.96%
2023-08-20 13:04:01.810629: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:04:01.812348: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:04:01.813951: finished real validation
2023-08-20 13:04:28.574010: train_loss -0.906
2023-08-20 13:04:28.576231: val_loss -0.7043
2023-08-20 13:04:28.578385: Pseudo dice [0.877]
2023-08-20 13:04:28.580048: Epoch time: 704.46 s
2023-08-20 13:04:29.862785: 
2023-08-20 13:04:29.865088: Epoch 112
2023-08-20 13:04:29.866694: Current learning rate: 0.00328
2023-08-20 13:04:29.868857: start training, 250
================num of epochs: 250================
2023-08-20 13:11:39.830052: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 13:11:40.143738: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 13:11:40.147145: The split file contains 1 splits.
2023-08-20 13:11:40.148960: Desired fold for training: 0
2023-08-20 13:11:40.150583: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 13:15:30.317788: dsc: 86.85%
2023-08-20 13:15:30.320801: miou: 76.76%
2023-08-20 13:15:30.322495: acc: 95.74%, sen: 83.99%, spe: 98.11%
2023-08-20 13:15:30.325727: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:15:30.327551: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:15:30.329260: finished real validation
2023-08-20 13:15:57.321808: train_loss -0.9045
2023-08-20 13:15:57.325874: val_loss -0.6671
2023-08-20 13:15:57.328901: Pseudo dice [0.8599]
2023-08-20 13:15:57.330642: Epoch time: 687.46 s
2023-08-20 13:15:58.696085: 
2023-08-20 13:15:58.698285: Epoch 113
2023-08-20 13:15:58.699972: Current learning rate: 0.00327
2023-08-20 13:15:58.702204: start training, 250
================num of epochs: 250================
2023-08-20 13:22:59.521108: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 13:22:59.884781: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 13:22:59.888633: The split file contains 1 splits.
2023-08-20 13:22:59.890396: Desired fold for training: 0
2023-08-20 13:22:59.892102: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 13:27:02.329960: dsc: 87.32%
2023-08-20 13:27:02.332213: miou: 77.49%
2023-08-20 13:27:02.333750: acc: 95.96%, sen: 83.13%, spe: 98.54%
2023-08-20 13:27:02.336283: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:27:02.337841: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:27:02.339339: finished real validation
2023-08-20 13:27:27.289951: train_loss -0.9051
2023-08-20 13:27:27.292119: val_loss -0.6683
2023-08-20 13:27:27.294101: Pseudo dice [0.8658]
2023-08-20 13:27:27.295877: Epoch time: 688.6 s
2023-08-20 13:27:28.559913: 
2023-08-20 13:27:28.562207: Epoch 114
2023-08-20 13:27:28.563954: Current learning rate: 0.00325
2023-08-20 13:27:28.566360: start training, 250
================num of epochs: 250================
2023-08-20 13:34:27.479374: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 13:34:27.820803: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 13:34:27.824343: The split file contains 1 splits.
2023-08-20 13:34:27.826047: Desired fold for training: 0
2023-08-20 13:34:27.827608: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 13:38:18.829756: dsc: 87.40%
2023-08-20 13:38:18.832018: miou: 77.61%
2023-08-20 13:38:18.833647: acc: 95.96%, sen: 83.67%, spe: 98.43%
2023-08-20 13:38:18.836232: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:38:18.837844: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:38:18.839441: finished real validation
2023-08-20 13:38:45.528040: train_loss -0.9038
2023-08-20 13:38:45.530609: val_loss -0.6439
2023-08-20 13:38:45.532833: Pseudo dice [0.851]
2023-08-20 13:38:45.534541: Epoch time: 676.97 s
2023-08-20 13:38:46.823980: 
2023-08-20 13:38:46.826273: Epoch 115
2023-08-20 13:38:46.828040: Current learning rate: 0.00324
2023-08-20 13:38:46.830110: start training, 250
================num of epochs: 250================
2023-08-20 13:45:42.202987: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 13:45:42.539550: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 13:45:42.543170: The split file contains 1 splits.
2023-08-20 13:45:42.544950: Desired fold for training: 0
2023-08-20 13:45:42.546624: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 13:49:33.720371: dsc: 87.66%
2023-08-20 13:49:33.722546: miou: 78.03%
2023-08-20 13:49:33.724019: acc: 95.98%, sen: 85.24%, spe: 98.14%
2023-08-20 13:49:33.726375: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:49:33.727922: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 13:49:33.729643: finished real validation
2023-08-20 13:49:58.700855: train_loss -0.906
2023-08-20 13:49:58.703133: val_loss -0.7176
2023-08-20 13:49:58.705122: Pseudo dice [0.8873]
2023-08-20 13:49:58.706792: Epoch time: 671.88 s
2023-08-20 13:49:59.983647: 
2023-08-20 13:49:59.985808: Epoch 116
2023-08-20 13:49:59.988225: Current learning rate: 0.00322
2023-08-20 13:49:59.990387: start training, 250
================num of epochs: 250================
2023-08-20 13:56:49.591871: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 13:56:49.926802: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 13:56:49.930190: The split file contains 1 splits.
2023-08-20 13:56:49.931938: Desired fold for training: 0
2023-08-20 13:56:49.933517: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 14:00:41.820485: dsc: 87.27%
2023-08-20 14:00:41.823550: miou: 77.42%
2023-08-20 14:00:41.825094: acc: 95.95%, sen: 82.97%, spe: 98.56%
2023-08-20 14:00:41.827971: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:00:41.829592: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:00:41.831223: finished real validation
2023-08-20 14:01:06.736443: train_loss -0.9068
2023-08-20 14:01:06.739235: val_loss -0.6911
2023-08-20 14:01:06.741444: Pseudo dice [0.8707]
2023-08-20 14:01:06.743373: Epoch time: 666.76 s
2023-08-20 14:01:08.020094: 
2023-08-20 14:01:08.022196: Epoch 117
2023-08-20 14:01:08.023885: Current learning rate: 0.0032
2023-08-20 14:01:08.025924: start training, 250
================num of epochs: 250================
2023-08-20 14:07:58.205473: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 14:07:58.540536: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 14:07:58.544367: The split file contains 1 splits.
2023-08-20 14:07:58.546136: Desired fold for training: 0
2023-08-20 14:07:58.547781: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 14:11:52.524557: dsc: 87.70%
2023-08-20 14:11:52.527045: miou: 78.09%
2023-08-20 14:11:52.528570: acc: 95.95%, sen: 86.14%, spe: 97.93%
2023-08-20 14:11:52.530859: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:11:52.532380: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:11:52.533979: finished real validation
2023-08-20 14:12:17.320553: train_loss -0.9069
2023-08-20 14:12:17.322760: val_loss -0.6996
2023-08-20 14:12:17.324689: Pseudo dice [0.8716]
2023-08-20 14:12:17.326337: Epoch time: 669.3 s
2023-08-20 14:12:18.598233: 
2023-08-20 14:12:18.600287: Epoch 118
2023-08-20 14:12:18.601954: Current learning rate: 0.00319
2023-08-20 14:12:18.604064: start training, 250
================num of epochs: 250================
2023-08-20 14:19:07.763203: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 14:19:08.104289: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 14:19:08.107989: The split file contains 1 splits.
2023-08-20 14:19:08.109775: Desired fold for training: 0
2023-08-20 14:19:08.111468: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 14:22:59.179254: dsc: 87.42%
2023-08-20 14:22:59.181462: miou: 77.65%
2023-08-20 14:22:59.183018: acc: 95.86%, sen: 85.78%, spe: 97.89%
2023-08-20 14:22:59.185417: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:22:59.186982: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:22:59.188500: finished real validation
2023-08-20 14:23:24.048157: train_loss -0.9061
2023-08-20 14:23:24.050694: val_loss -0.734
2023-08-20 14:23:24.052952: Pseudo dice [0.8818]
2023-08-20 14:23:24.054738: Epoch time: 665.45 s
2023-08-20 14:23:25.321668: 
2023-08-20 14:23:25.324148: Epoch 119
2023-08-20 14:23:25.325844: Current learning rate: 0.00317
2023-08-20 14:23:25.327851: start training, 250
================num of epochs: 250================
2023-08-20 14:30:15.998935: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 14:30:16.339795: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 14:30:16.343846: The split file contains 1 splits.
2023-08-20 14:30:16.345596: Desired fold for training: 0
2023-08-20 14:30:16.347293: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 14:34:06.377839: dsc: 87.25%
2023-08-20 14:34:06.379956: miou: 77.38%
2023-08-20 14:34:06.381494: acc: 95.87%, sen: 84.33%, spe: 98.19%
2023-08-20 14:34:06.383791: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:34:06.385274: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:34:06.386758: finished real validation
2023-08-20 14:34:31.138002: train_loss -0.9084
2023-08-20 14:34:31.140591: val_loss -0.6942
2023-08-20 14:34:31.142717: Pseudo dice [0.8764]
2023-08-20 14:34:31.144487: Epoch time: 665.82 s
2023-08-20 14:34:32.407933: 
2023-08-20 14:34:32.410103: Epoch 120
2023-08-20 14:34:32.411785: Current learning rate: 0.00316
2023-08-20 14:34:32.413859: start training, 250
================num of epochs: 250================
2023-08-20 14:41:23.034667: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 14:41:23.372160: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 14:41:23.375762: The split file contains 1 splits.
2023-08-20 14:41:23.377533: Desired fold for training: 0
2023-08-20 14:41:23.379107: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 14:45:13.061549: dsc: 87.44%
2023-08-20 14:45:13.063939: miou: 77.69%
2023-08-20 14:45:13.065612: acc: 95.98%, sen: 83.57%, spe: 98.48%
2023-08-20 14:45:13.068153: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:45:13.069740: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:45:13.071293: finished real validation
2023-08-20 14:45:37.868338: train_loss -0.9078
2023-08-20 14:45:37.870931: val_loss -0.6822
2023-08-20 14:45:37.873270: Pseudo dice [0.8649]
2023-08-20 14:45:37.875016: Epoch time: 665.46 s
2023-08-20 14:45:39.150958: 
2023-08-20 14:45:39.153469: Epoch 121
2023-08-20 14:45:39.155211: Current learning rate: 0.00314
2023-08-20 14:45:39.157404: start training, 250
================num of epochs: 250================
2023-08-20 14:52:28.618061: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 14:52:28.961362: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 14:52:28.964876: The split file contains 1 splits.
2023-08-20 14:52:28.966834: Desired fold for training: 0
2023-08-20 14:52:28.968583: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 14:56:20.202938: dsc: 87.48%
2023-08-20 14:56:20.205265: miou: 77.74%
2023-08-20 14:56:20.206887: acc: 95.94%, sen: 84.64%, spe: 98.21%
2023-08-20 14:56:20.209433: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:56:20.211027: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 14:56:20.212686: finished real validation
2023-08-20 14:56:44.772134: train_loss -0.9079
2023-08-20 14:56:44.774423: val_loss -0.6975
2023-08-20 14:56:44.776660: Pseudo dice [0.8788]
2023-08-20 14:56:44.778346: Epoch time: 665.62 s
2023-08-20 14:56:46.023077: 
2023-08-20 14:56:46.025262: Epoch 122
2023-08-20 14:56:46.026965: Current learning rate: 0.00313
2023-08-20 14:56:46.028954: start training, 250
================num of epochs: 250================
2023-08-20 15:03:35.190203: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 15:03:35.528674: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 15:03:35.532159: The split file contains 1 splits.
2023-08-20 15:03:35.534003: Desired fold for training: 0
2023-08-20 15:03:35.535894: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 15:07:27.132918: dsc: 87.57%
2023-08-20 15:07:27.135270: miou: 77.88%
2023-08-20 15:07:27.136859: acc: 95.98%, sen: 84.59%, spe: 98.27%
2023-08-20 15:07:27.139240: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:07:27.140870: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:07:27.142448: finished real validation
2023-08-20 15:07:52.078423: train_loss -0.907
2023-08-20 15:07:52.080953: val_loss -0.7019
2023-08-20 15:07:52.083145: Pseudo dice [0.8722]
2023-08-20 15:07:52.084767: Epoch time: 666.06 s
2023-08-20 15:07:53.364560: 
2023-08-20 15:07:53.366696: Epoch 123
2023-08-20 15:07:53.368508: Current learning rate: 0.00311
2023-08-20 15:07:53.370603: start training, 250
================num of epochs: 250================
2023-08-20 15:14:44.184483: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 15:14:44.523319: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 15:14:44.526842: The split file contains 1 splits.
2023-08-20 15:14:44.528579: Desired fold for training: 0
2023-08-20 15:14:44.530170: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 15:18:35.366070: dsc: 87.27%
2023-08-20 15:18:35.368311: miou: 77.41%
2023-08-20 15:18:35.369842: acc: 95.94%, sen: 83.00%, spe: 98.55%
2023-08-20 15:18:35.372332: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:18:35.373971: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:18:35.375537: finished real validation
2023-08-20 15:19:02.860959: train_loss -0.9089
2023-08-20 15:19:02.863149: val_loss -0.7086
2023-08-20 15:19:02.865170: Pseudo dice [0.8731]
2023-08-20 15:19:02.866880: Epoch time: 669.5 s
2023-08-20 15:19:04.305756: 
2023-08-20 15:19:04.308218: Epoch 124
2023-08-20 15:19:04.309989: Current learning rate: 0.00309
2023-08-20 15:19:04.312431: start training, 250
================num of epochs: 250================
2023-08-20 15:25:58.512702: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 15:25:58.870146: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 15:25:58.873975: The split file contains 1 splits.
2023-08-20 15:25:58.875679: Desired fold for training: 0
2023-08-20 15:25:58.877304: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 15:29:50.308036: dsc: 87.69%
2023-08-20 15:29:50.310316: miou: 78.09%
2023-08-20 15:29:50.312105: acc: 95.92%, sen: 86.77%, spe: 97.76%
2023-08-20 15:29:50.314777: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:29:50.316343: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:29:50.317885: finished real validation
2023-08-20 15:30:15.020258: train_loss -0.9105
2023-08-20 15:30:15.022446: val_loss -0.7123
2023-08-20 15:30:15.024472: Pseudo dice [0.8777]
2023-08-20 15:30:15.026164: Epoch time: 670.72 s
2023-08-20 15:30:16.294663: 
2023-08-20 15:30:16.296870: Epoch 125
2023-08-20 15:30:16.298620: Current learning rate: 0.00308
2023-08-20 15:30:16.300805: start training, 250
================num of epochs: 250================
2023-08-20 15:37:15.323764: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 15:37:15.660298: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 15:37:15.664117: The split file contains 1 splits.
2023-08-20 15:37:15.665919: Desired fold for training: 0
2023-08-20 15:37:15.667813: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 15:41:08.136399: dsc: 87.21%
2023-08-20 15:41:08.138654: miou: 77.32%
2023-08-20 15:41:08.140322: acc: 95.86%, sen: 84.32%, spe: 98.18%
2023-08-20 15:41:08.142668: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:41:08.144261: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:41:08.145911: finished real validation
2023-08-20 15:41:33.130138: train_loss -0.9099
2023-08-20 15:41:33.132560: val_loss -0.6809
2023-08-20 15:41:33.134748: Pseudo dice [0.8673]
2023-08-20 15:41:33.136673: Epoch time: 676.84 s
2023-08-20 15:41:34.415897: 
2023-08-20 15:41:34.418107: Epoch 126
2023-08-20 15:41:34.419724: Current learning rate: 0.00306
2023-08-20 15:41:34.421804: start training, 250
================num of epochs: 250================
2023-08-20 15:48:25.968644: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 15:48:26.309453: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 15:48:26.312955: The split file contains 1 splits.
2023-08-20 15:48:26.314797: Desired fold for training: 0
2023-08-20 15:48:26.316518: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 15:52:16.724718: dsc: 87.32%
2023-08-20 15:52:16.727266: miou: 77.49%
2023-08-20 15:52:16.729281: acc: 95.83%, sen: 85.63%, spe: 97.89%
2023-08-20 15:52:16.731954: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:52:16.733479: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 15:52:16.734928: finished real validation
2023-08-20 15:52:41.573336: train_loss -0.9092
2023-08-20 15:52:41.575722: val_loss -0.7202
2023-08-20 15:52:41.578103: Pseudo dice [0.8827]
2023-08-20 15:52:41.579955: Epoch time: 667.16 s
2023-08-20 15:52:42.910296: 
2023-08-20 15:52:42.912603: Epoch 127
2023-08-20 15:52:42.914672: Current learning rate: 0.00305
2023-08-20 15:52:42.916760: start training, 250
================num of epochs: 250================
2023-08-20 15:59:33.710093: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 15:59:34.053584: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 15:59:34.057327: The split file contains 1 splits.
2023-08-20 15:59:34.059150: Desired fold for training: 0
2023-08-20 15:59:34.060891: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 16:03:26.016657: dsc: 87.51%
2023-08-20 16:03:26.018930: miou: 77.79%
2023-08-20 16:03:26.020519: acc: 95.94%, sen: 84.82%, spe: 98.18%
2023-08-20 16:03:26.022924: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 16:03:26.024418: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 16:03:26.025939: finished real validation
2023-08-20 16:03:50.852209: train_loss -0.907
2023-08-20 16:03:50.854711: val_loss -0.6928
2023-08-20 16:03:50.857050: Pseudo dice [0.872]
2023-08-20 16:03:50.858779: Epoch time: 667.94 s
2023-08-20 16:03:52.127510: 
2023-08-20 16:03:52.129726: Epoch 128
2023-08-20 16:03:52.131351: Current learning rate: 0.00303
2023-08-20 16:03:52.133391: start training, 250
================num of epochs: 250================
2023-08-20 16:10:42.215995: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 16:10:42.558021: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 16:10:42.561525: The split file contains 1 splits.
2023-08-20 16:10:42.563303: Desired fold for training: 0
2023-08-20 16:10:42.564981: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 16:14:35.752429: dsc: 87.43%
2023-08-20 16:14:35.754689: miou: 77.66%
2023-08-20 16:14:35.756262: acc: 95.99%, sen: 83.26%, spe: 98.55%
2023-08-20 16:14:35.758785: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 16:14:35.760501: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 16:14:35.762086: finished real validation
2023-08-20 16:15:02.187237: train_loss -0.9107
2023-08-20 16:15:02.189740: val_loss -0.6653
2023-08-20 16:15:02.192056: Pseudo dice [0.8581]
2023-08-20 16:15:02.193890: Epoch time: 670.06 s
2023-08-20 16:15:03.487787: 
2023-08-20 16:15:03.489919: Epoch 129
2023-08-20 16:15:03.491619: Current learning rate: 0.00301
2023-08-20 16:15:03.493795: start training, 250
================num of epochs: 250================
2023-08-20 16:21:57.302677: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 16:21:57.652721: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 16:21:57.656207: The split file contains 1 splits.
2023-08-20 16:21:57.657935: Desired fold for training: 0
2023-08-20 16:21:57.659650: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 16:25:49.585196: dsc: 87.23%
2023-08-20 16:25:49.587745: miou: 77.35%
2023-08-20 16:25:49.589386: acc: 95.89%, sen: 83.71%, spe: 98.35%
2023-08-20 16:25:49.592094: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 16:25:49.593765: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 16:25:49.595433: finished real validation
2023-08-20 16:26:14.476150: train_loss -0.9108
2023-08-20 16:26:14.478741: val_loss -0.7019
2023-08-20 16:26:14.480966: Pseudo dice [0.8727]
2023-08-20 16:26:14.482717: Epoch time: 670.99 s
2023-08-20 16:26:15.759715: 
2023-08-20 16:26:15.762007: Epoch 130
2023-08-20 16:26:15.763651: Current learning rate: 0.003
2023-08-20 16:26:15.765641: start training, 250
================num of epochs: 250================
2023-08-20 16:33:05.875223: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 16:33:06.200966: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 16:33:06.204412: The split file contains 1 splits.
2023-08-20 16:33:06.206528: Desired fold for training: 0
2023-08-20 16:33:06.208118: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 16:36:57.711457: dsc: 87.21%
2023-08-20 16:36:57.714819: miou: 77.33%
2023-08-20 16:36:57.716704: acc: 95.82%, sen: 85.08%, spe: 97.98%
2023-08-20 16:36:57.719620: current best miou: 0.7857692081274821 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 16:36:57.721362: current best dsc: 0.880034446278108 at epoch: 106, (106, 0.7857692081274821, 0.880034446278108)
2023-08-20 16:36:57.722996: finished real validation
2023-08-20 16:37:24.143288: train_loss -0.9105
2023-08-20 16:37:24.145751: val_loss -0.7189
2023-08-20 16:37:24.147899: Pseudo dice [0.8749]
2023-08-20 16:37:24.149667: Epoch time: 668.39 s
2023-08-20 16:37:25.444569: 
2023-08-20 16:37:25.446725: Epoch 131
2023-08-20 16:37:25.448431: Current learning rate: 0.00298
2023-08-20 16:37:25.450551: start training, 250
================num of epochs: 250================
2023-08-20 16:44:22.569559: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 16:44:22.906914: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 16:44:22.910489: The split file contains 1 splits.
2023-08-20 16:44:22.912238: Desired fold for training: 0
2023-08-20 16:44:22.913891: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 16:48:13.979032: dsc: 88.13%
2023-08-20 16:48:14.002223: miou: 78.77%
2023-08-20 16:48:14.003961: acc: 96.09%, sen: 86.69%, spe: 97.98%
2023-08-20 16:48:14.006550: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 16:48:14.008348: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 16:48:16.150597: finished real validation
2023-08-20 16:48:41.066718: train_loss -0.9099
2023-08-20 16:48:41.068871: val_loss -0.7202
2023-08-20 16:48:41.070915: Pseudo dice [0.8848]
2023-08-20 16:48:41.072679: Epoch time: 675.62 s
2023-08-20 16:48:42.347499: 
2023-08-20 16:48:42.349697: Epoch 132
2023-08-20 16:48:42.351576: Current learning rate: 0.00297
2023-08-20 16:48:42.353742: start training, 250
================num of epochs: 250================
2023-08-20 16:55:33.098772: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 16:55:33.464687: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 16:55:33.468379: The split file contains 1 splits.
2023-08-20 16:55:33.470703: Desired fold for training: 0
2023-08-20 16:55:33.472810: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 16:59:56.133696: dsc: 87.50%
2023-08-20 16:59:56.136666: miou: 77.78%
2023-08-20 16:59:56.138248: acc: 95.97%, sen: 84.12%, spe: 98.36%
2023-08-20 16:59:56.140748: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 16:59:56.142419: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 16:59:56.144032: finished real validation
2023-08-20 17:00:20.957906: train_loss -0.9114
2023-08-20 17:00:20.960886: val_loss -0.7117
2023-08-20 17:00:20.963213: Pseudo dice [0.8798]
2023-08-20 17:00:20.965068: Epoch time: 698.61 s
2023-08-20 17:00:22.248889: 
2023-08-20 17:00:22.251779: Epoch 133
2023-08-20 17:00:22.253788: Current learning rate: 0.00295
2023-08-20 17:00:22.256557: start training, 250
================num of epochs: 250================
2023-08-20 17:07:12.533247: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 17:07:12.902784: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 17:07:12.906412: The split file contains 1 splits.
2023-08-20 17:07:12.908226: Desired fold for training: 0
2023-08-20 17:07:12.909913: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 17:11:03.719336: dsc: 87.43%
2023-08-20 17:11:03.721693: miou: 77.68%
2023-08-20 17:11:03.723397: acc: 95.95%, sen: 84.11%, spe: 98.33%
2023-08-20 17:11:03.725897: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:11:03.727576: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:11:03.729166: finished real validation
2023-08-20 17:11:28.667361: train_loss -0.9112
2023-08-20 17:11:28.669994: val_loss -0.6574
2023-08-20 17:11:28.672298: Pseudo dice [0.8602]
2023-08-20 17:11:28.674038: Epoch time: 666.42 s
2023-08-20 17:11:29.942034: 
2023-08-20 17:11:29.944268: Epoch 134
2023-08-20 17:11:29.945985: Current learning rate: 0.00294
2023-08-20 17:11:29.948106: start training, 250
================num of epochs: 250================
2023-08-20 17:18:19.828953: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 17:18:20.169532: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 17:18:20.173084: The split file contains 1 splits.
2023-08-20 17:18:20.174963: Desired fold for training: 0
2023-08-20 17:18:20.176706: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 17:22:14.377803: dsc: 87.20%
2023-08-20 17:22:14.380308: miou: 77.31%
2023-08-20 17:22:14.381987: acc: 95.91%, sen: 83.22%, spe: 98.46%
2023-08-20 17:22:14.384768: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:22:14.386511: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:22:14.388105: finished real validation
2023-08-20 17:22:39.303611: train_loss -0.9147
2023-08-20 17:22:39.306267: val_loss -0.681
2023-08-20 17:22:39.308658: Pseudo dice [0.8688]
2023-08-20 17:22:39.310475: Epoch time: 669.36 s
2023-08-20 17:22:40.595643: 
2023-08-20 17:22:40.597842: Epoch 135
2023-08-20 17:22:40.599642: Current learning rate: 0.00292
2023-08-20 17:22:40.601812: start training, 250
================num of epochs: 250================
2023-08-20 17:29:33.682457: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 17:29:34.028076: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 17:29:34.031580: The split file contains 1 splits.
2023-08-20 17:29:34.033350: Desired fold for training: 0
2023-08-20 17:29:34.035080: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 17:33:25.734587: dsc: 87.57%
2023-08-20 17:33:25.736842: miou: 77.88%
2023-08-20 17:33:25.738454: acc: 95.97%, sen: 84.78%, spe: 98.22%
2023-08-20 17:33:25.740971: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:33:25.742568: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:33:25.744167: finished real validation
2023-08-20 17:33:50.585433: train_loss -0.9127
2023-08-20 17:33:50.588275: val_loss -0.7023
2023-08-20 17:33:50.590635: Pseudo dice [0.8782]
2023-08-20 17:33:50.592457: Epoch time: 669.99 s
2023-08-20 17:33:51.870025: 
2023-08-20 17:33:51.872159: Epoch 136
2023-08-20 17:33:51.873870: Current learning rate: 0.0029
2023-08-20 17:33:51.876000: start training, 250
================num of epochs: 250================
2023-08-20 17:40:41.915090: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 17:40:42.252249: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 17:40:42.256040: The split file contains 1 splits.
2023-08-20 17:40:42.257848: Desired fold for training: 0
2023-08-20 17:40:42.259594: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 17:44:34.658674: dsc: 87.66%
2023-08-20 17:44:34.661098: miou: 78.04%
2023-08-20 17:44:34.662855: acc: 96.00%, sen: 84.86%, spe: 98.24%
2023-08-20 17:44:34.665408: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:44:34.667094: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:44:34.668728: finished real validation
2023-08-20 17:44:59.553967: train_loss -0.9106
2023-08-20 17:45:00.384220: val_loss -0.7243
2023-08-20 17:45:00.386504: Pseudo dice [0.8835]
2023-08-20 17:45:00.388275: Epoch time: 667.69 s
2023-08-20 17:45:01.676131: 
2023-08-20 17:45:01.678448: Epoch 137
2023-08-20 17:45:01.680243: Current learning rate: 0.00289
2023-08-20 17:45:01.682378: start training, 250
================num of epochs: 250================
2023-08-20 17:52:01.920887: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 17:52:02.270805: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 17:52:02.274539: The split file contains 1 splits.
2023-08-20 17:52:02.276338: Desired fold for training: 0
2023-08-20 17:52:02.278091: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 17:55:54.857274: dsc: 87.66%
2023-08-20 17:55:54.859528: miou: 78.03%
2023-08-20 17:55:54.861170: acc: 96.02%, sen: 84.36%, spe: 98.37%
2023-08-20 17:55:54.863658: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:55:54.865390: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 17:55:54.867077: finished real validation
2023-08-20 17:56:19.804689: train_loss -0.9137
2023-08-20 17:56:19.807471: val_loss -0.7023
2023-08-20 17:56:19.809758: Pseudo dice [0.8745]
2023-08-20 17:56:19.811647: Epoch time: 678.13 s
2023-08-20 17:56:21.084243: 
2023-08-20 17:56:21.086702: Epoch 138
2023-08-20 17:56:21.088780: Current learning rate: 0.00287
2023-08-20 17:56:21.090986: start training, 250
================num of epochs: 250================
2023-08-20 18:03:11.542234: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 18:03:11.882866: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 18:03:11.886612: The split file contains 1 splits.
2023-08-20 18:03:11.888528: Desired fold for training: 0
2023-08-20 18:03:11.890239: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 18:07:03.661262: dsc: 87.74%
2023-08-20 18:07:03.663828: miou: 78.16%
2023-08-20 18:07:03.665532: acc: 96.06%, sen: 84.11%, spe: 98.47%
2023-08-20 18:07:03.668518: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:07:03.670294: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:07:03.671958: finished real validation
2023-08-20 18:07:28.632208: train_loss -0.9135
2023-08-20 18:07:28.634753: val_loss -0.6791
2023-08-20 18:07:28.636992: Pseudo dice [0.8752]
2023-08-20 18:07:28.638752: Epoch time: 667.55 s
2023-08-20 18:07:29.913669: 
2023-08-20 18:07:29.915908: Epoch 139
2023-08-20 18:07:29.917599: Current learning rate: 0.00286
2023-08-20 18:07:29.919735: start training, 250
================num of epochs: 250================
2023-08-20 18:14:20.323625: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 18:14:20.661215: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 18:14:20.664728: The split file contains 1 splits.
2023-08-20 18:14:20.666726: Desired fold for training: 0
2023-08-20 18:14:20.668386: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 18:18:12.203521: dsc: 87.49%
2023-08-20 18:18:12.205745: miou: 77.75%
2023-08-20 18:18:12.207546: acc: 96.01%, sen: 83.16%, spe: 98.60%
2023-08-20 18:18:12.210052: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:18:12.211671: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:18:12.213301: finished real validation
2023-08-20 18:18:37.033562: train_loss -0.9133
2023-08-20 18:18:37.036170: val_loss -0.6736
2023-08-20 18:18:37.038490: Pseudo dice [0.8655]
2023-08-20 18:18:37.040287: Epoch time: 667.12 s
2023-08-20 18:18:38.372416: 
2023-08-20 18:18:38.374538: Epoch 140
2023-08-20 18:18:38.376219: Current learning rate: 0.00284
2023-08-20 18:18:38.378345: start training, 250
================num of epochs: 250================
2023-08-20 18:25:30.088774: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 18:25:30.437075: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 18:25:30.440682: The split file contains 1 splits.
2023-08-20 18:25:30.442789: Desired fold for training: 0
2023-08-20 18:25:30.444624: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 18:29:22.237989: dsc: 87.31%
2023-08-20 18:29:22.240404: miou: 77.48%
2023-08-20 18:29:22.242099: acc: 95.96%, sen: 82.95%, spe: 98.58%
2023-08-20 18:29:22.244772: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:29:22.246482: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:29:22.248206: finished real validation
2023-08-20 18:29:47.173864: train_loss -0.9116
2023-08-20 18:29:47.177859: val_loss -0.6562
2023-08-20 18:29:47.181345: Pseudo dice [0.8604]
2023-08-20 18:29:47.184558: Epoch time: 668.8 s
2023-08-20 18:29:48.459781: 
2023-08-20 18:29:48.462631: Epoch 141
2023-08-20 18:29:48.465267: Current learning rate: 0.00282
2023-08-20 18:29:48.468019: start training, 250
================num of epochs: 250================
2023-08-20 18:36:38.763158: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 18:36:39.094026: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 18:36:39.097682: The split file contains 1 splits.
2023-08-20 18:36:39.099531: Desired fold for training: 0
2023-08-20 18:36:39.101204: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 18:40:33.898001: dsc: 87.31%
2023-08-20 18:40:33.900534: miou: 77.47%
2023-08-20 18:40:33.902257: acc: 95.94%, sen: 83.38%, spe: 98.47%
2023-08-20 18:40:33.904942: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:40:33.906606: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:40:33.908233: finished real validation
2023-08-20 18:40:58.877780: train_loss -0.9129
2023-08-20 18:40:58.880109: val_loss -0.6538
2023-08-20 18:40:58.882255: Pseudo dice [0.862]
2023-08-20 18:40:58.884073: Epoch time: 670.42 s
2023-08-20 18:41:00.216861: 
2023-08-20 18:41:00.219209: Epoch 142
2023-08-20 18:41:00.221006: Current learning rate: 0.00281
2023-08-20 18:41:00.223091: start training, 250
================num of epochs: 250================
2023-08-20 18:47:51.483543: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 18:47:51.824510: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 18:47:51.828337: The split file contains 1 splits.
2023-08-20 18:47:51.830150: Desired fold for training: 0
2023-08-20 18:47:51.831929: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 18:51:42.107518: dsc: 87.34%
2023-08-20 18:51:42.109959: miou: 77.52%
2023-08-20 18:51:42.111562: acc: 95.90%, sen: 84.47%, spe: 98.20%
2023-08-20 18:51:42.114090: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:51:42.115838: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 18:51:42.117651: finished real validation
2023-08-20 18:52:06.933857: train_loss -0.9118
2023-08-20 18:52:06.936515: val_loss -0.6943
2023-08-20 18:52:06.938773: Pseudo dice [0.8713]
2023-08-20 18:52:06.940531: Epoch time: 666.72 s
2023-08-20 18:52:08.214731: 
2023-08-20 18:52:08.217084: Epoch 143
2023-08-20 18:52:08.218848: Current learning rate: 0.00279
2023-08-20 18:52:08.220925: start training, 250
================num of epochs: 250================
2023-08-20 18:58:58.401585: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 18:58:58.732880: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 18:58:58.736511: The split file contains 1 splits.
2023-08-20 18:58:58.738449: Desired fold for training: 0
2023-08-20 18:58:58.740361: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 19:02:50.979171: dsc: 86.98%
2023-08-20 19:02:50.981799: miou: 76.96%
2023-08-20 19:02:50.983653: acc: 95.87%, sen: 82.26%, spe: 98.61%
2023-08-20 19:02:50.986718: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:02:50.988590: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:02:50.990381: finished real validation
2023-08-20 19:03:15.844779: train_loss -0.9122
2023-08-20 19:03:15.847320: val_loss -0.6563
2023-08-20 19:03:15.849437: Pseudo dice [0.8641]
2023-08-20 19:03:15.851293: Epoch time: 667.63 s
2023-08-20 19:03:17.185664: 
2023-08-20 19:03:17.187933: Epoch 144
2023-08-20 19:03:17.189785: Current learning rate: 0.00278
2023-08-20 19:03:17.192162: start training, 250
================num of epochs: 250================
2023-08-20 19:10:16.597337: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 19:10:16.931031: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 19:10:16.934869: The split file contains 1 splits.
2023-08-20 19:10:16.936766: Desired fold for training: 0
2023-08-20 19:10:16.938485: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 19:14:08.467119: dsc: 87.10%
2023-08-20 19:14:08.469423: miou: 77.15%
2023-08-20 19:14:08.471219: acc: 95.87%, sen: 83.15%, spe: 98.43%
2023-08-20 19:14:08.473907: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:14:08.475740: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:14:08.477465: finished real validation
2023-08-20 19:14:33.453854: train_loss -0.9154
2023-08-20 19:14:33.456567: val_loss -0.6975
2023-08-20 19:14:33.458941: Pseudo dice [0.8754]
2023-08-20 19:14:33.460748: Epoch time: 676.27 s
2023-08-20 19:14:34.752170: 
2023-08-20 19:14:34.754495: Epoch 145
2023-08-20 19:14:34.756321: Current learning rate: 0.00276
2023-08-20 19:14:34.758569: start training, 250
================num of epochs: 250================
2023-08-20 19:21:26.035414: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 19:21:26.377645: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 19:21:26.381438: The split file contains 1 splits.
2023-08-20 19:21:26.383300: Desired fold for training: 0
2023-08-20 19:21:26.385021: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 19:25:17.396278: dsc: 87.93%
2023-08-20 19:25:17.398491: miou: 78.45%
2023-08-20 19:25:17.400118: acc: 96.08%, sen: 85.23%, spe: 98.26%
2023-08-20 19:25:17.402580: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:25:17.404240: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:25:17.405789: finished real validation
2023-08-20 19:25:42.106797: train_loss -0.9147
2023-08-20 19:25:42.110110: val_loss -0.7218
2023-08-20 19:25:42.112096: Pseudo dice [0.8772]
2023-08-20 19:25:42.113915: Epoch time: 667.36 s
2023-08-20 19:25:43.429615: 
2023-08-20 19:25:43.431926: Epoch 146
2023-08-20 19:25:43.433652: Current learning rate: 0.00274
2023-08-20 19:25:43.435763: start training, 250
================num of epochs: 250================
2023-08-20 19:32:36.408239: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 19:32:36.752225: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 19:32:36.755841: The split file contains 1 splits.
2023-08-20 19:32:36.757910: Desired fold for training: 0
2023-08-20 19:32:36.759881: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 19:36:27.576365: dsc: 87.29%
2023-08-20 19:36:27.578722: miou: 77.44%
2023-08-20 19:36:27.580412: acc: 95.94%, sen: 83.22%, spe: 98.50%
2023-08-20 19:36:27.582966: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:36:27.584638: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:36:27.586324: finished real validation
2023-08-20 19:36:52.235967: train_loss -0.9137
2023-08-20 19:36:52.238847: val_loss -0.7022
2023-08-20 19:36:52.241374: Pseudo dice [0.8776]
2023-08-20 19:36:52.243320: Epoch time: 668.81 s
2023-08-20 19:36:53.521000: 
2023-08-20 19:36:53.523047: Epoch 147
2023-08-20 19:36:53.524755: Current learning rate: 0.00273
2023-08-20 19:36:53.526910: start training, 250
================num of epochs: 250================
2023-08-20 19:43:43.442987: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 19:43:43.790496: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 19:43:43.794251: The split file contains 1 splits.
2023-08-20 19:43:43.796266: Desired fold for training: 0
2023-08-20 19:43:43.798259: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 19:47:34.457792: dsc: 87.39%
2023-08-20 19:47:34.460125: miou: 77.61%
2023-08-20 19:47:34.461958: acc: 95.97%, sen: 83.37%, spe: 98.51%
2023-08-20 19:47:34.464546: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:47:34.466249: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:47:34.468149: finished real validation
2023-08-20 19:47:59.372585: train_loss -0.9139
2023-08-20 19:47:59.375009: val_loss -0.6574
2023-08-20 19:47:59.377315: Pseudo dice [0.8621]
2023-08-20 19:47:59.379113: Epoch time: 665.85 s
2023-08-20 19:48:00.652915: 
2023-08-20 19:48:00.655300: Epoch 148
2023-08-20 19:48:00.657022: Current learning rate: 0.00271
2023-08-20 19:48:00.659394: start training, 250
================num of epochs: 250================
2023-08-20 19:54:59.923460: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 19:55:00.259612: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 19:55:00.263245: The split file contains 1 splits.
2023-08-20 19:55:00.265193: Desired fold for training: 0
2023-08-20 19:55:00.267355: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 19:58:51.305776: dsc: 87.08%
2023-08-20 19:58:51.308051: miou: 77.12%
2023-08-20 19:58:51.309688: acc: 95.90%, sen: 82.52%, spe: 98.59%
2023-08-20 19:58:51.312242: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:58:51.313996: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 19:58:51.315627: finished real validation
2023-08-20 19:59:16.096551: train_loss -0.9143
2023-08-20 19:59:16.099125: val_loss -0.657
2023-08-20 19:59:16.101432: Pseudo dice [0.8622]
2023-08-20 19:59:16.103439: Epoch time: 675.45 s
2023-08-20 19:59:17.429717: 
2023-08-20 19:59:17.432046: Epoch 149
2023-08-20 19:59:17.433901: Current learning rate: 0.0027
2023-08-20 19:59:17.436071: start training, 250
================num of epochs: 250================
2023-08-20 20:06:07.818596: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 20:06:08.159593: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 20:06:08.163410: The split file contains 1 splits.
2023-08-20 20:06:08.165395: Desired fold for training: 0
2023-08-20 20:06:08.167330: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 20:10:02.105507: dsc: 87.42%
2023-08-20 20:10:02.107820: miou: 77.65%
2023-08-20 20:10:02.109652: acc: 95.97%, sen: 83.71%, spe: 98.43%
2023-08-20 20:10:02.112213: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:10:02.113940: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:10:02.115600: finished real validation
2023-08-20 20:10:27.108008: train_loss -0.9084
2023-08-20 20:10:27.110759: val_loss -0.6926
2023-08-20 20:10:27.113278: Pseudo dice [0.8721]
2023-08-20 20:10:27.115290: Epoch time: 669.68 s
2023-08-20 20:10:30.553173: 
2023-08-20 20:10:30.555464: Epoch 150
2023-08-20 20:10:30.557302: Current learning rate: 0.00268
2023-08-20 20:10:30.559445: start training, 250
================num of epochs: 250================
2023-08-20 20:17:21.790555: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 20:17:22.126033: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 20:17:22.129899: The split file contains 1 splits.
2023-08-20 20:17:22.131884: Desired fold for training: 0
2023-08-20 20:17:22.133857: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 20:21:13.594150: dsc: 87.41%
2023-08-20 20:21:13.596593: miou: 77.64%
2023-08-20 20:21:13.598198: acc: 95.97%, sen: 83.45%, spe: 98.49%
2023-08-20 20:21:13.600665: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:21:13.602340: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:21:13.603930: finished real validation
2023-08-20 20:21:38.352670: train_loss -0.9072
2023-08-20 20:21:38.355246: val_loss -0.6527
2023-08-20 20:21:38.357540: Pseudo dice [0.865]
2023-08-20 20:21:38.359402: Epoch time: 667.8 s
2023-08-20 20:21:39.696079: 
2023-08-20 20:21:39.698435: Epoch 151
2023-08-20 20:21:39.700256: Current learning rate: 0.00266
2023-08-20 20:21:39.702450: start training, 250
================num of epochs: 250================
2023-08-20 20:28:28.955564: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 20:28:29.292058: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 20:28:29.295507: The split file contains 1 splits.
2023-08-20 20:28:29.297535: Desired fold for training: 0
2023-08-20 20:28:29.299447: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 20:32:21.749905: dsc: 87.73%
2023-08-20 20:32:21.752197: miou: 78.15%
2023-08-20 20:32:21.753891: acc: 96.02%, sen: 85.06%, spe: 98.22%
2023-08-20 20:32:21.756560: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:32:21.758397: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:32:21.760099: finished real validation
2023-08-20 20:32:48.373879: train_loss -0.9115
2023-08-20 20:32:48.376845: val_loss -0.7011
2023-08-20 20:32:48.379321: Pseudo dice [0.8745]
2023-08-20 20:32:48.381341: Epoch time: 668.68 s
2023-08-20 20:32:49.681708: 
2023-08-20 20:32:49.683944: Epoch 152
2023-08-20 20:32:49.685952: Current learning rate: 0.00265
2023-08-20 20:32:49.688129: start training, 250
================num of epochs: 250================
2023-08-20 20:39:50.017653: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 20:39:50.358976: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 20:39:50.362833: The split file contains 1 splits.
2023-08-20 20:39:50.364789: Desired fold for training: 0
2023-08-20 20:39:50.366848: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 20:43:46.739775: dsc: 87.68%
2023-08-20 20:43:46.742200: miou: 78.06%
2023-08-20 20:43:46.743949: acc: 96.02%, sen: 84.62%, spe: 98.31%
2023-08-20 20:43:46.746543: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:43:46.748297: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:43:46.750048: finished real validation
2023-08-20 20:44:13.657321: train_loss -0.9139
2023-08-20 20:44:13.659938: val_loss -0.6737
2023-08-20 20:44:13.662251: Pseudo dice [0.8635]
2023-08-20 20:44:13.664092: Epoch time: 683.98 s
2023-08-20 20:44:15.028983: 
2023-08-20 20:44:15.031317: Epoch 153
2023-08-20 20:44:15.033142: Current learning rate: 0.00263
2023-08-20 20:44:15.035435: start training, 250
================num of epochs: 250================
2023-08-20 20:51:07.057122: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 20:51:07.389853: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 20:51:07.393775: The split file contains 1 splits.
2023-08-20 20:51:07.395701: Desired fold for training: 0
2023-08-20 20:51:07.397443: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 20:55:00.011484: dsc: 87.90%
2023-08-20 20:55:00.013987: miou: 78.41%
2023-08-20 20:55:00.015856: acc: 96.07%, sen: 85.20%, spe: 98.26%
2023-08-20 20:55:00.018448: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:55:00.020233: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 20:55:00.021949: finished real validation
2023-08-20 20:55:26.776717: train_loss -0.916
2023-08-20 20:55:26.779487: val_loss -0.6985
2023-08-20 20:55:26.782079: Pseudo dice [0.8725]
2023-08-20 20:55:26.784127: Epoch time: 671.75 s
2023-08-20 20:55:28.100637: 
2023-08-20 20:55:28.103193: Epoch 154
2023-08-20 20:55:28.105068: Current learning rate: 0.00262
2023-08-20 20:55:28.107471: start training, 250
================num of epochs: 250================
2023-08-20 21:02:24.057535: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 21:02:24.455744: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 21:02:24.459879: The split file contains 1 splits.
2023-08-20 21:02:24.462365: Desired fold for training: 0
2023-08-20 21:02:24.464706: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 21:06:46.597562: dsc: 87.88%
2023-08-20 21:06:46.600426: miou: 78.38%
2023-08-20 21:06:46.602241: acc: 96.09%, sen: 84.72%, spe: 98.37%
2023-08-20 21:06:46.605342: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:06:46.607061: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:06:46.608721: finished real validation
2023-08-20 21:07:11.634718: train_loss -0.9154
2023-08-20 21:07:11.637360: val_loss -0.6723
2023-08-20 21:07:11.639781: Pseudo dice [0.8724]
2023-08-20 21:07:11.641629: Epoch time: 703.54 s
2023-08-20 21:07:12.949121: 
2023-08-20 21:07:12.951440: Epoch 155
2023-08-20 21:07:12.953289: Current learning rate: 0.0026
2023-08-20 21:07:12.955675: start training, 250
================num of epochs: 250================
2023-08-20 21:14:03.893388: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 21:14:04.242891: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 21:14:04.246633: The split file contains 1 splits.
2023-08-20 21:14:04.248588: Desired fold for training: 0
2023-08-20 21:14:04.250775: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 21:17:56.052905: dsc: 87.83%
2023-08-20 21:17:56.055263: miou: 78.30%
2023-08-20 21:17:56.057038: acc: 96.01%, sen: 86.08%, spe: 98.00%
2023-08-20 21:17:56.059538: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:17:56.061509: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:17:56.063295: finished real validation
2023-08-20 21:18:20.895584: train_loss -0.9175
2023-08-20 21:18:20.898270: val_loss -0.7073
2023-08-20 21:18:20.900517: Pseudo dice [0.8796]
2023-08-20 21:18:20.902457: Epoch time: 667.95 s
2023-08-20 21:18:22.205518: 
2023-08-20 21:18:22.208132: Epoch 156
2023-08-20 21:18:22.210008: Current learning rate: 0.00258
2023-08-20 21:18:22.212358: start training, 250
================num of epochs: 250================
2023-08-20 21:25:13.543811: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 21:25:13.880372: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 21:25:13.883912: The split file contains 1 splits.
2023-08-20 21:25:13.886067: Desired fold for training: 0
2023-08-20 21:25:13.888014: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 21:29:04.984101: dsc: 87.75%
2023-08-20 21:29:04.986790: miou: 78.17%
2023-08-20 21:29:04.988519: acc: 96.02%, sen: 85.08%, spe: 98.22%
2023-08-20 21:29:04.991310: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:29:04.993074: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:29:04.994883: finished real validation
2023-08-20 21:29:29.917336: train_loss -0.9146
2023-08-20 21:29:29.920008: val_loss -0.7262
2023-08-20 21:29:29.922416: Pseudo dice [0.8826]
2023-08-20 21:29:29.924335: Epoch time: 667.72 s
2023-08-20 21:29:31.284054: 
2023-08-20 21:29:31.286417: Epoch 157
2023-08-20 21:29:31.288440: Current learning rate: 0.00257
2023-08-20 21:29:31.290639: start training, 250
================num of epochs: 250================
2023-08-20 21:36:22.451845: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 21:36:22.795559: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 21:36:22.799365: The split file contains 1 splits.
2023-08-20 21:36:22.801551: Desired fold for training: 0
2023-08-20 21:36:22.803476: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 21:40:15.718351: dsc: 87.89%
2023-08-20 21:40:15.720974: miou: 78.39%
2023-08-20 21:40:15.722623: acc: 96.10%, sen: 84.38%, spe: 98.46%
2023-08-20 21:40:15.725785: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:40:15.727662: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:40:15.729462: finished real validation
2023-08-20 21:40:42.469530: train_loss -0.9188
2023-08-20 21:40:42.472394: val_loss -0.675
2023-08-20 21:40:42.474943: Pseudo dice [0.8682]
2023-08-20 21:40:42.476892: Epoch time: 671.19 s
2023-08-20 21:40:43.799798: 
2023-08-20 21:40:43.802420: Epoch 158
2023-08-20 21:40:43.804414: Current learning rate: 0.00255
2023-08-20 21:40:43.806730: start training, 250
================num of epochs: 250================
2023-08-20 21:47:36.239124: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 21:47:36.579773: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 21:47:36.583562: The split file contains 1 splits.
2023-08-20 21:47:36.585513: Desired fold for training: 0
2023-08-20 21:47:36.587309: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 21:51:27.074499: dsc: 87.67%
2023-08-20 21:51:27.076915: miou: 78.05%
2023-08-20 21:51:27.078685: acc: 95.91%, sen: 86.87%, spe: 97.73%
2023-08-20 21:51:27.081381: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:51:27.083141: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 21:51:27.084917: finished real validation
2023-08-20 21:51:51.864919: train_loss -0.918
2023-08-20 21:51:51.867630: val_loss -0.6939
2023-08-20 21:51:51.869976: Pseudo dice [0.8735]
2023-08-20 21:51:51.871945: Epoch time: 668.07 s
2023-08-20 21:51:53.226603: 
2023-08-20 21:51:53.228998: Epoch 159
2023-08-20 21:51:53.230864: Current learning rate: 0.00253
2023-08-20 21:51:53.233303: start training, 250
================num of epochs: 250================
2023-08-20 21:58:44.056130: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 21:58:44.397714: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 21:58:44.401586: The split file contains 1 splits.
2023-08-20 21:58:44.403501: Desired fold for training: 0
2023-08-20 21:58:44.405251: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 22:02:35.850858: dsc: 87.70%
2023-08-20 22:02:35.853056: miou: 78.09%
2023-08-20 22:02:35.854724: acc: 96.07%, sen: 83.51%, spe: 98.60%
2023-08-20 22:02:35.857264: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:02:35.858964: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:02:35.860791: finished real validation
2023-08-20 22:03:00.562260: train_loss -0.9157
2023-08-20 22:03:00.564779: val_loss -0.7199
2023-08-20 22:03:00.566992: Pseudo dice [0.8802]
2023-08-20 22:03:00.568810: Epoch time: 667.34 s
2023-08-20 22:03:01.900223: 
2023-08-20 22:03:01.902988: Epoch 160
2023-08-20 22:03:01.904814: Current learning rate: 0.00252
2023-08-20 22:03:01.907132: start training, 250
================num of epochs: 250================
2023-08-20 22:09:51.804219: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 22:09:52.145556: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 22:09:52.149487: The split file contains 1 splits.
2023-08-20 22:09:52.151637: Desired fold for training: 0
2023-08-20 22:09:52.153595: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 22:13:44.652789: dsc: 87.76%
2023-08-20 22:13:44.655269: miou: 78.19%
2023-08-20 22:13:44.657039: acc: 96.03%, sen: 84.92%, spe: 98.27%
2023-08-20 22:13:44.659696: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:13:44.661653: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:13:44.663371: finished real validation
2023-08-20 22:14:09.522923: train_loss -0.9169
2023-08-20 22:14:09.525729: val_loss -0.6689
2023-08-20 22:14:09.528369: Pseudo dice [0.8675]
2023-08-20 22:14:09.530404: Epoch time: 667.63 s
2023-08-20 22:14:10.827717: 
2023-08-20 22:14:10.830098: Epoch 161
2023-08-20 22:14:10.831983: Current learning rate: 0.0025
2023-08-20 22:14:10.834232: start training, 250
================num of epochs: 250================
2023-08-20 22:21:51.928238: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 22:21:52.377450: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 22:21:52.459297: The split file contains 1 splits.
2023-08-20 22:21:52.461970: Desired fold for training: 0
2023-08-20 22:21:52.464510: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 22:26:31.005329: dsc: 88.09%
2023-08-20 22:26:31.023340: miou: 78.72%
2023-08-20 22:26:31.025243: acc: 96.07%, sen: 86.90%, spe: 97.91%
2023-08-20 22:26:31.028151: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:26:31.029959: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:26:31.031848: finished real validation
2023-08-20 22:27:01.384359: train_loss -0.9192
2023-08-20 22:27:01.391665: val_loss -0.7022
2023-08-20 22:27:01.491708: Pseudo dice [0.879]
2023-08-20 22:27:01.494649: Epoch time: 770.56 s
2023-08-20 22:27:05.171658: 
2023-08-20 22:27:05.248199: Epoch 162
2023-08-20 22:27:05.250189: Current learning rate: 0.00249
2023-08-20 22:27:05.252848: start training, 250
================num of epochs: 250================
2023-08-20 22:34:56.026661: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 22:34:56.683910: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 22:34:56.688874: The split file contains 1 splits.
2023-08-20 22:34:56.795224: Desired fold for training: 0
2023-08-20 22:34:56.904621: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 22:39:09.442172: dsc: 87.45%
2023-08-20 22:39:09.447003: miou: 77.70%
2023-08-20 22:39:09.450515: acc: 95.94%, sen: 84.37%, spe: 98.27%
2023-08-20 22:39:09.454360: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:39:09.457322: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:39:09.460547: finished real validation
2023-08-20 22:39:36.365864: train_loss -0.9192
2023-08-20 22:39:36.795881: val_loss -0.6958
2023-08-20 22:39:36.836384: Pseudo dice [0.869]
2023-08-20 22:39:36.852042: Epoch time: 751.2 s
2023-08-20 22:39:38.541100: 
2023-08-20 22:39:38.591323: Epoch 163
2023-08-20 22:39:38.593313: Current learning rate: 0.00247
2023-08-20 22:39:38.595728: start training, 250
================num of epochs: 250================
2023-08-20 22:47:10.418480: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 22:47:10.801152: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 22:47:10.805973: The split file contains 1 splits.
2023-08-20 22:47:10.808683: Desired fold for training: 0
2023-08-20 22:47:10.811446: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 22:51:24.966319: dsc: 87.79%
2023-08-20 22:51:24.980135: miou: 78.24%
2023-08-20 22:51:24.982074: acc: 96.03%, sen: 85.25%, spe: 98.20%
2023-08-20 22:51:24.985232: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:51:24.987090: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 22:51:24.989811: finished real validation
2023-08-20 22:51:52.873944: train_loss -0.9207
2023-08-20 22:51:52.876695: val_loss -0.7191
2023-08-20 22:51:52.879295: Pseudo dice [0.8788]
2023-08-20 22:51:52.881727: Epoch time: 734.34 s
2023-08-20 22:51:54.323984: 
2023-08-20 22:51:54.326900: Epoch 164
2023-08-20 22:51:54.328824: Current learning rate: 0.00245
2023-08-20 22:51:54.331225: start training, 250
================num of epochs: 250================
2023-08-20 22:58:58.579662: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 22:58:58.913572: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 22:58:58.918124: The split file contains 1 splits.
2023-08-20 22:58:58.921220: Desired fold for training: 0
2023-08-20 22:58:58.923835: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 23:03:10.882919: dsc: 87.94%
2023-08-20 23:03:10.886235: miou: 78.47%
2023-08-20 23:03:10.888122: acc: 96.11%, sen: 84.72%, spe: 98.40%
2023-08-20 23:03:10.890946: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:03:10.892639: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:03:10.894280: finished real validation
2023-08-20 23:03:36.628744: train_loss -0.9173
2023-08-20 23:03:36.631770: val_loss -0.7038
2023-08-20 23:03:36.634533: Pseudo dice [0.8761]
2023-08-20 23:03:36.636860: Epoch time: 702.31 s
2023-08-20 23:03:38.063017: 
2023-08-20 23:03:38.066999: Epoch 165
2023-08-20 23:03:38.068947: Current learning rate: 0.00244
2023-08-20 23:03:38.071398: start training, 250
================num of epochs: 250================
2023-08-20 23:10:42.772165: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 23:10:43.111633: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 23:10:43.119094: The split file contains 1 splits.
2023-08-20 23:10:43.125987: Desired fold for training: 0
2023-08-20 23:10:43.134238: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 23:14:45.401662: dsc: 87.55%
2023-08-20 23:14:45.404226: miou: 77.85%
2023-08-20 23:14:45.405889: acc: 96.02%, sen: 83.57%, spe: 98.52%
2023-08-20 23:14:45.408333: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:14:45.410000: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:14:45.411652: finished real validation
2023-08-20 23:15:10.709446: train_loss -0.9182
2023-08-20 23:15:10.712250: val_loss -0.7177
2023-08-20 23:15:10.714699: Pseudo dice [0.8843]
2023-08-20 23:15:10.716660: Epoch time: 692.65 s
2023-08-20 23:15:12.098013: 
2023-08-20 23:15:12.101203: Epoch 166
2023-08-20 23:15:12.103274: Current learning rate: 0.00242
2023-08-20 23:15:12.105978: start training, 250
================num of epochs: 250================
2023-08-20 23:22:30.428051: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 23:22:30.788542: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 23:22:30.792499: The split file contains 1 splits.
2023-08-20 23:22:30.795033: Desired fold for training: 0
2023-08-20 23:22:30.797444: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 23:26:23.968612: dsc: 87.67%
2023-08-20 23:26:23.981511: miou: 78.05%
2023-08-20 23:26:24.001363: acc: 96.01%, sen: 84.76%, spe: 98.27%
2023-08-20 23:26:24.022167: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:26:24.042780: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:26:24.063931: finished real validation
2023-08-20 23:26:49.529106: train_loss -0.92
2023-08-20 23:26:49.532374: val_loss -0.7299
2023-08-20 23:26:49.534944: Pseudo dice [0.8857]
2023-08-20 23:26:49.537543: Epoch time: 697.43 s
2023-08-20 23:26:50.907177: 
2023-08-20 23:26:50.909764: Epoch 167
2023-08-20 23:26:50.912022: Current learning rate: 0.0024
2023-08-20 23:26:50.914566: start training, 250
================num of epochs: 250================
2023-08-20 23:34:01.826282: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 23:34:02.173202: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 23:34:02.178698: The split file contains 1 splits.
2023-08-20 23:34:02.183210: Desired fold for training: 0
2023-08-20 23:34:02.186551: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 23:38:11.952480: dsc: 87.81%
2023-08-20 23:38:11.956446: miou: 78.27%
2023-08-20 23:38:11.959321: acc: 96.06%, sen: 84.68%, spe: 98.35%
2023-08-20 23:38:11.963604: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:38:11.969208: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:38:11.974152: finished real validation
2023-08-20 23:38:40.346315: train_loss -0.9206
2023-08-20 23:38:40.353167: val_loss -0.7135
2023-08-20 23:38:40.358507: Pseudo dice [0.882]
2023-08-20 23:38:40.362549: Epoch time: 709.44 s
2023-08-20 23:38:41.839779: 
2023-08-20 23:38:41.844718: Epoch 168
2023-08-20 23:38:41.847400: Current learning rate: 0.00239
2023-08-20 23:38:41.851717: start training, 250
================num of epochs: 250================
2023-08-20 23:46:10.009323: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 23:46:10.353891: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 23:46:10.357878: The split file contains 1 splits.
2023-08-20 23:46:10.360041: Desired fold for training: 0
2023-08-20 23:46:10.362475: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-20 23:50:21.058119: dsc: 87.64%
2023-08-20 23:50:21.060708: miou: 78.00%
2023-08-20 23:50:21.062381: acc: 96.04%, sen: 83.82%, spe: 98.50%
2023-08-20 23:50:21.065078: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:50:21.066911: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-20 23:50:21.068594: finished real validation
2023-08-20 23:50:48.599645: train_loss -0.9209
2023-08-20 23:50:48.602555: val_loss -0.6806
2023-08-20 23:50:48.605057: Pseudo dice [0.8767]
2023-08-20 23:50:48.607066: Epoch time: 726.76 s
2023-08-20 23:50:50.075156: 
2023-08-20 23:50:50.077687: Epoch 169
2023-08-20 23:50:50.079701: Current learning rate: 0.00237
2023-08-20 23:50:50.082308: start training, 250
================num of epochs: 250================
2023-08-20 23:58:12.305292: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-20 23:58:12.683327: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-20 23:58:12.687443: The split file contains 1 splits.
2023-08-20 23:58:12.689499: Desired fold for training: 0
2023-08-20 23:58:12.692194: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 00:02:33.519247: dsc: 87.38%
2023-08-21 00:02:34.294431: miou: 77.59%
2023-08-21 00:02:35.274070: acc: 95.96%, sen: 83.47%, spe: 98.47%
2023-08-21 00:02:36.247691: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:02:36.732147: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:02:37.200701: finished real validation
2023-08-21 00:03:05.215287: train_loss -0.9183
2023-08-21 00:03:05.349436: val_loss -0.6421
2023-08-21 00:03:05.358528: Pseudo dice [0.8609]
2023-08-21 00:03:05.367282: Epoch time: 735.14 s
2023-08-21 00:03:07.244880: 
2023-08-21 00:03:07.254261: Epoch 170
2023-08-21 00:03:07.263778: Current learning rate: 0.00236
2023-08-21 00:03:07.273661: start training, 250
================num of epochs: 250================
2023-08-21 00:10:28.477560: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 00:10:29.483424: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 00:10:30.257038: The split file contains 1 splits.
2023-08-21 00:10:30.348429: Desired fold for training: 0
2023-08-21 00:10:30.579800: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 00:16:47.819587: dsc: 87.94%
2023-08-21 00:16:47.822612: miou: 78.48%
2023-08-21 00:16:47.824498: acc: 96.06%, sen: 85.86%, spe: 98.11%
2023-08-21 00:16:47.827989: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:16:47.830009: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:16:47.831847: finished real validation
2023-08-21 00:17:15.880805: train_loss -0.9214
2023-08-21 00:17:15.883622: val_loss -0.6927
2023-08-21 00:17:15.885956: Pseudo dice [0.8827]
2023-08-21 00:17:15.889055: Epoch time: 848.64 s
2023-08-21 00:17:17.430596: 
2023-08-21 00:17:17.434690: Epoch 171
2023-08-21 00:17:17.438299: Current learning rate: 0.00234
2023-08-21 00:17:17.441755: start training, 250
================num of epochs: 250================
2023-08-21 00:24:41.263626: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 00:24:41.667414: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 00:24:41.673097: The split file contains 1 splits.
2023-08-21 00:24:41.678898: Desired fold for training: 0
2023-08-21 00:24:41.684741: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 00:28:54.356590: dsc: 87.63%
2023-08-21 00:28:54.359745: miou: 77.98%
2023-08-21 00:28:54.361567: acc: 96.01%, sen: 84.43%, spe: 98.34%
2023-08-21 00:28:54.364532: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:28:54.367295: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:28:54.369919: finished real validation
2023-08-21 00:29:21.526657: train_loss -0.9205
2023-08-21 00:29:21.530002: val_loss -0.6878
2023-08-21 00:29:21.534385: Pseudo dice [0.8764]
2023-08-21 00:29:21.536843: Epoch time: 724.1 s
2023-08-21 00:29:23.293735: 
2023-08-21 00:29:23.296281: Epoch 172
2023-08-21 00:29:23.298253: Current learning rate: 0.00232
2023-08-21 00:29:23.300778: start training, 250
================num of epochs: 250================
2023-08-21 00:36:41.686676: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 00:36:42.095574: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 00:36:42.099841: The split file contains 1 splits.
2023-08-21 00:36:42.102018: Desired fold for training: 0
2023-08-21 00:36:42.104417: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 00:40:56.833790: dsc: 87.50%
2023-08-21 00:40:56.837238: miou: 77.77%
2023-08-21 00:40:56.839086: acc: 96.03%, sen: 82.87%, spe: 98.68%
2023-08-21 00:40:56.842129: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:40:56.844898: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:40:56.847040: finished real validation
2023-08-21 00:41:25.536377: train_loss -0.9184
2023-08-21 00:41:25.539086: val_loss -0.6853
2023-08-21 00:41:25.543040: Pseudo dice [0.8785]
2023-08-21 00:41:25.545218: Epoch time: 722.25 s
2023-08-21 00:41:27.071512: 
2023-08-21 00:41:27.073947: Epoch 173
2023-08-21 00:41:27.075979: Current learning rate: 0.00231
2023-08-21 00:41:27.078652: start training, 250
================num of epochs: 250================
2023-08-21 00:48:55.232898: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 00:48:55.588765: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 00:48:55.597127: The split file contains 1 splits.
2023-08-21 00:48:55.599691: Desired fold for training: 0
2023-08-21 00:48:55.602259: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 00:54:21.993753: dsc: 87.71%
2023-08-21 00:54:22.105812: miou: 78.10%
2023-08-21 00:54:22.107980: acc: 95.99%, sen: 85.33%, spe: 98.14%
2023-08-21 00:54:22.112985: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:54:22.115518: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 00:54:22.119595: finished real validation
2023-08-21 00:54:50.680114: train_loss -0.9222
2023-08-21 00:54:50.685837: val_loss -0.6973
2023-08-21 00:54:50.689367: Pseudo dice [0.8771]
2023-08-21 00:54:50.698008: Epoch time: 803.61 s
2023-08-21 00:54:53.176366: 
2023-08-21 00:54:53.182472: Epoch 174
2023-08-21 00:54:53.190611: Current learning rate: 0.00229
2023-08-21 00:54:53.199011: start training, 250
================num of epochs: 250================
2023-08-21 01:01:50.637717: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 01:01:50.956033: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 01:01:50.960501: The split file contains 1 splits.
2023-08-21 01:01:50.962791: Desired fold for training: 0
2023-08-21 01:01:50.964957: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 01:05:45.917651: dsc: 87.65%
2023-08-21 01:05:45.920800: miou: 78.02%
2023-08-21 01:05:45.922822: acc: 96.06%, sen: 83.50%, spe: 98.59%
2023-08-21 01:05:45.925670: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:05:45.927430: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:05:45.929188: finished real validation
2023-08-21 01:06:11.678145: train_loss -0.9216
2023-08-21 01:06:11.680704: val_loss -0.6965
2023-08-21 01:06:11.683235: Pseudo dice [0.8783]
2023-08-21 01:06:11.685139: Epoch time: 678.5 s
2023-08-21 01:06:12.995092: 
2023-08-21 01:06:12.997407: Epoch 175
2023-08-21 01:06:12.999165: Current learning rate: 0.00227
2023-08-21 01:06:13.001371: start training, 250
================num of epochs: 250================
2023-08-21 01:13:08.305755: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 01:13:08.638969: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 01:13:08.643019: The split file contains 1 splits.
2023-08-21 01:13:08.646453: Desired fold for training: 0
2023-08-21 01:13:08.649055: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 01:16:58.848038: dsc: 87.38%
2023-08-21 01:16:58.851009: miou: 77.59%
2023-08-21 01:16:58.852777: acc: 95.95%, sen: 83.74%, spe: 98.41%
2023-08-21 01:16:58.855373: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:16:58.857168: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:16:58.858926: finished real validation
2023-08-21 01:17:24.605514: train_loss -0.9203
2023-08-21 01:17:24.608020: val_loss -0.6853
2023-08-21 01:17:24.610358: Pseudo dice [0.8725]
2023-08-21 01:17:24.612329: Epoch time: 671.61 s
2023-08-21 01:17:25.943264: 
2023-08-21 01:17:25.945654: Epoch 176
2023-08-21 01:17:25.947598: Current learning rate: 0.00226
2023-08-21 01:17:25.949882: start training, 250
================num of epochs: 250================
2023-08-21 01:24:20.101482: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 01:24:20.418950: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 01:24:20.423450: The split file contains 1 splits.
2023-08-21 01:24:20.425704: Desired fold for training: 0
2023-08-21 01:24:20.427941: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 01:28:00.896831: dsc: 87.72%
2023-08-21 01:28:00.899409: miou: 78.13%
2023-08-21 01:28:00.901387: acc: 96.02%, sen: 84.84%, spe: 98.27%
2023-08-21 01:28:00.904118: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:28:00.906003: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:28:00.907880: finished real validation
2023-08-21 01:28:26.847072: train_loss -0.9219
2023-08-21 01:28:26.850062: val_loss -0.7016
2023-08-21 01:28:26.852648: Pseudo dice [0.8793]
2023-08-21 01:28:26.854721: Epoch time: 660.91 s
2023-08-21 01:28:28.193604: 
2023-08-21 01:28:28.195955: Epoch 177
2023-08-21 01:28:28.197861: Current learning rate: 0.00224
2023-08-21 01:28:28.200075: start training, 250
================num of epochs: 250================
2023-08-21 01:35:23.427045: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 01:35:23.757365: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 01:35:23.761386: The split file contains 1 splits.
2023-08-21 01:35:23.763931: Desired fold for training: 0
2023-08-21 01:35:23.766436: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 01:39:03.095643: dsc: 87.77%
2023-08-21 01:39:03.097994: miou: 78.21%
2023-08-21 01:39:03.099908: acc: 96.09%, sen: 83.75%, spe: 98.58%
2023-08-21 01:39:03.102579: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:39:03.104328: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:39:03.106111: finished real validation
2023-08-21 01:39:28.747444: train_loss -0.9208
2023-08-21 01:39:28.749797: val_loss -0.6745
2023-08-21 01:39:28.752017: Pseudo dice [0.8746]
2023-08-21 01:39:28.754025: Epoch time: 660.56 s
2023-08-21 01:39:30.097244: 
2023-08-21 01:39:30.099469: Epoch 178
2023-08-21 01:39:30.101399: Current learning rate: 0.00222
2023-08-21 01:39:30.103622: start training, 250
================num of epochs: 250================
2023-08-21 01:46:24.051880: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 01:46:24.388910: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 01:46:24.392512: The split file contains 1 splits.
2023-08-21 01:46:24.394496: Desired fold for training: 0
2023-08-21 01:46:24.396659: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 01:50:08.386318: dsc: 87.95%
2023-08-21 01:50:08.389098: miou: 78.50%
2023-08-21 01:50:08.390950: acc: 96.08%, sen: 85.44%, spe: 98.22%
2023-08-21 01:50:08.393611: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:50:08.395461: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 01:50:08.397240: finished real validation
2023-08-21 01:50:34.413671: train_loss -0.9207
2023-08-21 01:50:34.416318: val_loss -0.7107
2023-08-21 01:50:34.418783: Pseudo dice [0.8824]
2023-08-21 01:50:34.420761: Epoch time: 664.32 s
2023-08-21 01:50:34.422579: Yayy! New best EMA pseudo Dice: 0.8767
2023-08-21 01:50:37.717362: 
2023-08-21 01:50:37.719756: Epoch 179
2023-08-21 01:50:37.721689: Current learning rate: 0.00221
2023-08-21 01:50:37.723922: start training, 250
================num of epochs: 250================
2023-08-21 01:57:36.261523: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 01:57:36.583173: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 01:57:36.587662: The split file contains 1 splits.
2023-08-21 01:57:36.589996: Desired fold for training: 0
2023-08-21 01:57:36.592103: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 02:01:16.641146: dsc: 87.60%
2023-08-21 02:01:16.643888: miou: 77.94%
2023-08-21 02:01:16.645991: acc: 96.03%, sen: 83.66%, spe: 98.52%
2023-08-21 02:01:16.648788: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:01:16.650779: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:01:16.652574: finished real validation
2023-08-21 02:01:42.763537: train_loss -0.9201
2023-08-21 02:01:42.766498: val_loss -0.7338
2023-08-21 02:01:42.769144: Pseudo dice [0.8877]
2023-08-21 02:01:42.771227: Epoch time: 665.05 s
2023-08-21 02:01:42.773071: Yayy! New best EMA pseudo Dice: 0.8778
2023-08-21 02:01:46.262367: 
2023-08-21 02:01:46.264803: Epoch 180
2023-08-21 02:01:46.266679: Current learning rate: 0.00219
2023-08-21 02:01:46.268924: start training, 250
================num of epochs: 250================
2023-08-21 02:08:39.743424: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 02:08:40.079355: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 02:08:40.083064: The split file contains 1 splits.
2023-08-21 02:08:40.085317: Desired fold for training: 0
2023-08-21 02:08:40.087377: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 02:12:52.576618: dsc: 88.02%
2023-08-21 02:12:52.579921: miou: 78.61%
2023-08-21 02:12:52.581984: acc: 96.14%, sen: 84.70%, spe: 98.44%
2023-08-21 02:12:52.584948: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:12:52.587015: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:12:52.588866: finished real validation
2023-08-21 02:13:18.371661: train_loss -0.9238
2023-08-21 02:13:18.374449: val_loss -0.6595
2023-08-21 02:13:18.376928: Pseudo dice [0.8611]
2023-08-21 02:13:18.378901: Epoch time: 692.11 s
2023-08-21 02:13:19.778405: 
2023-08-21 02:13:19.780833: Epoch 181
2023-08-21 02:13:19.782796: Current learning rate: 0.00218
2023-08-21 02:13:19.785065: start training, 250
================num of epochs: 250================
2023-08-21 02:20:15.532264: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 02:20:15.969628: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 02:20:15.973478: The split file contains 1 splits.
2023-08-21 02:20:15.975637: Desired fold for training: 0
2023-08-21 02:20:15.977726: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 02:23:56.603272: dsc: 88.08%
2023-08-21 02:23:56.605648: miou: 78.69%
2023-08-21 02:23:56.607464: acc: 96.11%, sen: 85.75%, spe: 98.20%
2023-08-21 02:23:56.610211: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:23:56.612078: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:23:56.613967: finished real validation
2023-08-21 02:24:22.644381: train_loss -0.9226
2023-08-21 02:24:22.647111: val_loss -0.7054
2023-08-21 02:24:22.649690: Pseudo dice [0.882]
2023-08-21 02:24:22.651692: Epoch time: 662.87 s
2023-08-21 02:24:24.047777: 
2023-08-21 02:24:24.050241: Epoch 182
2023-08-21 02:24:24.052158: Current learning rate: 0.00216
2023-08-21 02:24:24.054496: start training, 250
================num of epochs: 250================
2023-08-21 02:31:18.240133: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 02:31:18.576961: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 02:31:18.581064: The split file contains 1 splits.
2023-08-21 02:31:18.583402: Desired fold for training: 0
2023-08-21 02:31:18.585728: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 02:35:36.026371: dsc: 87.98%
2023-08-21 02:35:36.029106: miou: 78.54%
2023-08-21 02:35:36.031270: acc: 96.11%, sen: 85.05%, spe: 98.33%
2023-08-21 02:35:36.034117: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:35:36.036031: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:35:36.037921: finished real validation
2023-08-21 02:36:02.349421: train_loss -0.9205
2023-08-21 02:36:02.352391: val_loss -0.6968
2023-08-21 02:36:02.355014: Pseudo dice [0.8745]
2023-08-21 02:36:02.357152: Epoch time: 698.3 s
2023-08-21 02:36:03.760581: 
2023-08-21 02:36:03.763075: Epoch 183
2023-08-21 02:36:03.765115: Current learning rate: 0.00214
2023-08-21 02:36:03.767493: start training, 250
================num of epochs: 250================
2023-08-21 02:42:57.978130: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 02:42:58.311407: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 02:42:58.315707: The split file contains 1 splits.
2023-08-21 02:42:58.318086: Desired fold for training: 0
2023-08-21 02:42:58.320210: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 02:46:42.999185: dsc: 87.86%
2023-08-21 02:46:43.001787: miou: 78.34%
2023-08-21 02:46:43.003768: acc: 96.08%, sen: 84.60%, spe: 98.39%
2023-08-21 02:46:43.006680: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:46:43.008672: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:46:43.010633: finished real validation
2023-08-21 02:47:09.488381: train_loss -0.9215
2023-08-21 02:47:09.491054: val_loss -0.7107
2023-08-21 02:47:09.493597: Pseudo dice [0.8827]
2023-08-21 02:47:09.495689: Epoch time: 665.73 s
2023-08-21 02:47:10.865388: 
2023-08-21 02:47:10.868118: Epoch 184
2023-08-21 02:47:10.870113: Current learning rate: 0.00213
2023-08-21 02:47:10.872760: start training, 250
================num of epochs: 250================
2023-08-21 02:54:04.746385: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 02:54:05.080174: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 02:54:05.084168: The split file contains 1 splits.
2023-08-21 02:54:05.086595: Desired fold for training: 0
2023-08-21 02:54:05.088916: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 02:57:43.868504: dsc: 87.94%
2023-08-21 02:57:43.871061: miou: 78.47%
2023-08-21 02:57:43.872938: acc: 96.06%, sen: 85.84%, spe: 98.11%
2023-08-21 02:57:43.875747: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:57:43.877635: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 02:57:43.879606: finished real validation
2023-08-21 02:58:09.634898: train_loss -0.9225
2023-08-21 02:58:09.637609: val_loss -0.6635
2023-08-21 02:58:09.640245: Pseudo dice [0.8634]
2023-08-21 02:58:09.642358: Epoch time: 658.77 s
2023-08-21 02:58:11.011297: 
2023-08-21 02:58:11.013804: Epoch 185
2023-08-21 02:58:11.015831: Current learning rate: 0.00211
2023-08-21 02:58:11.018143: start training, 250
================num of epochs: 250================
2023-08-21 03:05:04.207833: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 03:05:04.541276: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 03:05:04.545828: The split file contains 1 splits.
2023-08-21 03:05:04.548677: Desired fold for training: 0
2023-08-21 03:05:04.551603: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 03:08:45.307347: dsc: 87.96%
2023-08-21 03:08:45.309813: miou: 78.51%
2023-08-21 03:08:45.311687: acc: 96.07%, sen: 85.72%, spe: 98.15%
2023-08-21 03:08:45.314473: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:08:45.316299: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:08:45.318129: finished real validation
2023-08-21 03:09:11.169755: train_loss -0.9248
2023-08-21 03:09:11.172368: val_loss -0.6887
2023-08-21 03:09:11.174861: Pseudo dice [0.8721]
2023-08-21 03:09:11.176826: Epoch time: 660.16 s
2023-08-21 03:09:12.528956: 
2023-08-21 03:09:12.532422: Epoch 186
2023-08-21 03:09:12.534321: Current learning rate: 0.00209
2023-08-21 03:09:12.536668: start training, 250
================num of epochs: 250================
2023-08-21 03:16:06.370269: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 03:16:06.705588: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 03:16:06.709690: The split file contains 1 splits.
2023-08-21 03:16:06.711818: Desired fold for training: 0
2023-08-21 03:16:06.713926: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 03:19:44.265001: dsc: 87.89%
2023-08-21 03:19:44.267434: miou: 78.39%
2023-08-21 03:19:44.269334: acc: 96.10%, sen: 84.45%, spe: 98.44%
2023-08-21 03:19:44.272431: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:19:44.274259: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:19:44.276248: finished real validation
2023-08-21 03:20:10.171533: train_loss -0.9268
2023-08-21 03:20:10.174334: val_loss -0.6947
2023-08-21 03:20:10.176757: Pseudo dice [0.8809]
2023-08-21 03:20:10.178843: Epoch time: 657.65 s
2023-08-21 03:20:11.564291: 
2023-08-21 03:20:11.566617: Epoch 187
2023-08-21 03:20:11.568496: Current learning rate: 0.00208
2023-08-21 03:20:11.570847: start training, 250
================num of epochs: 250================
2023-08-21 03:27:05.967832: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 03:27:06.292755: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 03:27:06.296942: The split file contains 1 splits.
2023-08-21 03:27:06.299841: Desired fold for training: 0
2023-08-21 03:27:06.302263: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 03:30:48.713118: dsc: 87.85%
2023-08-21 03:30:48.715793: miou: 78.33%
2023-08-21 03:30:48.717749: acc: 96.08%, sen: 84.59%, spe: 98.39%
2023-08-21 03:30:48.720649: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:30:48.722816: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:30:48.724737: finished real validation
2023-08-21 03:31:14.592238: train_loss -0.9235
2023-08-21 03:31:14.594926: val_loss -0.698
2023-08-21 03:31:14.597378: Pseudo dice [0.872]
2023-08-21 03:31:14.599486: Epoch time: 663.03 s
2023-08-21 03:31:16.665590: 
2023-08-21 03:31:16.668063: Epoch 188
2023-08-21 03:31:16.670092: Current learning rate: 0.00206
2023-08-21 03:31:16.672429: start training, 250
================num of epochs: 250================
2023-08-21 03:38:10.932344: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 03:38:11.254612: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 03:38:11.258665: The split file contains 1 splits.
2023-08-21 03:38:11.261244: Desired fold for training: 0
2023-08-21 03:38:11.263399: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 03:41:51.664718: dsc: 88.01%
2023-08-21 03:41:51.667383: miou: 78.58%
2023-08-21 03:41:51.669269: acc: 96.13%, sen: 84.83%, spe: 98.40%
2023-08-21 03:41:51.672034: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:41:51.674037: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:41:51.675831: finished real validation
2023-08-21 03:42:17.574912: train_loss -0.9235
2023-08-21 03:42:17.577897: val_loss -0.6785
2023-08-21 03:42:17.580366: Pseudo dice [0.8714]
2023-08-21 03:42:17.582435: Epoch time: 660.91 s
2023-08-21 03:42:18.936178: 
2023-08-21 03:42:18.938683: Epoch 189
2023-08-21 03:42:18.940707: Current learning rate: 0.00204
2023-08-21 03:42:18.942977: start training, 250
================num of epochs: 250================
2023-08-21 03:49:16.533816: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 03:49:16.869349: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 03:49:16.873355: The split file contains 1 splits.
2023-08-21 03:49:16.875474: Desired fold for training: 0
2023-08-21 03:49:16.877845: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 03:52:57.272837: dsc: 88.09%
2023-08-21 03:52:57.275605: miou: 78.71%
2023-08-21 03:52:57.277675: acc: 96.14%, sen: 85.23%, spe: 98.33%
2023-08-21 03:52:57.280738: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:52:57.282667: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 03:52:57.284533: finished real validation
2023-08-21 03:53:23.111386: train_loss -0.9243
2023-08-21 03:53:23.114167: val_loss -0.6961
2023-08-21 03:53:23.116790: Pseudo dice [0.8837]
2023-08-21 03:53:23.118859: Epoch time: 664.18 s
2023-08-21 03:53:24.479224: 
2023-08-21 03:53:24.481934: Epoch 190
2023-08-21 03:53:24.484276: Current learning rate: 0.00203
2023-08-21 03:53:24.486667: start training, 250
================num of epochs: 250================
2023-08-21 04:00:18.441794: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 04:00:18.779633: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 04:00:18.783583: The split file contains 1 splits.
2023-08-21 04:00:18.785744: Desired fold for training: 0
2023-08-21 04:00:18.787689: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 04:03:58.639235: dsc: 87.79%
2023-08-21 04:03:58.643925: miou: 78.24%
2023-08-21 04:03:58.645944: acc: 96.02%, sen: 85.43%, spe: 98.15%
2023-08-21 04:03:58.648753: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 04:03:58.650655: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 04:03:58.652659: finished real validation
2023-08-21 04:04:24.444487: train_loss -0.9236
2023-08-21 04:04:24.447313: val_loss -0.6465
2023-08-21 04:04:24.449869: Pseudo dice [0.8597]
2023-08-21 04:04:24.451977: Epoch time: 659.97 s
2023-08-21 04:04:25.869099: 
2023-08-21 04:04:25.871586: Epoch 191
2023-08-21 04:04:25.873566: Current learning rate: 0.00201
2023-08-21 04:04:25.875983: start training, 250
================num of epochs: 250================
2023-08-21 04:11:23.071346: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 04:11:23.410984: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 04:11:23.414822: The split file contains 1 splits.
2023-08-21 04:11:23.416974: Desired fold for training: 0
2023-08-21 04:11:23.418825: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 04:15:02.484159: dsc: 87.18%
2023-08-21 04:15:02.486887: miou: 77.27%
2023-08-21 04:15:02.488814: acc: 95.88%, sen: 83.65%, spe: 98.34%
2023-08-21 04:15:02.491627: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 04:15:02.493548: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 04:15:02.495431: finished real validation
2023-08-21 04:15:28.439754: train_loss -0.9248
2023-08-21 04:15:28.442714: val_loss -0.6908
2023-08-21 04:15:28.445410: Pseudo dice [0.8748]
2023-08-21 04:15:28.447473: Epoch time: 662.57 s
2023-08-21 04:15:29.815190: 
2023-08-21 04:15:29.817625: Epoch 192
2023-08-21 04:15:29.819645: Current learning rate: 0.00199
2023-08-21 04:15:29.822001: start training, 250
================num of epochs: 250================
2023-08-21 04:22:24.378583: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 04:22:24.709389: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 04:22:24.713888: The split file contains 1 splits.
2023-08-21 04:22:24.716136: Desired fold for training: 0
2023-08-21 04:22:24.718169: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 04:26:04.933894: dsc: 87.98%
2023-08-21 04:26:04.936594: miou: 78.54%
2023-08-21 04:26:04.938561: acc: 96.16%, sen: 83.95%, spe: 98.62%
2023-08-21 04:26:04.941441: current best miou: 0.7877124255588048 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 04:26:04.943400: current best dsc: 0.8812518325620307 at epoch: 131, (131, 0.7877124255588048, 0.8812518325620307)
2023-08-21 04:26:04.945371: finished real validation
2023-08-21 04:26:31.719275: train_loss -0.9194
2023-08-21 04:26:31.722494: val_loss -0.7185
2023-08-21 04:26:31.725313: Pseudo dice [0.8836]
2023-08-21 04:26:31.727588: Epoch time: 661.91 s
2023-08-21 04:26:33.106237: 
2023-08-21 04:26:33.109364: Epoch 193
2023-08-21 04:26:33.111434: Current learning rate: 0.00198
2023-08-21 04:26:33.114256: start training, 250
================num of epochs: 250================
2023-08-21 04:33:27.392451: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 04:33:27.706140: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 04:33:27.710410: The split file contains 1 splits.
2023-08-21 04:33:27.713165: Desired fold for training: 0
2023-08-21 04:33:27.715496: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 04:37:08.615368: dsc: 88.29%
2023-08-21 04:37:08.618819: miou: 79.03%
2023-08-21 04:37:08.621094: acc: 96.21%, sen: 85.21%, spe: 98.43%
2023-08-21 04:37:08.624469: current best miou: 0.7903377318009416 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 04:37:08.626922: current best dsc: 0.8828923367502542 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 04:37:10.795958: finished real validation
2023-08-21 04:37:36.943684: train_loss -0.9262
2023-08-21 04:37:36.946868: val_loss -0.7256
2023-08-21 04:37:36.949592: Pseudo dice [0.8857]
2023-08-21 04:37:36.951679: Epoch time: 663.84 s
2023-08-21 04:37:38.315458: 
2023-08-21 04:37:38.317935: Epoch 194
2023-08-21 04:37:38.319993: Current learning rate: 0.00196
2023-08-21 04:37:38.322435: start training, 250
================num of epochs: 250================
2023-08-21 04:44:33.371027: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 04:44:33.718708: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 04:44:33.722699: The split file contains 1 splits.
2023-08-21 04:44:33.724861: Desired fold for training: 0
2023-08-21 04:44:33.726888: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 04:48:17.089164: dsc: 88.22%
2023-08-21 04:48:17.091903: miou: 78.92%
2023-08-21 04:48:17.093787: acc: 96.18%, sen: 85.49%, spe: 98.33%
2023-08-21 04:48:17.096792: current best miou: 0.7903377318009416 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 04:48:17.099004: current best dsc: 0.8828923367502542 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 04:48:17.100880: finished real validation
2023-08-21 04:48:42.809052: train_loss -0.9256
2023-08-21 04:48:42.811714: val_loss -0.7023
2023-08-21 04:48:42.814003: Pseudo dice [0.8775]
2023-08-21 04:48:42.815940: Epoch time: 664.5 s
2023-08-21 04:48:44.178850: 
2023-08-21 04:48:44.181226: Epoch 195
2023-08-21 04:48:44.183261: Current learning rate: 0.00194
2023-08-21 04:48:44.185618: start training, 250
================num of epochs: 250================
2023-08-21 04:55:36.569611: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 04:55:36.907477: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 04:55:36.915801: The split file contains 1 splits.
2023-08-21 04:55:36.922267: Desired fold for training: 0
2023-08-21 04:55:36.928933: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 04:59:31.938758: dsc: 88.04%
2023-08-21 04:59:31.941641: miou: 78.64%
2023-08-21 04:59:31.943654: acc: 96.08%, sen: 86.04%, spe: 98.11%
2023-08-21 04:59:31.946537: current best miou: 0.7903377318009416 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 04:59:31.948544: current best dsc: 0.8828923367502542 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 04:59:31.950525: finished real validation
2023-08-21 04:59:57.793626: train_loss -0.9258
2023-08-21 04:59:57.796426: val_loss -0.6834
2023-08-21 04:59:57.798837: Pseudo dice [0.8777]
2023-08-21 04:59:57.800848: Epoch time: 673.62 s
2023-08-21 04:59:59.197677: 
2023-08-21 04:59:59.200146: Epoch 196
2023-08-21 04:59:59.202169: Current learning rate: 0.00193
2023-08-21 04:59:59.204546: start training, 250
================num of epochs: 250================
2023-08-21 05:06:53.871834: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 05:06:54.211815: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 05:06:54.215554: The split file contains 1 splits.
2023-08-21 05:06:54.217937: Desired fold for training: 0
2023-08-21 05:06:54.219899: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 05:10:35.495050: dsc: 87.77%
2023-08-21 05:10:35.497624: miou: 78.20%
2023-08-21 05:10:35.499612: acc: 96.03%, sen: 84.98%, spe: 98.26%
2023-08-21 05:10:35.502355: current best miou: 0.7903377318009416 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:10:35.504409: current best dsc: 0.8828923367502542 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:10:35.506353: finished real validation
2023-08-21 05:11:01.225249: train_loss -0.9255
2023-08-21 05:11:01.228159: val_loss -0.6696
2023-08-21 05:11:01.230833: Pseudo dice [0.8771]
2023-08-21 05:11:01.232927: Epoch time: 662.03 s
2023-08-21 05:11:02.602239: 
2023-08-21 05:11:02.604738: Epoch 197
2023-08-21 05:11:02.606717: Current learning rate: 0.00191
2023-08-21 05:11:02.609120: start training, 250
================num of epochs: 250================
2023-08-21 05:17:55.747751: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 05:17:56.087960: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 05:17:56.091813: The split file contains 1 splits.
2023-08-21 05:17:56.093855: Desired fold for training: 0
2023-08-21 05:17:56.095776: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 05:21:47.708424: dsc: 87.88%
2023-08-21 05:21:47.711053: miou: 78.39%
2023-08-21 05:21:47.713050: acc: 96.08%, sen: 84.90%, spe: 98.33%
2023-08-21 05:21:47.715926: current best miou: 0.7903377318009416 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:21:47.717887: current best dsc: 0.8828923367502542 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:21:47.719722: finished real validation
2023-08-21 05:22:14.174638: train_loss -0.9258
2023-08-21 05:22:14.177315: val_loss -0.6789
2023-08-21 05:22:14.179776: Pseudo dice [0.8732]
2023-08-21 05:22:14.182094: Epoch time: 671.58 s
2023-08-21 05:22:15.558732: 
2023-08-21 05:22:15.561023: Epoch 198
2023-08-21 05:22:15.562995: Current learning rate: 0.00189
2023-08-21 05:22:15.565359: start training, 250
================num of epochs: 250================
2023-08-21 05:29:09.808647: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 05:29:10.151368: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 05:29:10.155573: The split file contains 1 splits.
2023-08-21 05:29:10.157911: Desired fold for training: 0
2023-08-21 05:29:10.160056: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 05:32:47.726846: dsc: 87.92%
2023-08-21 05:32:47.729691: miou: 78.45%
2023-08-21 05:32:47.731698: acc: 96.12%, sen: 84.35%, spe: 98.49%
2023-08-21 05:32:47.734572: current best miou: 0.7903377318009416 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:32:47.736542: current best dsc: 0.8828923367502542 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:32:47.738493: finished real validation
2023-08-21 05:33:13.585770: train_loss -0.9269
2023-08-21 05:33:13.588555: val_loss -0.7245
2023-08-21 05:33:13.591018: Pseudo dice [0.886]
2023-08-21 05:33:13.593207: Epoch time: 658.03 s
2023-08-21 05:33:14.987692: 
2023-08-21 05:33:14.990443: Epoch 199
2023-08-21 05:33:14.992550: Current learning rate: 0.00188
2023-08-21 05:33:14.995087: start training, 250
================num of epochs: 250================
2023-08-21 05:40:09.703169: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 05:40:10.038102: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 05:40:10.041962: The split file contains 1 splits.
2023-08-21 05:40:10.044343: Desired fold for training: 0
2023-08-21 05:40:10.046383: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 05:43:49.838560: dsc: 87.66%
2023-08-21 05:43:49.841149: miou: 78.02%
2023-08-21 05:43:49.843274: acc: 96.01%, sen: 84.50%, spe: 98.33%
2023-08-21 05:43:49.846095: current best miou: 0.7903377318009416 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:43:49.848140: current best dsc: 0.8828923367502542 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:43:49.850157: finished real validation
2023-08-21 05:44:15.523535: train_loss -0.9247
2023-08-21 05:44:15.526388: val_loss -0.669
2023-08-21 05:44:15.529143: Pseudo dice [0.8695]
2023-08-21 05:44:15.531396: Epoch time: 660.54 s
2023-08-21 05:44:19.039540: 
2023-08-21 05:44:19.042201: Epoch 200
2023-08-21 05:44:19.044229: Current learning rate: 0.00186
2023-08-21 05:44:19.046617: start training, 250
================num of epochs: 250================
2023-08-21 05:51:12.686572: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 05:51:13.019904: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 05:51:13.024262: The split file contains 1 splits.
2023-08-21 05:51:13.026900: Desired fold for training: 0
2023-08-21 05:51:13.029932: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 05:54:54.545099: dsc: 87.76%
2023-08-21 05:54:54.547874: miou: 78.20%
2023-08-21 05:54:54.549911: acc: 96.04%, sen: 84.74%, spe: 98.32%
2023-08-21 05:54:54.552823: current best miou: 0.7903377318009416 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:54:54.554854: current best dsc: 0.8828923367502542 at epoch: 193, (193, 0.7903377318009416, 0.8828923367502542)
2023-08-21 05:54:54.556801: finished real validation
2023-08-21 05:55:20.333965: train_loss -0.9253
2023-08-21 05:55:20.336712: val_loss -0.6536
2023-08-21 05:55:20.339190: Pseudo dice [0.8676]
2023-08-21 05:55:20.341350: Epoch time: 661.3 s
2023-08-21 05:55:21.703875: 
2023-08-21 05:55:21.706409: Epoch 201
2023-08-21 05:55:21.708415: Current learning rate: 0.00184
2023-08-21 05:55:21.710830: start training, 250
================num of epochs: 250================
2023-08-21 06:02:15.975558: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 06:02:16.319116: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 06:02:16.323307: The split file contains 1 splits.
2023-08-21 06:02:16.325664: Desired fold for training: 0
2023-08-21 06:02:16.327709: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 06:05:56.216562: dsc: 88.59%
2023-08-21 06:05:56.219518: miou: 79.52%
2023-08-21 06:05:56.221572: acc: 96.27%, sen: 86.39%, spe: 98.26%
2023-08-21 06:05:56.224828: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:05:56.227120: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:05:58.310688: finished real validation
2023-08-21 06:06:24.229757: train_loss -0.9272
2023-08-21 06:06:24.232553: val_loss -0.7084
2023-08-21 06:06:24.235161: Pseudo dice [0.8819]
2023-08-21 06:06:24.237326: Epoch time: 662.53 s
2023-08-21 06:06:25.601389: 
2023-08-21 06:06:25.603924: Epoch 202
2023-08-21 06:06:25.606289: Current learning rate: 0.00183
2023-08-21 06:06:25.608682: start training, 250
================num of epochs: 250================
2023-08-21 06:13:19.708156: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 06:13:20.049659: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 06:13:20.053527: The split file contains 1 splits.
2023-08-21 06:13:20.055629: Desired fold for training: 0
2023-08-21 06:13:20.057596: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 06:16:58.320411: dsc: 88.15%
2023-08-21 06:16:58.323348: miou: 78.81%
2023-08-21 06:16:58.325285: acc: 96.17%, sen: 85.07%, spe: 98.40%
2023-08-21 06:16:58.328164: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:16:58.330194: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:16:58.332218: finished real validation
2023-08-21 06:17:24.137808: train_loss -0.927
2023-08-21 06:17:24.140685: val_loss -0.7036
2023-08-21 06:17:24.143119: Pseudo dice [0.8781]
2023-08-21 06:17:24.145317: Epoch time: 658.54 s
2023-08-21 06:17:25.555497: 
2023-08-21 06:17:25.558024: Epoch 203
2023-08-21 06:17:25.560160: Current learning rate: 0.00181
2023-08-21 06:17:25.562647: start training, 250
================num of epochs: 250================
2023-08-21 06:24:19.202212: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 06:24:19.544924: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 06:24:19.549096: The split file contains 1 splits.
2023-08-21 06:24:19.551239: Desired fold for training: 0
2023-08-21 06:24:19.553321: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 06:27:59.947123: dsc: 87.50%
2023-08-21 06:27:59.949699: miou: 77.78%
2023-08-21 06:27:59.951617: acc: 95.95%, sen: 84.70%, spe: 98.21%
2023-08-21 06:27:59.954678: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:27:59.956693: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:27:59.958788: finished real validation
2023-08-21 06:28:25.868208: train_loss -0.9274
2023-08-21 06:28:25.871237: val_loss -0.7129
2023-08-21 06:28:25.873752: Pseudo dice [0.8804]
2023-08-21 06:28:25.875899: Epoch time: 660.32 s
2023-08-21 06:28:27.244455: 
2023-08-21 06:28:27.246962: Epoch 204
2023-08-21 06:28:27.248966: Current learning rate: 0.00179
2023-08-21 06:28:27.251291: start training, 250
================num of epochs: 250================
2023-08-21 06:35:20.881351: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 06:35:21.240633: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 06:35:21.245358: The split file contains 1 splits.
2023-08-21 06:35:21.247974: Desired fold for training: 0
2023-08-21 06:35:21.250396: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 06:39:04.236753: dsc: 87.96%
2023-08-21 06:39:04.239354: miou: 78.51%
2023-08-21 06:39:04.241546: acc: 96.12%, sen: 84.53%, spe: 98.46%
2023-08-21 06:39:04.244476: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:39:04.246697: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:39:04.248699: finished real validation
2023-08-21 06:39:29.894131: train_loss -0.9251
2023-08-21 06:39:29.897386: val_loss -0.6959
2023-08-21 06:39:29.899990: Pseudo dice [0.8721]
2023-08-21 06:39:29.902509: Epoch time: 662.65 s
2023-08-21 06:39:31.323125: 
2023-08-21 06:39:31.325731: Epoch 205
2023-08-21 06:39:31.327980: Current learning rate: 0.00178
2023-08-21 06:39:31.330614: start training, 250
================num of epochs: 250================
2023-08-21 06:46:25.636783: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 06:46:25.974050: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 06:46:25.978147: The split file contains 1 splits.
2023-08-21 06:46:25.980940: Desired fold for training: 0
2023-08-21 06:46:25.983391: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 06:50:07.117957: dsc: 88.06%
2023-08-21 06:50:07.120530: miou: 78.66%
2023-08-21 06:50:07.122515: acc: 96.12%, sen: 85.31%, spe: 98.30%
2023-08-21 06:50:07.125388: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:50:07.127328: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 06:50:07.129371: finished real validation
2023-08-21 06:50:32.653355: train_loss -0.9276
2023-08-21 06:50:32.655847: val_loss -0.6977
2023-08-21 06:50:32.658298: Pseudo dice [0.8813]
2023-08-21 06:50:32.660466: Epoch time: 661.33 s
2023-08-21 06:50:34.298211: 
2023-08-21 06:50:34.300673: Epoch 206
2023-08-21 06:50:34.302696: Current learning rate: 0.00176
2023-08-21 06:50:34.305130: start training, 250
================num of epochs: 250================
2023-08-21 06:57:26.740057: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 06:57:27.072127: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 06:57:27.076649: The split file contains 1 splits.
2023-08-21 06:57:27.079096: Desired fold for training: 0
2023-08-21 06:57:27.081263: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 07:01:07.301360: dsc: 88.05%
2023-08-21 07:01:07.304113: miou: 78.65%
2023-08-21 07:01:07.306117: acc: 96.10%, sen: 85.73%, spe: 98.19%
2023-08-21 07:01:07.308959: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:01:07.311023: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:01:07.312901: finished real validation
2023-08-21 07:01:33.114027: train_loss -0.9298
2023-08-21 07:01:33.116796: val_loss -0.6871
2023-08-21 07:01:33.119242: Pseudo dice [0.8764]
2023-08-21 07:01:33.121381: Epoch time: 658.82 s
2023-08-21 07:01:34.431082: 
2023-08-21 07:01:34.433595: Epoch 207
2023-08-21 07:01:34.435662: Current learning rate: 0.00174
2023-08-21 07:01:34.438107: start training, 250
================num of epochs: 250================
2023-08-21 07:08:27.232141: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 07:08:27.572978: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 07:08:27.576840: The split file contains 1 splits.
2023-08-21 07:08:27.579261: Desired fold for training: 0
2023-08-21 07:08:27.581451: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 07:12:08.235475: dsc: 87.76%
2023-08-21 07:12:08.238157: miou: 78.18%
2023-08-21 07:12:08.240148: acc: 96.04%, sen: 84.77%, spe: 98.30%
2023-08-21 07:12:08.242979: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:12:08.244957: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:12:08.246910: finished real validation
2023-08-21 07:12:34.196525: train_loss -0.9272
2023-08-21 07:12:34.199242: val_loss -0.6972
2023-08-21 07:12:34.201654: Pseudo dice [0.8759]
2023-08-21 07:12:34.203759: Epoch time: 659.77 s
2023-08-21 07:12:35.519876: 
2023-08-21 07:12:35.522450: Epoch 208
2023-08-21 07:12:35.524600: Current learning rate: 0.00173
2023-08-21 07:12:35.527046: start training, 250
================num of epochs: 250================
2023-08-21 07:19:29.730139: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 07:19:30.067583: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 07:19:30.071686: The split file contains 1 splits.
2023-08-21 07:19:30.073985: Desired fold for training: 0
2023-08-21 07:19:30.076205: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 07:23:10.254605: dsc: 88.08%
2023-08-21 07:23:10.257663: miou: 78.70%
2023-08-21 07:23:10.259797: acc: 96.15%, sen: 84.89%, spe: 98.42%
2023-08-21 07:23:10.262876: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:23:10.265003: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:23:10.267000: finished real validation
2023-08-21 07:23:35.930044: train_loss -0.9287
2023-08-21 07:23:35.933743: val_loss -0.6788
2023-08-21 07:23:35.937448: Pseudo dice [0.872]
2023-08-21 07:23:35.939599: Epoch time: 660.41 s
2023-08-21 07:23:37.252123: 
2023-08-21 07:23:37.254659: Epoch 209
2023-08-21 07:23:37.256730: Current learning rate: 0.00171
2023-08-21 07:23:37.259138: start training, 250
================num of epochs: 250================
2023-08-21 07:30:30.391312: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 07:30:30.724537: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 07:30:30.728625: The split file contains 1 splits.
2023-08-21 07:30:30.730877: Desired fold for training: 0
2023-08-21 07:30:30.733073: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 07:34:12.418546: dsc: 88.01%
2023-08-21 07:34:12.421158: miou: 78.58%
2023-08-21 07:34:12.423211: acc: 96.11%, sen: 85.29%, spe: 98.28%
2023-08-21 07:34:12.426170: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:34:12.428130: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:34:12.430093: finished real validation
2023-08-21 07:34:38.194710: train_loss -0.9285
2023-08-21 07:34:38.197788: val_loss -0.6877
2023-08-21 07:34:38.200513: Pseudo dice [0.8738]
2023-08-21 07:34:38.202767: Epoch time: 660.95 s
2023-08-21 07:34:39.576341: 
2023-08-21 07:34:39.578788: Epoch 210
2023-08-21 07:34:39.580924: Current learning rate: 0.00169
2023-08-21 07:34:39.583552: start training, 250
================num of epochs: 250================
2023-08-21 07:41:33.484603: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 07:41:33.824958: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 07:41:33.828960: The split file contains 1 splits.
2023-08-21 07:41:33.831384: Desired fold for training: 0
2023-08-21 07:41:33.833610: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 07:45:14.314702: dsc: 88.20%
2023-08-21 07:45:14.317573: miou: 78.89%
2023-08-21 07:45:14.319685: acc: 96.16%, sen: 85.77%, spe: 98.24%
2023-08-21 07:45:14.322624: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:45:14.324731: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:45:14.326777: finished real validation
2023-08-21 07:45:40.161590: train_loss -0.9303
2023-08-21 07:45:40.164586: val_loss -0.6904
2023-08-21 07:45:40.167315: Pseudo dice [0.8766]
2023-08-21 07:45:40.169559: Epoch time: 660.59 s
2023-08-21 07:45:41.490122: 
2023-08-21 07:45:41.492836: Epoch 211
2023-08-21 07:45:41.494958: Current learning rate: 0.00167
2023-08-21 07:45:41.497394: start training, 250
================num of epochs: 250================
2023-08-21 07:52:35.296062: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 07:52:35.642178: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 07:52:35.646251: The split file contains 1 splits.
2023-08-21 07:52:35.648505: Desired fold for training: 0
2023-08-21 07:52:35.650658: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 07:56:17.155113: dsc: 88.21%
2023-08-21 07:56:17.158309: miou: 78.90%
2023-08-21 07:56:17.160504: acc: 96.17%, sen: 85.61%, spe: 98.29%
2023-08-21 07:56:17.163951: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:56:17.166332: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 07:56:17.168689: finished real validation
2023-08-21 07:56:41.945166: train_loss -0.9279
2023-08-21 07:56:41.948335: val_loss -0.6771
2023-08-21 07:56:41.951022: Pseudo dice [0.8719]
2023-08-21 07:56:41.953285: Epoch time: 660.46 s
2023-08-21 07:56:43.231580: 
2023-08-21 07:56:43.234352: Epoch 212
2023-08-21 07:56:43.236593: Current learning rate: 0.00166
2023-08-21 07:56:43.239217: start training, 250
================num of epochs: 250================
2023-08-21 08:03:37.403139: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 08:03:37.735325: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 08:03:37.739392: The split file contains 1 splits.
2023-08-21 08:03:37.741727: Desired fold for training: 0
2023-08-21 08:03:37.743873: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 08:07:17.410717: dsc: 88.06%
2023-08-21 08:07:17.413449: miou: 78.67%
2023-08-21 08:07:17.415515: acc: 96.14%, sen: 85.03%, spe: 98.37%
2023-08-21 08:07:17.418684: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:07:17.420763: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:07:17.422801: finished real validation
2023-08-21 08:07:43.270793: train_loss -0.9282
2023-08-21 08:07:43.273668: val_loss -0.7034
2023-08-21 08:07:43.276320: Pseudo dice [0.8807]
2023-08-21 08:07:43.278638: Epoch time: 660.04 s
2023-08-21 08:07:44.606791: 
2023-08-21 08:07:44.609451: Epoch 213
2023-08-21 08:07:44.611626: Current learning rate: 0.00164
2023-08-21 08:07:44.614157: start training, 250
================num of epochs: 250================
2023-08-21 08:14:38.586278: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 08:14:38.923771: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 08:14:38.927905: The split file contains 1 splits.
2023-08-21 08:14:38.930283: Desired fold for training: 0
2023-08-21 08:14:38.932811: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 08:18:19.565404: dsc: 87.60%
2023-08-21 08:18:19.568043: miou: 77.94%
2023-08-21 08:18:19.570094: acc: 95.99%, sen: 84.56%, spe: 98.29%
2023-08-21 08:18:19.573053: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:18:19.575058: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:18:19.577129: finished real validation
2023-08-21 08:18:45.493222: train_loss -0.9283
2023-08-21 08:18:45.496357: val_loss -0.6703
2023-08-21 08:18:45.499081: Pseudo dice [0.8724]
2023-08-21 08:18:45.501337: Epoch time: 660.89 s
2023-08-21 08:18:46.814589: 
2023-08-21 08:18:46.817136: Epoch 214
2023-08-21 08:18:46.819406: Current learning rate: 0.00162
2023-08-21 08:18:46.821928: start training, 250
================num of epochs: 250================
2023-08-21 08:25:40.911188: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 08:25:41.251420: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 08:25:41.255635: The split file contains 1 splits.
2023-08-21 08:25:41.258048: Desired fold for training: 0
2023-08-21 08:25:41.260226: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 08:29:22.166449: dsc: 88.16%
2023-08-21 08:29:22.169314: miou: 78.83%
2023-08-21 08:29:22.171383: acc: 96.10%, sen: 86.59%, spe: 98.02%
2023-08-21 08:29:22.174396: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:29:22.176533: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:29:22.178590: finished real validation
2023-08-21 08:29:47.856063: train_loss -0.9301
2023-08-21 08:29:47.859063: val_loss -0.7221
2023-08-21 08:29:47.861870: Pseudo dice [0.8882]
2023-08-21 08:29:47.864112: Epoch time: 661.04 s
2023-08-21 08:29:49.180958: 
2023-08-21 08:29:49.183560: Epoch 215
2023-08-21 08:29:49.185671: Current learning rate: 0.00161
2023-08-21 08:29:49.188179: start training, 250
================num of epochs: 250================
2023-08-21 08:36:42.388860: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 08:36:42.732275: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 08:36:42.736483: The split file contains 1 splits.
2023-08-21 08:36:42.738853: Desired fold for training: 0
2023-08-21 08:36:42.741344: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 08:40:26.076861: dsc: 87.84%
2023-08-21 08:40:26.079383: miou: 78.31%
2023-08-21 08:40:26.081325: acc: 96.06%, sen: 85.03%, spe: 98.27%
2023-08-21 08:40:26.084034: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:40:26.085942: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:40:26.087848: finished real validation
2023-08-21 08:40:51.775973: train_loss -0.9286
2023-08-21 08:40:51.778828: val_loss -0.7161
2023-08-21 08:40:51.781317: Pseudo dice [0.8852]
2023-08-21 08:40:51.783586: Epoch time: 662.6 s
2023-08-21 08:40:51.785662: Yayy! New best EMA pseudo Dice: 0.8778
2023-08-21 08:40:55.253089: 
2023-08-21 08:40:55.255825: Epoch 216
2023-08-21 08:40:55.257980: Current learning rate: 0.00159
2023-08-21 08:40:55.260461: start training, 250
================num of epochs: 250================
2023-08-21 08:47:48.531561: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 08:47:48.861269: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 08:47:48.865247: The split file contains 1 splits.
2023-08-21 08:47:48.867822: Desired fold for training: 0
2023-08-21 08:47:48.870018: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 08:51:28.983711: dsc: 88.00%
2023-08-21 08:51:28.986350: miou: 78.57%
2023-08-21 08:51:28.988456: acc: 96.09%, sen: 85.66%, spe: 98.19%
2023-08-21 08:51:28.991527: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:51:28.993622: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 08:51:28.995749: finished real validation
2023-08-21 08:51:54.708398: train_loss -0.9299
2023-08-21 08:51:54.711672: val_loss -0.7172
2023-08-21 08:51:54.714607: Pseudo dice [0.8885]
2023-08-21 08:51:54.716865: Epoch time: 659.46 s
2023-08-21 08:51:54.718931: Yayy! New best EMA pseudo Dice: 0.8789
2023-08-21 08:51:58.151944: 
2023-08-21 08:51:58.154637: Epoch 217
2023-08-21 08:51:58.157018: Current learning rate: 0.00157
2023-08-21 08:51:58.159570: start training, 250
================num of epochs: 250================
2023-08-21 08:58:51.963696: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 08:58:52.292209: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 08:58:52.296475: The split file contains 1 splits.
2023-08-21 08:58:52.299318: Desired fold for training: 0
2023-08-21 08:58:52.301753: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 09:02:47.754377: dsc: 88.05%
2023-08-21 09:02:47.756859: miou: 78.65%
2023-08-21 09:02:47.758790: acc: 96.11%, sen: 85.65%, spe: 98.21%
2023-08-21 09:02:47.761706: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:02:47.763727: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:02:47.765646: finished real validation
2023-08-21 09:03:13.623095: train_loss -0.9297
2023-08-21 09:03:13.626036: val_loss -0.6699
2023-08-21 09:03:13.628887: Pseudo dice [0.8787]
2023-08-21 09:03:13.631225: Epoch time: 675.47 s
2023-08-21 09:03:14.960615: 
2023-08-21 09:03:14.963273: Epoch 218
2023-08-21 09:03:14.965448: Current learning rate: 0.00156
2023-08-21 09:03:14.968063: start training, 250
================num of epochs: 250================
2023-08-21 09:10:10.277873: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 09:10:10.614622: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 09:10:10.618818: The split file contains 1 splits.
2023-08-21 09:10:10.621229: Desired fold for training: 0
2023-08-21 09:10:10.623438: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 09:13:49.857940: dsc: 88.09%
2023-08-21 09:13:49.860585: miou: 78.72%
2023-08-21 09:13:49.862604: acc: 96.12%, sen: 85.60%, spe: 98.24%
2023-08-21 09:13:49.865621: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:13:49.867879: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:13:49.869993: finished real validation
2023-08-21 09:14:15.650749: train_loss -0.9292
2023-08-21 09:14:15.653513: val_loss -0.6857
2023-08-21 09:14:15.656432: Pseudo dice [0.8771]
2023-08-21 09:14:15.658620: Epoch time: 660.69 s
2023-08-21 09:14:16.971707: 
2023-08-21 09:14:16.974385: Epoch 219
2023-08-21 09:14:16.976526: Current learning rate: 0.00154
2023-08-21 09:14:16.979138: start training, 250
================num of epochs: 250================
2023-08-21 09:21:11.252658: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 09:21:11.669961: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 09:21:11.674550: The split file contains 1 splits.
2023-08-21 09:21:11.677308: Desired fold for training: 0
2023-08-21 09:21:11.680136: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 09:25:10.093992: dsc: 88.22%
2023-08-21 09:25:10.097026: miou: 78.92%
2023-08-21 09:25:10.099171: acc: 96.13%, sen: 86.43%, spe: 98.09%
2023-08-21 09:25:10.102166: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:25:10.104280: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:25:10.106326: finished real validation
2023-08-21 09:25:35.901490: train_loss -0.9294
2023-08-21 09:25:35.904758: val_loss -0.7
2023-08-21 09:25:35.907456: Pseudo dice [0.8811]
2023-08-21 09:25:35.909664: Epoch time: 678.93 s
2023-08-21 09:25:35.911840: Yayy! New best EMA pseudo Dice: 0.8789
2023-08-21 09:25:39.215344: 
2023-08-21 09:25:39.217958: Epoch 220
2023-08-21 09:25:39.220198: Current learning rate: 0.00152
2023-08-21 09:25:39.222731: start training, 250
================num of epochs: 250================
2023-08-21 09:32:34.769832: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 09:32:35.110911: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 09:32:35.115061: The split file contains 1 splits.
2023-08-21 09:32:35.117394: Desired fold for training: 0
2023-08-21 09:32:35.119594: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 09:36:15.835010: dsc: 87.82%
2023-08-21 09:36:15.837887: miou: 78.29%
2023-08-21 09:36:15.840031: acc: 96.03%, sen: 85.38%, spe: 98.18%
2023-08-21 09:36:15.843003: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:36:15.845063: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:36:15.847090: finished real validation
2023-08-21 09:36:41.943934: train_loss -0.9328
2023-08-21 09:36:41.946980: val_loss -0.6628
2023-08-21 09:36:41.949712: Pseudo dice [0.8751]
2023-08-21 09:36:41.951890: Epoch time: 662.73 s
2023-08-21 09:36:43.284498: 
2023-08-21 09:36:43.495893: Epoch 221
2023-08-21 09:36:43.498216: Current learning rate: 0.0015
2023-08-21 09:36:43.500773: start training, 250
================num of epochs: 250================
2023-08-21 09:43:38.702789: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 09:43:39.040972: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 09:43:39.045670: The split file contains 1 splits.
2023-08-21 09:43:39.048128: Desired fold for training: 0
2023-08-21 09:43:39.050889: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 09:47:17.861481: dsc: 88.13%
2023-08-21 09:47:17.864426: miou: 78.78%
2023-08-21 09:47:17.866491: acc: 96.16%, sen: 85.14%, spe: 98.38%
2023-08-21 09:47:17.869482: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:47:17.871571: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:47:17.873596: finished real validation
2023-08-21 09:47:43.648533: train_loss -0.9311
2023-08-21 09:47:43.651320: val_loss -0.6858
2023-08-21 09:47:43.653954: Pseudo dice [0.8761]
2023-08-21 09:47:43.656195: Epoch time: 660.37 s
2023-08-21 09:47:44.973110: 
2023-08-21 09:47:44.975666: Epoch 222
2023-08-21 09:47:44.977802: Current learning rate: 0.00149
2023-08-21 09:47:44.980290: start training, 250
================num of epochs: 250================
2023-08-21 09:54:39.967925: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 09:54:40.294871: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 09:54:40.299865: The split file contains 1 splits.
2023-08-21 09:54:40.304842: Desired fold for training: 0
2023-08-21 09:54:40.307453: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 09:58:20.543349: dsc: 87.84%
2023-08-21 09:58:20.546615: miou: 78.31%
2023-08-21 09:58:20.549174: acc: 96.04%, sen: 85.42%, spe: 98.17%
2023-08-21 09:58:20.552808: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:58:20.555073: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 09:58:20.557517: finished real validation
2023-08-21 09:58:46.600406: train_loss -0.9318
2023-08-21 09:58:46.603705: val_loss -0.7085
2023-08-21 09:58:46.606695: Pseudo dice [0.8851]
2023-08-21 09:58:46.609079: Epoch time: 661.63 s
2023-08-21 09:58:46.611356: Yayy! New best EMA pseudo Dice: 0.879
2023-08-21 09:58:50.260620: 
2023-08-21 09:58:50.263429: Epoch 223
2023-08-21 09:58:50.265679: Current learning rate: 0.00147
2023-08-21 09:58:50.268265: start training, 250
================num of epochs: 250================
2023-08-21 10:05:45.890563: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 10:05:46.224607: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 10:05:46.228917: The split file contains 1 splits.
2023-08-21 10:05:46.231326: Desired fold for training: 0
2023-08-21 10:05:46.233463: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 10:09:25.349698: dsc: 88.37%
2023-08-21 10:09:25.352363: miou: 79.16%
2023-08-21 10:09:25.354580: acc: 96.17%, sen: 86.93%, spe: 98.03%
2023-08-21 10:09:25.357731: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:09:25.359912: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:09:25.362096: finished real validation
2023-08-21 10:09:51.228606: train_loss -0.9324
2023-08-21 10:09:51.232915: val_loss -0.7312
2023-08-21 10:09:51.236963: Pseudo dice [0.8953]
2023-08-21 10:09:51.240786: Epoch time: 660.97 s
2023-08-21 10:09:51.244326: Yayy! New best EMA pseudo Dice: 0.8806
2023-08-21 10:09:54.561740: 
2023-08-21 10:09:54.564300: Epoch 224
2023-08-21 10:09:54.566417: Current learning rate: 0.00145
2023-08-21 10:09:54.568895: start training, 250
================num of epochs: 250================
2023-08-21 10:17:01.711282: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 10:17:02.231803: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 10:17:02.237219: The split file contains 1 splits.
2023-08-21 10:17:02.360260: Desired fold for training: 0
2023-08-21 10:17:02.365131: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 10:22:55.609831: dsc: 88.09%
2023-08-21 10:22:55.797295: miou: 78.72%
2023-08-21 10:22:55.995525: acc: 96.12%, sen: 85.66%, spe: 98.23%
2023-08-21 10:22:55.999027: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:22:56.096917: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:22:56.196867: finished real validation
2023-08-21 10:23:24.038043: train_loss -0.9333
2023-08-21 10:23:24.249930: val_loss -0.6815
2023-08-21 10:23:24.252751: Pseudo dice [0.8782]
2023-08-21 10:23:24.540477: Epoch time: 809.48 s
2023-08-21 10:23:26.875069: 
2023-08-21 10:23:26.877757: Epoch 225
2023-08-21 10:23:26.880162: Current learning rate: 0.00144
2023-08-21 10:23:26.883309: start training, 250
================num of epochs: 250================
2023-08-21 10:31:25.341773: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 10:31:25.686308: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 10:31:25.691002: The split file contains 1 splits.
2023-08-21 10:31:25.694850: Desired fold for training: 0
2023-08-21 10:31:25.697827: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 10:35:25.256939: dsc: 88.05%
2023-08-21 10:35:25.259851: miou: 78.66%
2023-08-21 10:35:25.261996: acc: 96.11%, sen: 85.58%, spe: 98.23%
2023-08-21 10:35:25.265001: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:35:25.267107: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:35:25.269230: finished real validation
2023-08-21 10:35:51.079000: train_loss -0.9323
2023-08-21 10:35:51.081664: val_loss -0.6979
2023-08-21 10:35:51.084061: Pseudo dice [0.8809]
2023-08-21 10:35:51.086171: Epoch time: 744.21 s
2023-08-21 10:35:52.384236: 
2023-08-21 10:35:52.387038: Epoch 226
2023-08-21 10:35:52.389261: Current learning rate: 0.00142
2023-08-21 10:35:52.391797: start training, 250
================num of epochs: 250================
2023-08-21 10:42:44.462578: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 10:42:44.798891: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 10:42:44.804169: The split file contains 1 splits.
2023-08-21 10:42:44.808221: Desired fold for training: 0
2023-08-21 10:42:44.810987: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 10:46:21.979265: dsc: 87.97%
2023-08-21 10:46:21.982007: miou: 78.53%
2023-08-21 10:46:21.984087: acc: 96.11%, sen: 84.92%, spe: 98.36%
2023-08-21 10:46:21.987039: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:46:21.989071: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:46:21.991092: finished real validation
2023-08-21 10:46:47.672851: train_loss -0.9314
2023-08-21 10:46:47.675741: val_loss -0.6668
2023-08-21 10:46:47.678406: Pseudo dice [0.8756]
2023-08-21 10:46:47.680590: Epoch time: 655.29 s
2023-08-21 10:46:48.994125: 
2023-08-21 10:46:48.996828: Epoch 227
2023-08-21 10:46:48.999028: Current learning rate: 0.0014
2023-08-21 10:46:49.001706: start training, 250
================num of epochs: 250================
2023-08-21 10:53:41.738530: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 10:53:42.374419: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 10:53:42.379022: The split file contains 1 splits.
2023-08-21 10:53:42.381942: Desired fold for training: 0
2023-08-21 10:53:42.384679: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 10:57:22.757435: dsc: 88.09%
2023-08-21 10:57:22.760902: miou: 78.71%
2023-08-21 10:57:22.762913: acc: 96.17%, sen: 84.63%, spe: 98.49%
2023-08-21 10:57:22.765861: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:57:22.767871: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 10:57:22.769873: finished real validation
2023-08-21 10:57:48.158361: train_loss -0.9323
2023-08-21 10:57:48.161153: val_loss -0.7024
2023-08-21 10:57:48.163735: Pseudo dice [0.8801]
2023-08-21 10:57:48.166019: Epoch time: 659.17 s
2023-08-21 10:57:49.457408: 
2023-08-21 10:57:49.459874: Epoch 228
2023-08-21 10:57:49.462367: Current learning rate: 0.00138
2023-08-21 10:57:49.464891: start training, 250
================num of epochs: 250================
2023-08-21 11:04:40.146886: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 11:04:40.486856: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 11:04:40.491239: The split file contains 1 splits.
2023-08-21 11:04:40.494019: Desired fold for training: 0
2023-08-21 11:04:40.496335: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 11:08:17.510865: dsc: 87.88%
2023-08-21 11:08:17.514227: miou: 78.38%
2023-08-21 11:08:17.516481: acc: 96.09%, sen: 84.70%, spe: 98.38%
2023-08-21 11:08:17.519516: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:08:17.521667: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:08:17.523694: finished real validation
2023-08-21 11:08:43.844230: train_loss -0.9327
2023-08-21 11:08:43.847379: val_loss -0.6828
2023-08-21 11:08:43.850147: Pseudo dice [0.8772]
2023-08-21 11:08:43.852386: Epoch time: 654.39 s
2023-08-21 11:08:45.159698: 
2023-08-21 11:08:45.162316: Epoch 229
2023-08-21 11:08:45.164545: Current learning rate: 0.00137
2023-08-21 11:08:45.167174: start training, 250
================num of epochs: 250================
2023-08-21 11:15:37.381620: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 11:15:37.715693: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 11:15:37.719988: The split file contains 1 splits.
2023-08-21 11:15:37.722510: Desired fold for training: 0
2023-08-21 11:15:37.724861: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 11:19:17.927950: dsc: 88.21%
2023-08-21 11:19:17.930767: miou: 78.91%
2023-08-21 11:19:17.932823: acc: 96.14%, sen: 86.12%, spe: 98.16%
2023-08-21 11:19:17.935825: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:19:17.937988: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:19:17.940057: finished real validation
2023-08-21 11:19:43.670684: train_loss -0.9328
2023-08-21 11:19:43.673564: val_loss -0.6938
2023-08-21 11:19:43.676218: Pseudo dice [0.8839]
2023-08-21 11:19:43.678505: Epoch time: 658.51 s
2023-08-21 11:19:44.984315: 
2023-08-21 11:19:44.987632: Epoch 230
2023-08-21 11:19:44.990421: Current learning rate: 0.00135
2023-08-21 11:19:44.993556: start training, 250
================num of epochs: 250================
2023-08-21 11:26:36.119314: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 11:26:36.458412: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 11:26:36.462580: The split file contains 1 splits.
2023-08-21 11:26:36.465581: Desired fold for training: 0
2023-08-21 11:26:36.468606: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 11:30:17.517623: dsc: 87.90%
2023-08-21 11:30:17.520396: miou: 78.42%
2023-08-21 11:30:17.522516: acc: 96.08%, sen: 84.98%, spe: 98.31%
2023-08-21 11:30:17.525648: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:30:17.527974: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:30:17.530047: finished real validation
2023-08-21 11:30:43.346372: train_loss -0.9339
2023-08-21 11:30:43.349666: val_loss -0.6805
2023-08-21 11:30:43.352596: Pseudo dice [0.8755]
2023-08-21 11:30:43.354878: Epoch time: 658.36 s
2023-08-21 11:30:44.665075: 
2023-08-21 11:30:44.668139: Epoch 231
2023-08-21 11:30:44.670584: Current learning rate: 0.00133
2023-08-21 11:30:44.673203: start training, 250
================num of epochs: 250================
2023-08-21 11:37:37.597134: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 11:37:37.923299: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 11:37:37.927542: The split file contains 1 splits.
2023-08-21 11:37:37.930047: Desired fold for training: 0
2023-08-21 11:37:37.932448: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 11:41:18.413475: dsc: 87.86%
2023-08-21 11:41:18.416172: miou: 78.35%
2023-08-21 11:41:18.418365: acc: 96.12%, sen: 83.89%, spe: 98.58%
2023-08-21 11:41:18.421384: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:41:18.423522: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:41:18.425788: finished real validation
2023-08-21 11:41:44.263494: train_loss -0.9331
2023-08-21 11:41:44.266593: val_loss -0.6803
2023-08-21 11:41:44.269516: Pseudo dice [0.8778]
2023-08-21 11:41:44.271757: Epoch time: 659.6 s
2023-08-21 11:41:45.580961: 
2023-08-21 11:41:45.583737: Epoch 232
2023-08-21 11:41:45.585994: Current learning rate: 0.00131
2023-08-21 11:41:45.588635: start training, 250
================num of epochs: 250================
2023-08-21 11:48:38.556990: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 11:48:38.896709: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 11:48:38.901255: The split file contains 1 splits.
2023-08-21 11:48:38.903646: Desired fold for training: 0
2023-08-21 11:48:38.906531: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 11:52:15.998237: dsc: 87.98%
2023-08-21 11:52:16.001777: miou: 78.53%
2023-08-21 11:52:16.004537: acc: 96.13%, sen: 84.52%, spe: 98.47%
2023-08-21 11:52:16.007807: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:52:16.010043: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 11:52:16.012100: finished real validation
2023-08-21 11:52:41.811336: train_loss -0.9324
2023-08-21 11:52:41.814381: val_loss -0.6436
2023-08-21 11:52:41.817138: Pseudo dice [0.8631]
2023-08-21 11:52:41.819452: Epoch time: 656.23 s
2023-08-21 11:52:43.455935: 
2023-08-21 11:52:43.459001: Epoch 233
2023-08-21 11:52:43.461398: Current learning rate: 0.0013
2023-08-21 11:52:43.464096: start training, 250
================num of epochs: 250================
2023-08-21 11:59:35.170565: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 11:59:35.501239: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 11:59:35.506044: The split file contains 1 splits.
2023-08-21 11:59:35.508874: Desired fold for training: 0
2023-08-21 11:59:35.511421: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 12:03:15.913525: dsc: 88.07%
2023-08-21 12:03:15.916240: miou: 78.68%
2023-08-21 12:03:15.918409: acc: 96.15%, sen: 84.75%, spe: 98.45%
2023-08-21 12:03:15.921495: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:03:15.923706: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:03:15.925769: finished real validation
2023-08-21 12:03:41.622805: train_loss -0.9343
2023-08-21 12:03:41.626475: val_loss -0.6829
2023-08-21 12:03:41.630943: Pseudo dice [0.8803]
2023-08-21 12:03:41.633972: Epoch time: 658.17 s
2023-08-21 12:03:42.941301: 
2023-08-21 12:03:42.944160: Epoch 234
2023-08-21 12:03:42.946416: Current learning rate: 0.00128
2023-08-21 12:03:42.949024: start training, 250
================num of epochs: 250================
2023-08-21 12:10:35.370160: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 12:10:35.705149: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 12:10:35.709291: The split file contains 1 splits.
2023-08-21 12:10:35.712141: Desired fold for training: 0
2023-08-21 12:10:35.714664: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 12:14:13.576728: dsc: 88.06%
2023-08-21 12:14:13.580217: miou: 78.67%
2023-08-21 12:14:13.583036: acc: 96.15%, sen: 84.69%, spe: 98.46%
2023-08-21 12:14:13.586670: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:14:13.589508: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:14:13.592277: finished real validation
2023-08-21 12:14:39.290684: train_loss -0.9344
2023-08-21 12:14:39.293413: val_loss -0.6889
2023-08-21 12:14:39.295952: Pseudo dice [0.8872]
2023-08-21 12:14:39.298251: Epoch time: 656.35 s
2023-08-21 12:14:40.612213: 
2023-08-21 12:14:40.614792: Epoch 235
2023-08-21 12:14:40.616945: Current learning rate: 0.00126
2023-08-21 12:14:40.619416: start training, 250
================num of epochs: 250================
2023-08-21 12:21:38.519872: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 12:21:38.838032: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 12:21:38.842313: The split file contains 1 splits.
2023-08-21 12:21:38.845389: Desired fold for training: 0
2023-08-21 12:21:38.848393: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 12:25:20.162011: dsc: 88.01%
2023-08-21 12:25:20.165226: miou: 78.58%
2023-08-21 12:25:20.167479: acc: 96.12%, sen: 84.90%, spe: 98.38%
2023-08-21 12:25:20.170669: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:25:20.172850: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:25:20.175056: finished real validation
2023-08-21 12:25:46.235136: train_loss -0.9348
2023-08-21 12:25:46.238423: val_loss -0.6468
2023-08-21 12:25:46.241498: Pseudo dice [0.8689]
2023-08-21 12:25:46.243860: Epoch time: 665.63 s
2023-08-21 12:25:47.587052: 
2023-08-21 12:25:47.589908: Epoch 236
2023-08-21 12:25:47.592240: Current learning rate: 0.00124
2023-08-21 12:25:47.594882: start training, 250
================num of epochs: 250================
2023-08-21 12:32:43.602576: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 12:32:43.936949: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 12:32:43.941361: The split file contains 1 splits.
2023-08-21 12:32:43.944145: Desired fold for training: 0
2023-08-21 12:32:43.946442: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 12:36:22.792765: dsc: 88.09%
2023-08-21 12:36:22.795642: miou: 78.71%
2023-08-21 12:36:22.797785: acc: 96.14%, sen: 85.24%, spe: 98.33%
2023-08-21 12:36:22.800647: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:36:22.802755: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:36:22.804896: finished real validation
2023-08-21 12:36:48.632562: train_loss -0.9343
2023-08-21 12:36:48.635733: val_loss -0.6658
2023-08-21 12:36:48.638695: Pseudo dice [0.868]
2023-08-21 12:36:48.640967: Epoch time: 661.05 s
2023-08-21 12:36:49.957580: 
2023-08-21 12:36:49.960146: Epoch 237
2023-08-21 12:36:49.962429: Current learning rate: 0.00123
2023-08-21 12:36:49.965100: start training, 250
================num of epochs: 250================
2023-08-21 12:43:43.759619: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 12:43:44.080233: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 12:43:44.084510: The split file contains 1 splits.
2023-08-21 12:43:44.087134: Desired fold for training: 0
2023-08-21 12:43:44.091166: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 12:47:23.127381: dsc: 88.18%
2023-08-21 12:47:23.130423: miou: 78.86%
2023-08-21 12:47:23.132540: acc: 96.17%, sen: 85.35%, spe: 98.35%
2023-08-21 12:47:23.135528: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:47:23.137658: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:47:23.139696: finished real validation
2023-08-21 12:47:48.760530: train_loss -0.9328
2023-08-21 12:47:48.763675: val_loss -0.7247
2023-08-21 12:47:48.766527: Pseudo dice [0.8886]
2023-08-21 12:47:48.768825: Epoch time: 658.81 s
2023-08-21 12:47:50.060863: 
2023-08-21 12:47:50.063520: Epoch 238
2023-08-21 12:47:50.065729: Current learning rate: 0.00121
2023-08-21 12:47:50.068364: start training, 250
================num of epochs: 250================
2023-08-21 12:54:43.124585: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 12:54:43.845330: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 12:54:43.849307: The split file contains 1 splits.
2023-08-21 12:54:43.851519: Desired fold for training: 0
2023-08-21 12:54:43.853575: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 12:58:21.876821: dsc: 87.85%
2023-08-21 12:58:21.879781: miou: 78.33%
2023-08-21 12:58:21.882138: acc: 96.08%, sen: 84.63%, spe: 98.38%
2023-08-21 12:58:21.885377: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:58:21.887649: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 12:58:21.889825: finished real validation
2023-08-21 12:58:47.630038: train_loss -0.9347
2023-08-21 12:58:47.633020: val_loss -0.6383
2023-08-21 12:58:47.635738: Pseudo dice [0.8696]
2023-08-21 12:58:47.638050: Epoch time: 657.57 s
2023-08-21 12:58:48.940376: 
2023-08-21 12:58:48.943543: Epoch 239
2023-08-21 12:58:48.945895: Current learning rate: 0.00119
2023-08-21 12:58:48.948512: start training, 250
================num of epochs: 250================
2023-08-21 13:05:41.620300: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 13:05:41.956962: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 13:05:41.961511: The split file contains 1 splits.
2023-08-21 13:05:41.964496: Desired fold for training: 0
2023-08-21 13:05:41.967265: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 13:09:24.272935: dsc: 87.98%
2023-08-21 13:09:24.276035: miou: 78.55%
2023-08-21 13:09:24.278198: acc: 96.04%, sen: 86.56%, spe: 97.95%
2023-08-21 13:09:24.281176: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:09:24.283458: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:09:24.285600: finished real validation
2023-08-21 13:09:50.019542: train_loss -0.9346
2023-08-21 13:09:50.022753: val_loss -0.6926
2023-08-21 13:09:50.025613: Pseudo dice [0.8813]
2023-08-21 13:09:50.027849: Epoch time: 661.08 s
2023-08-21 13:09:51.334067: 
2023-08-21 13:09:51.336847: Epoch 240
2023-08-21 13:09:51.339251: Current learning rate: 0.00117
2023-08-21 13:09:51.341938: start training, 250
================num of epochs: 250================
2023-08-21 13:16:44.065877: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 13:16:44.390848: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 13:16:44.395147: The split file contains 1 splits.
2023-08-21 13:16:44.398129: Desired fold for training: 0
2023-08-21 13:16:44.401800: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 13:20:22.739447: dsc: 87.86%
2023-08-21 13:20:22.743473: miou: 78.36%
2023-08-21 13:20:22.745885: acc: 96.10%, sen: 84.35%, spe: 98.46%
2023-08-21 13:20:22.749032: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:20:22.751426: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:20:22.754720: finished real validation
2023-08-21 13:20:48.684582: train_loss -0.9349
2023-08-21 13:20:48.687518: val_loss -0.6405
2023-08-21 13:20:48.690727: Pseudo dice [0.8676]
2023-08-21 13:20:48.693104: Epoch time: 657.35 s
2023-08-21 13:20:50.013981: 
2023-08-21 13:20:50.016812: Epoch 241
2023-08-21 13:20:50.018955: Current learning rate: 0.00116
2023-08-21 13:20:50.021403: start training, 250
================num of epochs: 250================
2023-08-21 13:27:41.372304: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 13:27:41.712610: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 13:27:41.717323: The split file contains 1 splits.
2023-08-21 13:27:41.720344: Desired fold for training: 0
2023-08-21 13:27:41.723173: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 13:31:25.571747: dsc: 87.90%
2023-08-21 13:31:25.574851: miou: 78.41%
2023-08-21 13:31:25.577086: acc: 96.07%, sen: 85.28%, spe: 98.24%
2023-08-21 13:31:25.580312: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:31:25.582533: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:31:25.584714: finished real validation
2023-08-21 13:31:51.488039: train_loss -0.9359
2023-08-21 13:31:51.491303: val_loss -0.6659
2023-08-21 13:31:51.493986: Pseudo dice [0.8725]
2023-08-21 13:31:51.496378: Epoch time: 661.48 s
2023-08-21 13:31:52.820664: 
2023-08-21 13:31:52.823338: Epoch 242
2023-08-21 13:31:52.825604: Current learning rate: 0.00114
2023-08-21 13:31:52.828274: start training, 250
================num of epochs: 250================
2023-08-21 13:38:45.364203: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 13:38:45.689616: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 13:38:45.694228: The split file contains 1 splits.
2023-08-21 13:38:45.696560: Desired fold for training: 0
2023-08-21 13:38:45.699248: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 13:42:25.370341: dsc: 88.33%
2023-08-21 13:42:25.373274: miou: 79.11%
2023-08-21 13:42:25.375454: acc: 96.17%, sen: 86.55%, spe: 98.11%
2023-08-21 13:42:25.378591: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:42:25.380757: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:42:25.382992: finished real validation
2023-08-21 13:42:51.158921: train_loss -0.9351
2023-08-21 13:42:51.162097: val_loss -0.6887
2023-08-21 13:42:51.164883: Pseudo dice [0.8803]
2023-08-21 13:42:51.167274: Epoch time: 658.34 s
2023-08-21 13:42:52.480695: 
2023-08-21 13:42:52.483394: Epoch 243
2023-08-21 13:42:52.485733: Current learning rate: 0.00112
2023-08-21 13:42:52.488391: start training, 250
================num of epochs: 250================
2023-08-21 13:49:44.977577: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 13:49:45.301434: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 13:49:45.306487: The split file contains 1 splits.
2023-08-21 13:49:45.309695: Desired fold for training: 0
2023-08-21 13:49:45.312085: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 13:53:24.490159: dsc: 88.16%
2023-08-21 13:53:24.493147: miou: 78.83%
2023-08-21 13:53:24.495345: acc: 96.16%, sen: 85.38%, spe: 98.33%
2023-08-21 13:53:24.498326: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:53:24.500505: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 13:53:24.502573: finished real validation
2023-08-21 13:53:50.304743: train_loss -0.9362
2023-08-21 13:53:50.307721: val_loss -0.6948
2023-08-21 13:53:50.310369: Pseudo dice [0.8818]
2023-08-21 13:53:50.312726: Epoch time: 657.83 s
2023-08-21 13:53:51.629953: 
2023-08-21 13:53:51.632736: Epoch 244
2023-08-21 13:53:51.635019: Current learning rate: 0.0011
2023-08-21 13:53:51.637553: start training, 250
================num of epochs: 250================
2023-08-21 14:00:43.648604: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 14:00:43.982229: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 14:00:43.986752: The split file contains 1 splits.
2023-08-21 14:00:43.989530: Desired fold for training: 0
2023-08-21 14:00:43.992145: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 14:04:22.820448: dsc: 88.02%
2023-08-21 14:04:22.823317: miou: 78.60%
2023-08-21 14:04:22.825484: acc: 96.12%, sen: 85.14%, spe: 98.33%
2023-08-21 14:04:22.828554: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:04:22.830805: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:04:22.832997: finished real validation
2023-08-21 14:04:48.543632: train_loss -0.9364
2023-08-21 14:04:48.546768: val_loss -0.7008
2023-08-21 14:04:48.549421: Pseudo dice [0.8856]
2023-08-21 14:04:48.551666: Epoch time: 656.92 s
2023-08-21 14:04:49.872704: 
2023-08-21 14:04:49.875436: Epoch 245
2023-08-21 14:04:49.879289: Current learning rate: 0.00109
2023-08-21 14:04:49.881968: start training, 250
================num of epochs: 250================
2023-08-21 14:11:40.590307: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 14:11:40.924035: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 14:11:40.928257: The split file contains 1 splits.
2023-08-21 14:11:40.930944: Desired fold for training: 0
2023-08-21 14:11:40.934372: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 14:15:20.559695: dsc: 88.22%
2023-08-21 14:15:20.562586: miou: 78.92%
2023-08-21 14:15:20.564835: acc: 96.19%, sen: 85.22%, spe: 98.39%
2023-08-21 14:15:20.568146: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:15:20.571418: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:15:20.574419: finished real validation
2023-08-21 14:15:46.248823: train_loss -0.9368
2023-08-21 14:15:46.251817: val_loss -0.6904
2023-08-21 14:15:46.255059: Pseudo dice [0.8838]
2023-08-21 14:15:46.257649: Epoch time: 656.38 s
2023-08-21 14:15:47.576240: 
2023-08-21 14:15:47.578992: Epoch 246
2023-08-21 14:15:47.581384: Current learning rate: 0.00107
2023-08-21 14:15:47.584121: start training, 250
================num of epochs: 250================
2023-08-21 14:22:39.207580: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 14:22:39.541824: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 14:22:39.546279: The split file contains 1 splits.
2023-08-21 14:22:39.549207: Desired fold for training: 0
2023-08-21 14:22:39.551560: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 14:26:18.625909: dsc: 87.98%
2023-08-21 14:26:18.628764: miou: 78.54%
2023-08-21 14:26:18.630673: acc: 96.11%, sen: 84.96%, spe: 98.35%
2023-08-21 14:26:18.633438: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:26:18.636465: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:26:18.638442: finished real validation
2023-08-21 14:26:44.607222: train_loss -0.9361
2023-08-21 14:26:44.610721: val_loss -0.7109
2023-08-21 14:26:44.613642: Pseudo dice [0.8875]
2023-08-21 14:26:44.616009: Epoch time: 657.03 s
2023-08-21 14:26:45.940065: 
2023-08-21 14:26:45.942805: Epoch 247
2023-08-21 14:26:45.945059: Current learning rate: 0.00105
2023-08-21 14:26:45.947670: start training, 250
================num of epochs: 250================
2023-08-21 14:33:38.425527: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 14:33:38.757701: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 14:33:38.762814: The split file contains 1 splits.
2023-08-21 14:33:38.765298: Desired fold for training: 0
2023-08-21 14:33:38.767608: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 14:37:16.328475: dsc: 87.94%
2023-08-21 14:37:49.681429: miou: 78.48%
2023-08-21 14:37:49.683619: acc: 96.09%, sen: 85.14%, spe: 98.29%
2023-08-21 14:37:49.686615: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:37:49.689043: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:37:49.691447: finished real validation
2023-08-21 14:38:15.526937: train_loss -0.9352
2023-08-21 14:38:15.530205: val_loss -0.6599
2023-08-21 14:38:15.533428: Pseudo dice [0.8696]
2023-08-21 14:38:15.535846: Epoch time: 689.59 s
2023-08-21 14:38:16.843782: 
2023-08-21 14:38:16.846426: Epoch 248
2023-08-21 14:38:16.848475: Current learning rate: 0.00103
2023-08-21 14:38:16.851026: start training, 250
================num of epochs: 250================
2023-08-21 14:45:08.232741: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 14:45:08.562449: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 14:45:08.566685: The split file contains 1 splits.
2023-08-21 14:45:08.569260: Desired fold for training: 0
2023-08-21 14:45:08.572615: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 14:48:47.424777: dsc: 88.29%
2023-08-21 14:48:47.427773: miou: 79.03%
2023-08-21 14:48:47.430143: acc: 96.19%, sen: 85.80%, spe: 98.28%
2023-08-21 14:48:47.433265: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:48:47.435478: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:48:47.437712: finished real validation
2023-08-21 14:49:13.290503: train_loss -0.937
2023-08-21 14:49:13.293435: val_loss -0.6876
2023-08-21 14:49:13.296164: Pseudo dice [0.8836]
2023-08-21 14:49:13.298648: Epoch time: 656.45 s
2023-08-21 14:49:14.613039: 
2023-08-21 14:49:14.616275: Epoch 249
2023-08-21 14:49:14.618564: Current learning rate: 0.00101
2023-08-21 14:49:14.621235: start training, 250
================num of epochs: 250================
2023-08-21 14:56:06.709276: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 14:56:07.043574: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 14:56:07.048412: The split file contains 1 splits.
2023-08-21 14:56:07.050972: Desired fold for training: 0
2023-08-21 14:56:07.053636: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 14:59:46.321673: dsc: 88.00%
2023-08-21 14:59:46.324682: miou: 78.57%
2023-08-21 14:59:46.326936: acc: 96.09%, sen: 85.68%, spe: 98.18%
2023-08-21 14:59:46.330126: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:59:46.332418: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 14:59:46.334628: finished real validation
2023-08-21 15:00:12.138088: train_loss -0.9377
2023-08-21 15:00:12.141292: val_loss -0.6895
2023-08-21 15:00:12.144063: Pseudo dice [0.8785]
2023-08-21 15:00:12.146543: Epoch time: 657.53 s
2023-08-21 15:00:15.449576: 
2023-08-21 15:00:15.452412: Epoch 250
2023-08-21 15:00:15.454721: Current learning rate: 0.001
2023-08-21 15:00:15.457320: start training, 250
================num of epochs: 250================
2023-08-21 15:07:08.539857: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 15:07:08.862904: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 15:07:08.867466: The split file contains 1 splits.
2023-08-21 15:07:08.870156: Desired fold for training: 0
2023-08-21 15:07:08.873942: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 15:10:47.715218: dsc: 87.95%
2023-08-21 15:10:47.718001: miou: 78.50%
2023-08-21 15:10:47.720288: acc: 96.05%, sen: 86.08%, spe: 98.06%
2023-08-21 15:10:47.723318: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:10:47.725620: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:10:47.727887: finished real validation
2023-08-21 15:11:13.564372: train_loss -0.9362
2023-08-21 15:11:13.567868: val_loss -0.6781
2023-08-21 15:11:13.570777: Pseudo dice [0.8803]
2023-08-21 15:11:13.573249: Epoch time: 658.12 s
2023-08-21 15:11:14.896696: 
2023-08-21 15:11:14.899571: Epoch 251
2023-08-21 15:11:14.901963: Current learning rate: 0.00098
2023-08-21 15:11:14.904668: start training, 250
================num of epochs: 250================
2023-08-21 15:18:06.985471: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 15:18:07.311260: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 15:18:07.316717: The split file contains 1 splits.
2023-08-21 15:18:07.319714: Desired fold for training: 0
2023-08-21 15:18:07.322297: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 15:21:46.879856: dsc: 88.01%
2023-08-21 15:21:46.883235: miou: 78.58%
2023-08-21 15:21:46.885534: acc: 96.17%, sen: 83.97%, spe: 98.62%
2023-08-21 15:21:46.888692: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:21:46.890917: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:21:46.893126: finished real validation
2023-08-21 15:22:12.710255: train_loss -0.9376
2023-08-21 15:22:12.713165: val_loss -0.6356
2023-08-21 15:22:12.715967: Pseudo dice [0.8665]
2023-08-21 15:22:12.718560: Epoch time: 657.82 s
2023-08-21 15:22:14.030018: 
2023-08-21 15:22:14.033160: Epoch 252
2023-08-21 15:22:14.035722: Current learning rate: 0.00096
2023-08-21 15:22:14.038682: start training, 250
================num of epochs: 250================
2023-08-21 15:29:06.357003: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 15:29:06.684850: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 15:29:06.689140: The split file contains 1 splits.
2023-08-21 15:29:06.691738: Desired fold for training: 0
2023-08-21 15:29:06.694364: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 15:32:47.185959: dsc: 88.16%
2023-08-21 15:32:47.188767: miou: 78.82%
2023-08-21 15:32:47.191063: acc: 96.16%, sen: 85.30%, spe: 98.35%
2023-08-21 15:32:47.194202: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:32:47.196465: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:32:47.198671: finished real validation
2023-08-21 15:33:12.940307: train_loss -0.9371
2023-08-21 15:33:12.943308: val_loss -0.6543
2023-08-21 15:33:12.946135: Pseudo dice [0.8726]
2023-08-21 15:33:12.948588: Epoch time: 658.91 s
2023-08-21 15:33:14.265825: 
2023-08-21 15:33:14.268423: Epoch 253
2023-08-21 15:33:14.270811: Current learning rate: 0.00094
2023-08-21 15:33:14.273612: start training, 250
================num of epochs: 250================
2023-08-21 15:40:06.165165: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 15:40:06.495170: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 15:40:06.499986: The split file contains 1 splits.
2023-08-21 15:40:06.502780: Desired fold for training: 0
2023-08-21 15:40:06.505383: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 15:43:45.003232: dsc: 88.52%
2023-08-21 15:43:45.006380: miou: 79.40%
2023-08-21 15:43:45.008777: acc: 96.27%, sen: 85.89%, spe: 98.36%
2023-08-21 15:43:45.012141: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:43:45.014433: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:43:45.017072: finished real validation
2023-08-21 15:44:10.586869: train_loss -0.9364
2023-08-21 15:44:10.589809: val_loss -0.7
2023-08-21 15:44:10.592508: Pseudo dice [0.8868]
2023-08-21 15:44:10.594915: Epoch time: 656.32 s
2023-08-21 15:44:11.895005: 
2023-08-21 15:44:11.897888: Epoch 254
2023-08-21 15:44:11.900357: Current learning rate: 0.00092
2023-08-21 15:44:11.903091: start training, 250
================num of epochs: 250================
2023-08-21 15:51:05.181648: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 15:51:05.515800: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 15:51:05.520415: The split file contains 1 splits.
2023-08-21 15:51:05.523158: Desired fold for training: 0
2023-08-21 15:51:05.525532: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 15:54:43.468780: dsc: 87.95%
2023-08-21 15:54:43.471884: miou: 78.49%
2023-08-21 15:54:43.474256: acc: 96.05%, sen: 85.99%, spe: 98.08%
2023-08-21 15:54:43.477447: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:54:43.479677: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 15:54:43.481871: finished real validation
2023-08-21 15:55:09.145265: train_loss -0.9354
2023-08-21 15:55:09.148155: val_loss -0.6695
2023-08-21 15:55:09.150725: Pseudo dice [0.8771]
2023-08-21 15:55:09.153116: Epoch time: 657.25 s
2023-08-21 15:55:10.467797: 
2023-08-21 15:55:10.470766: Epoch 255
2023-08-21 15:55:10.473332: Current learning rate: 0.00091
2023-08-21 15:55:10.476141: start training, 250
================num of epochs: 250================
2023-08-21 16:02:01.397095: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 16:02:01.726393: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 16:02:01.730863: The split file contains 1 splits.
2023-08-21 16:02:01.733463: Desired fold for training: 0
2023-08-21 16:02:01.736858: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 16:05:39.842477: dsc: 88.33%
2023-08-21 16:05:39.845660: miou: 79.10%
2023-08-21 16:05:39.847986: acc: 96.23%, sen: 85.16%, spe: 98.46%
2023-08-21 16:05:39.851073: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:05:39.853372: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:05:39.855601: finished real validation
2023-08-21 16:06:05.740328: train_loss -0.9376
2023-08-21 16:06:05.743875: val_loss -0.6783
2023-08-21 16:06:05.746772: Pseudo dice [0.8761]
2023-08-21 16:06:05.749167: Epoch time: 655.28 s
2023-08-21 16:06:07.062565: 
2023-08-21 16:06:07.065352: Epoch 256
2023-08-21 16:06:07.067584: Current learning rate: 0.00089
2023-08-21 16:06:07.070316: start training, 250
================num of epochs: 250================
2023-08-21 16:12:58.670620: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 16:12:59.004012: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 16:12:59.009020: The split file contains 1 splits.
2023-08-21 16:12:59.012053: Desired fold for training: 0
2023-08-21 16:12:59.014848: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 16:16:37.660268: dsc: 88.13%
2023-08-21 16:16:37.663064: miou: 78.77%
2023-08-21 16:16:37.665253: acc: 96.14%, sen: 85.56%, spe: 98.27%
2023-08-21 16:16:37.668314: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:16:37.670571: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:16:37.672794: finished real validation
2023-08-21 16:17:03.122897: train_loss -0.9387
2023-08-21 16:17:03.126014: val_loss -0.6853
2023-08-21 16:17:03.128691: Pseudo dice [0.8834]
2023-08-21 16:17:03.131042: Epoch time: 656.06 s
2023-08-21 16:17:04.428091: 
2023-08-21 16:17:04.432084: Epoch 257
2023-08-21 16:17:04.434511: Current learning rate: 0.00087
2023-08-21 16:17:04.437157: start training, 250
================num of epochs: 250================
2023-08-21 16:23:55.099655: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 16:23:55.412411: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 16:23:55.416633: The split file contains 1 splits.
2023-08-21 16:23:55.419016: Desired fold for training: 0
2023-08-21 16:23:55.421535: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 16:27:36.487562: dsc: 88.10%
2023-08-21 16:27:36.490529: miou: 78.73%
2023-08-21 16:27:36.492746: acc: 96.16%, sen: 84.97%, spe: 98.41%
2023-08-21 16:27:36.495880: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:27:36.498072: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:27:36.500376: finished real validation
2023-08-21 16:28:02.053997: train_loss -0.938
2023-08-21 16:28:02.057330: val_loss -0.6604
2023-08-21 16:28:02.060099: Pseudo dice [0.8753]
2023-08-21 16:28:02.062670: Epoch time: 657.63 s
2023-08-21 16:28:03.360594: 
2023-08-21 16:28:03.363492: Epoch 258
2023-08-21 16:28:03.365850: Current learning rate: 0.00085
2023-08-21 16:28:03.368570: start training, 250
================num of epochs: 250================
2023-08-21 16:34:54.864497: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 16:34:55.198869: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 16:34:55.203997: The split file contains 1 splits.
2023-08-21 16:34:55.206853: Desired fold for training: 0
2023-08-21 16:34:55.210623: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 16:38:34.102081: dsc: 88.10%
2023-08-21 16:38:34.104966: miou: 78.73%
2023-08-21 16:38:34.107336: acc: 96.15%, sen: 84.99%, spe: 98.40%
2023-08-21 16:38:34.110531: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:38:34.112840: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:38:34.115108: finished real validation
2023-08-21 16:38:59.889353: train_loss -0.938
2023-08-21 16:38:59.892533: val_loss -0.6766
2023-08-21 16:38:59.895566: Pseudo dice [0.8844]
2023-08-21 16:38:59.898057: Epoch time: 656.53 s
2023-08-21 16:39:01.194599: 
2023-08-21 16:39:01.197676: Epoch 259
2023-08-21 16:39:01.200178: Current learning rate: 0.00083
2023-08-21 16:39:01.203174: start training, 250
================num of epochs: 250================
2023-08-21 16:45:52.303501: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 16:45:52.629660: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 16:45:52.634166: The split file contains 1 splits.
2023-08-21 16:45:52.638034: Desired fold for training: 0
2023-08-21 16:45:52.640714: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 16:49:31.172334: dsc: 88.09%
2023-08-21 16:49:31.176211: miou: 78.72%
2023-08-21 16:49:31.178629: acc: 96.14%, sen: 85.16%, spe: 98.35%
2023-08-21 16:49:31.182388: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:49:31.185021: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 16:49:31.187352: finished real validation
2023-08-21 16:49:56.730848: train_loss -0.937
2023-08-21 16:49:56.734005: val_loss -0.7012
2023-08-21 16:49:56.736913: Pseudo dice [0.8881]
2023-08-21 16:49:56.739594: Epoch time: 655.54 s
2023-08-21 16:49:58.041817: 
2023-08-21 16:49:58.044597: Epoch 260
2023-08-21 16:49:58.046880: Current learning rate: 0.00082
2023-08-21 16:49:58.049687: start training, 250
================num of epochs: 250================
2023-08-21 16:56:50.942770: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 16:56:51.273223: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 16:56:51.277437: The split file contains 1 splits.
2023-08-21 16:56:51.280749: Desired fold for training: 0
2023-08-21 16:56:51.283686: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 17:00:29.506943: dsc: 88.13%
2023-08-21 17:00:29.509779: miou: 78.78%
2023-08-21 17:00:29.511999: acc: 96.13%, sen: 85.80%, spe: 98.21%
2023-08-21 17:00:29.515115: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:00:29.517387: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:00:29.519599: finished real validation
2023-08-21 17:00:55.026096: train_loss -0.9387
2023-08-21 17:00:55.029056: val_loss -0.6283
2023-08-21 17:00:55.031782: Pseudo dice [0.8635]
2023-08-21 17:00:55.034244: Epoch time: 656.99 s
2023-08-21 17:00:56.329753: 
2023-08-21 17:00:56.332568: Epoch 261
2023-08-21 17:00:56.334955: Current learning rate: 0.0008
2023-08-21 17:00:56.337593: start training, 250
================num of epochs: 250================
2023-08-21 17:07:48.767998: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 17:07:49.092640: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 17:07:49.097218: The split file contains 1 splits.
2023-08-21 17:07:49.100039: Desired fold for training: 0
2023-08-21 17:07:49.102732: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 17:11:30.231703: dsc: 88.26%
2023-08-21 17:11:30.235147: miou: 78.98%
2023-08-21 17:11:30.237543: acc: 96.17%, sen: 85.91%, spe: 98.23%
2023-08-21 17:11:30.240820: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:11:30.243900: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:11:30.246305: finished real validation
2023-08-21 17:11:55.948305: train_loss -0.939
2023-08-21 17:11:55.951745: val_loss -0.6706
2023-08-21 17:11:55.954693: Pseudo dice [0.8783]
2023-08-21 17:11:55.957085: Epoch time: 659.62 s
2023-08-21 17:11:57.292814: 
2023-08-21 17:11:57.295770: Epoch 262
2023-08-21 17:11:57.298285: Current learning rate: 0.00078
2023-08-21 17:11:57.301088: start training, 250
================num of epochs: 250================
2023-08-21 17:18:48.929299: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 17:18:49.235522: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 17:18:49.239879: The split file contains 1 splits.
2023-08-21 17:18:49.242548: Desired fold for training: 0
2023-08-21 17:18:49.245094: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 17:22:30.564003: dsc: 88.15%
2023-08-21 17:22:30.566957: miou: 78.82%
2023-08-21 17:22:30.569248: acc: 96.12%, sen: 86.23%, spe: 98.11%
2023-08-21 17:22:30.572738: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:22:30.575019: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:22:30.577273: finished real validation
2023-08-21 17:22:56.311512: train_loss -0.9379
2023-08-21 17:22:56.314415: val_loss -0.6958
2023-08-21 17:22:56.317036: Pseudo dice [0.8876]
2023-08-21 17:22:56.319577: Epoch time: 659.02 s
2023-08-21 17:22:57.624574: 
2023-08-21 17:22:57.627355: Epoch 263
2023-08-21 17:22:57.629642: Current learning rate: 0.00076
2023-08-21 17:22:57.632281: start training, 250
================num of epochs: 250================
2023-08-21 17:29:49.492206: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 17:29:49.833959: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 17:29:49.839171: The split file contains 1 splits.
2023-08-21 17:29:49.842505: Desired fold for training: 0
2023-08-21 17:29:49.845864: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 17:33:32.432834: dsc: 88.28%
2023-08-21 17:33:32.435730: miou: 79.01%
2023-08-21 17:33:32.438064: acc: 96.19%, sen: 85.64%, spe: 98.31%
2023-08-21 17:33:32.441314: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:33:32.443583: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:33:32.445870: finished real validation
2023-08-21 17:33:57.932287: train_loss -0.9379
2023-08-21 17:33:57.935484: val_loss -0.7027
2023-08-21 17:33:57.938466: Pseudo dice [0.8818]
2023-08-21 17:33:57.940974: Epoch time: 660.31 s
2023-08-21 17:33:59.238303: 
2023-08-21 17:33:59.240927: Epoch 264
2023-08-21 17:33:59.243198: Current learning rate: 0.00074
2023-08-21 17:33:59.245766: start training, 250
================num of epochs: 250================
2023-08-21 17:40:53.161647: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 17:40:53.488101: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 17:40:53.492592: The split file contains 1 splits.
2023-08-21 17:40:53.495353: Desired fold for training: 0
2023-08-21 17:40:53.497812: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 17:44:31.637827: dsc: 88.25%
2023-08-21 17:44:31.641367: miou: 78.97%
2023-08-21 17:44:31.643979: acc: 96.13%, sen: 86.69%, spe: 98.03%
2023-08-21 17:44:31.647411: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:44:31.649885: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:44:31.652153: finished real validation
2023-08-21 17:44:57.187879: train_loss -0.9391
2023-08-21 17:44:57.191263: val_loss -0.6709
2023-08-21 17:44:57.194031: Pseudo dice [0.8813]
2023-08-21 17:44:57.196373: Epoch time: 657.95 s
2023-08-21 17:44:58.497214: 
2023-08-21 17:44:58.500059: Epoch 265
2023-08-21 17:44:58.502465: Current learning rate: 0.00072
2023-08-21 17:44:58.505223: start training, 250
================num of epochs: 250================
2023-08-21 17:51:50.285647: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 17:51:50.624967: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 17:51:50.629569: The split file contains 1 splits.
2023-08-21 17:51:50.632882: Desired fold for training: 0
2023-08-21 17:51:50.635494: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 17:55:30.381235: dsc: 88.05%
2023-08-21 17:55:30.384253: miou: 78.66%
2023-08-21 17:55:30.386535: acc: 96.11%, sen: 85.51%, spe: 98.25%
2023-08-21 17:55:30.389709: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:55:30.392067: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 17:55:30.394349: finished real validation
2023-08-21 17:55:56.062252: train_loss -0.9387
2023-08-21 17:55:56.065348: val_loss -0.6365
2023-08-21 17:55:56.068049: Pseudo dice [0.8732]
2023-08-21 17:55:56.070596: Epoch time: 657.57 s
2023-08-21 17:55:57.367153: 
2023-08-21 17:55:57.369994: Epoch 266
2023-08-21 17:55:57.372452: Current learning rate: 0.0007
2023-08-21 17:55:57.375283: start training, 250
================num of epochs: 250================
2023-08-21 18:02:49.425165: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 18:02:49.742171: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 18:02:49.746831: The split file contains 1 splits.
2023-08-21 18:02:49.749572: Desired fold for training: 0
2023-08-21 18:02:49.752062: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 18:06:27.504164: dsc: 88.16%
2023-08-21 18:06:27.507426: miou: 78.83%
2023-08-21 18:06:27.509921: acc: 96.12%, sen: 86.18%, spe: 98.12%
2023-08-21 18:06:27.513032: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:06:27.515347: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:06:27.517657: finished real validation
2023-08-21 18:06:53.284029: train_loss -0.9379
2023-08-21 18:06:53.287093: val_loss -0.6772
2023-08-21 18:06:53.290164: Pseudo dice [0.8817]
2023-08-21 18:06:53.292692: Epoch time: 655.92 s
2023-08-21 18:06:54.619694: 
2023-08-21 18:06:54.622461: Epoch 267
2023-08-21 18:06:54.624920: Current learning rate: 0.00069
2023-08-21 18:06:54.627901: start training, 250
================num of epochs: 250================
2023-08-21 18:13:46.960444: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 18:13:47.294178: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 18:13:47.298981: The split file contains 1 splits.
2023-08-21 18:13:47.302413: Desired fold for training: 0
2023-08-21 18:13:47.305140: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 18:17:25.901988: dsc: 88.03%
2023-08-21 18:17:25.905187: miou: 78.63%
2023-08-21 18:17:25.907588: acc: 96.13%, sen: 84.96%, spe: 98.38%
2023-08-21 18:17:25.911093: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:17:25.913456: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:17:25.915830: finished real validation
2023-08-21 18:17:51.466938: train_loss -0.9379
2023-08-21 18:17:51.469954: val_loss -0.6696
2023-08-21 18:17:51.472841: Pseudo dice [0.8769]
2023-08-21 18:17:51.475354: Epoch time: 656.85 s
2023-08-21 18:17:52.794176: 
2023-08-21 18:17:52.797053: Epoch 268
2023-08-21 18:17:52.799540: Current learning rate: 0.00067
2023-08-21 18:17:52.802409: start training, 250
================num of epochs: 250================
2023-08-21 18:24:43.394885: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 18:24:43.729713: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 18:24:43.735352: The split file contains 1 splits.
2023-08-21 18:24:43.738276: Desired fold for training: 0
2023-08-21 18:24:43.741043: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 18:28:23.408916: dsc: 88.27%
2023-08-21 18:28:23.412245: miou: 79.00%
2023-08-21 18:28:23.414662: acc: 96.20%, sen: 85.35%, spe: 98.38%
2023-08-21 18:28:23.418262: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:28:23.420990: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:28:23.423798: finished real validation
2023-08-21 18:28:49.069543: train_loss -0.9384
2023-08-21 18:28:49.072493: val_loss -0.6807
2023-08-21 18:28:49.075354: Pseudo dice [0.8774]
2023-08-21 18:28:49.077782: Epoch time: 656.28 s
2023-08-21 18:28:50.402021: 
2023-08-21 18:28:50.405031: Epoch 269
2023-08-21 18:28:50.407444: Current learning rate: 0.00065
2023-08-21 18:28:50.410259: start training, 250
================num of epochs: 250================
2023-08-21 18:35:43.640887: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 18:35:43.990961: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 18:35:43.995721: The split file contains 1 splits.
2023-08-21 18:35:43.998687: Desired fold for training: 0
2023-08-21 18:35:44.001525: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 18:39:23.900703: dsc: 88.18%
2023-08-21 18:39:23.903698: miou: 78.85%
2023-08-21 18:39:23.906006: acc: 96.18%, sen: 85.01%, spe: 98.43%
2023-08-21 18:39:23.909155: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:39:23.911520: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:39:23.913794: finished real validation
2023-08-21 18:39:49.619092: train_loss -0.9414
2023-08-21 18:39:49.622161: val_loss -0.6793
2023-08-21 18:39:49.625117: Pseudo dice [0.8782]
2023-08-21 18:39:49.627618: Epoch time: 659.22 s
2023-08-21 18:39:50.948882: 
2023-08-21 18:39:50.952289: Epoch 270
2023-08-21 18:39:50.954589: Current learning rate: 0.00063
2023-08-21 18:39:50.957293: start training, 250
================num of epochs: 250================
2023-08-21 18:46:42.453718: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 18:46:42.789973: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 18:46:42.794120: The split file contains 1 splits.
2023-08-21 18:46:42.797132: Desired fold for training: 0
2023-08-21 18:46:42.800080: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 18:50:20.333429: dsc: 88.01%
2023-08-21 18:50:20.336619: miou: 78.59%
2023-08-21 18:50:20.339039: acc: 96.14%, sen: 84.61%, spe: 98.46%
2023-08-21 18:50:20.342600: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:50:20.345190: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 18:50:20.347569: finished real validation
2023-08-21 18:50:46.017640: train_loss -0.9408
2023-08-21 18:50:46.020742: val_loss -0.6725
2023-08-21 18:50:46.023470: Pseudo dice [0.8756]
2023-08-21 18:50:46.025921: Epoch time: 655.07 s
2023-08-21 18:50:47.344234: 
2023-08-21 18:50:47.347238: Epoch 271
2023-08-21 18:50:47.349642: Current learning rate: 0.00061
2023-08-21 18:50:47.352491: start training, 250
================num of epochs: 250================
2023-08-21 18:57:38.903890: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 18:57:39.237196: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 18:57:39.241685: The split file contains 1 splits.
2023-08-21 18:57:39.245064: Desired fold for training: 0
2023-08-21 18:57:39.247821: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 19:01:19.500593: dsc: 88.45%
2023-08-21 19:01:19.503583: miou: 79.29%
2023-08-21 19:01:19.506036: acc: 96.26%, sen: 85.39%, spe: 98.45%
2023-08-21 19:01:19.509295: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:01:19.511610: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:01:19.513998: finished real validation
2023-08-21 19:01:45.032610: train_loss -0.9388
2023-08-21 19:01:45.035988: val_loss -0.6793
2023-08-21 19:01:45.038960: Pseudo dice [0.8809]
2023-08-21 19:01:45.041468: Epoch time: 657.69 s
2023-08-21 19:01:46.354402: 
2023-08-21 19:01:46.357213: Epoch 272
2023-08-21 19:01:46.359738: Current learning rate: 0.00059
2023-08-21 19:01:46.362648: start training, 250
================num of epochs: 250================
2023-08-21 19:08:37.791809: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 19:08:38.120279: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 19:08:38.124681: The split file contains 1 splits.
2023-08-21 19:08:38.128186: Desired fold for training: 0
2023-08-21 19:08:38.130842: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 19:12:18.068786: dsc: 88.05%
2023-08-21 19:12:18.071783: miou: 78.65%
2023-08-21 19:12:18.074169: acc: 96.10%, sen: 85.72%, spe: 98.19%
2023-08-21 19:12:18.077402: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:12:18.079822: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:12:18.082139: finished real validation
2023-08-21 19:12:43.644867: train_loss -0.9395
2023-08-21 19:12:43.648134: val_loss -0.6777
2023-08-21 19:12:43.651027: Pseudo dice [0.8795]
2023-08-21 19:12:43.653430: Epoch time: 657.29 s
2023-08-21 19:12:44.982010: 
2023-08-21 19:12:44.985405: Epoch 273
2023-08-21 19:12:44.987858: Current learning rate: 0.00057
2023-08-21 19:12:44.990809: start training, 250
================num of epochs: 250================
2023-08-21 19:19:37.750230: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 19:19:38.090222: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 19:19:38.095046: The split file contains 1 splits.
2023-08-21 19:19:38.098011: Desired fold for training: 0
2023-08-21 19:19:38.100847: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 19:23:17.501501: dsc: 88.08%
2023-08-21 19:23:17.504510: miou: 78.70%
2023-08-21 19:23:17.506912: acc: 96.17%, sen: 84.42%, spe: 98.54%
2023-08-21 19:23:17.510395: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:23:17.512924: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:23:17.515254: finished real validation
2023-08-21 19:23:43.379512: train_loss -0.9401
2023-08-21 19:23:43.382773: val_loss -0.6853
2023-08-21 19:23:43.385785: Pseudo dice [0.8828]
2023-08-21 19:23:43.388657: Epoch time: 658.4 s
2023-08-21 19:23:44.709849: 
2023-08-21 19:23:44.713009: Epoch 274
2023-08-21 19:23:44.715568: Current learning rate: 0.00055
2023-08-21 19:23:44.718521: start training, 250
================num of epochs: 250================
2023-08-21 19:30:37.356923: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 19:30:37.690859: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 19:30:37.695296: The split file contains 1 splits.
2023-08-21 19:30:37.699241: Desired fold for training: 0
2023-08-21 19:30:37.702065: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 19:34:18.415410: dsc: 88.05%
2023-08-21 19:34:18.418553: miou: 78.64%
2023-08-21 19:34:18.420891: acc: 96.13%, sen: 85.16%, spe: 98.33%
2023-08-21 19:34:18.424069: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:34:18.426466: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:34:18.429031: finished real validation
2023-08-21 19:34:44.172028: train_loss -0.9397
2023-08-21 19:34:44.175109: val_loss -0.6972
2023-08-21 19:34:44.177933: Pseudo dice [0.8893]
2023-08-21 19:34:44.180455: Epoch time: 659.47 s
2023-08-21 19:34:45.507739: 
2023-08-21 19:34:45.510549: Epoch 275
2023-08-21 19:34:45.512975: Current learning rate: 0.00053
2023-08-21 19:34:45.515658: start training, 250
================num of epochs: 250================
2023-08-21 19:41:38.739795: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 19:41:39.066594: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 19:41:39.071033: The split file contains 1 splits.
2023-08-21 19:41:39.074094: Desired fold for training: 0
2023-08-21 19:41:39.077101: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 19:45:21.000412: dsc: 88.13%
2023-08-21 19:45:21.003402: miou: 78.78%
2023-08-21 19:45:21.005683: acc: 96.16%, sen: 85.08%, spe: 98.39%
2023-08-21 19:45:21.008934: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:45:21.011230: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:45:21.013518: finished real validation
2023-08-21 19:45:46.583696: train_loss -0.9403
2023-08-21 19:45:46.586680: val_loss -0.6702
2023-08-21 19:45:46.589534: Pseudo dice [0.8766]
2023-08-21 19:45:46.592098: Epoch time: 661.08 s
2023-08-21 19:45:47.887882: 
2023-08-21 19:45:47.890797: Epoch 276
2023-08-21 19:45:47.893322: Current learning rate: 0.00051
2023-08-21 19:45:47.896105: start training, 250
================num of epochs: 250================
2023-08-21 19:52:39.614122: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 19:52:39.938298: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 19:52:39.942713: The split file contains 1 splits.
2023-08-21 19:52:39.945997: Desired fold for training: 0
2023-08-21 19:52:39.948431: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 19:56:19.865688: dsc: 88.08%
2023-08-21 19:56:19.868945: miou: 78.69%
2023-08-21 19:56:19.871273: acc: 96.15%, sen: 84.92%, spe: 98.41%
2023-08-21 19:56:19.874409: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:56:19.876677: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 19:56:19.879125: finished real validation
2023-08-21 19:56:45.377561: train_loss -0.9413
2023-08-21 19:56:45.380872: val_loss -0.6611
2023-08-21 19:56:45.383911: Pseudo dice [0.8803]
2023-08-21 19:56:45.386472: Epoch time: 657.49 s
2023-08-21 19:56:46.700957: 
2023-08-21 19:56:46.703850: Epoch 277
2023-08-21 19:56:46.706265: Current learning rate: 0.0005
2023-08-21 19:56:46.709037: start training, 250
================num of epochs: 250================
2023-08-21 20:03:40.909521: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 20:03:41.241443: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 20:03:41.245978: The split file contains 1 splits.
2023-08-21 20:03:41.248981: Desired fold for training: 0
2023-08-21 20:03:41.252103: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 20:07:21.951687: dsc: 88.07%
2023-08-21 20:07:21.955122: miou: 78.68%
2023-08-21 20:07:21.957720: acc: 96.13%, sen: 85.35%, spe: 98.29%
2023-08-21 20:07:21.961338: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:07:21.963854: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:07:21.966250: finished real validation
2023-08-21 20:07:47.804775: train_loss -0.9403
2023-08-21 20:07:47.808221: val_loss -0.7101
2023-08-21 20:07:47.811380: Pseudo dice [0.8886]
2023-08-21 20:07:47.814283: Epoch time: 661.11 s
2023-08-21 20:07:47.816682: Yayy! New best EMA pseudo Dice: 0.8807
2023-08-21 20:07:51.529536: 
2023-08-21 20:07:51.532479: Epoch 278
2023-08-21 20:07:51.534943: Current learning rate: 0.00048
2023-08-21 20:07:51.537961: start training, 250
================num of epochs: 250================
2023-08-21 20:14:44.327338: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 20:14:44.652378: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 20:14:44.656895: The split file contains 1 splits.
2023-08-21 20:14:44.660086: Desired fold for training: 0
2023-08-21 20:14:44.663209: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 20:18:22.607310: dsc: 88.25%
2023-08-21 20:18:22.610321: miou: 78.98%
2023-08-21 20:18:22.612696: acc: 96.15%, sen: 86.32%, spe: 98.13%
2023-08-21 20:18:22.616041: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:18:22.618431: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:18:22.620841: finished real validation
2023-08-21 20:18:48.410039: train_loss -0.9397
2023-08-21 20:18:48.413574: val_loss -0.6724
2023-08-21 20:18:48.416764: Pseudo dice [0.8778]
2023-08-21 20:18:48.419322: Epoch time: 656.88 s
2023-08-21 20:18:49.744453: 
2023-08-21 20:18:49.747527: Epoch 279
2023-08-21 20:18:49.750051: Current learning rate: 0.00046
2023-08-21 20:18:49.752936: start training, 250
================num of epochs: 250================
2023-08-21 20:25:42.471413: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 20:25:42.808703: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 20:25:42.813068: The split file contains 1 splits.
2023-08-21 20:25:42.816313: Desired fold for training: 0
2023-08-21 20:25:42.819509: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 20:29:24.257174: dsc: 88.13%
2023-08-21 20:29:24.260589: miou: 78.78%
2023-08-21 20:29:24.262992: acc: 96.14%, sen: 85.56%, spe: 98.27%
2023-08-21 20:29:24.266548: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:29:24.269287: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:29:24.271643: finished real validation
2023-08-21 20:29:49.958305: train_loss -0.9402
2023-08-21 20:29:49.961317: val_loss -0.6828
2023-08-21 20:29:49.964200: Pseudo dice [0.8823]
2023-08-21 20:29:49.966680: Epoch time: 660.22 s
2023-08-21 20:29:51.282307: 
2023-08-21 20:29:51.285177: Epoch 280
2023-08-21 20:29:51.287629: Current learning rate: 0.00044
2023-08-21 20:29:51.290494: start training, 250
================num of epochs: 250================
2023-08-21 20:36:45.388280: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 20:36:45.724769: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 20:36:45.730572: The split file contains 1 splits.
2023-08-21 20:36:45.734498: Desired fold for training: 0
2023-08-21 20:36:45.737715: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 20:40:25.719874: dsc: 88.23%
2023-08-21 20:40:25.722934: miou: 78.94%
2023-08-21 20:40:25.725365: acc: 96.16%, sen: 85.86%, spe: 98.24%
2023-08-21 20:40:25.728647: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:40:25.731077: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:40:25.733469: finished real validation
2023-08-21 20:40:51.592169: train_loss -0.9413
2023-08-21 20:40:51.595296: val_loss -0.7055
2023-08-21 20:40:51.598117: Pseudo dice [0.8901]
2023-08-21 20:40:51.600659: Epoch time: 660.31 s
2023-08-21 20:40:51.603069: Yayy! New best EMA pseudo Dice: 0.8816
2023-08-21 20:40:55.187742: 
2023-08-21 20:40:55.191130: Epoch 281
2023-08-21 20:40:55.193789: Current learning rate: 0.00042
2023-08-21 20:40:55.196734: start training, 250
================num of epochs: 250================
2023-08-21 20:47:47.906912: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 20:47:48.225459: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 20:47:48.230157: The split file contains 1 splits.
2023-08-21 20:47:48.232729: Desired fold for training: 0
2023-08-21 20:47:48.235244: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 20:52:35.734407: dsc: 88.29%
2023-08-21 20:52:35.737480: miou: 79.04%
2023-08-21 20:52:35.739911: acc: 96.20%, sen: 85.52%, spe: 98.35%
2023-08-21 20:52:35.743140: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:52:35.745548: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 20:52:35.747882: finished real validation
2023-08-21 20:53:01.457836: train_loss -0.9407
2023-08-21 20:53:01.461400: val_loss -0.678
2023-08-21 20:53:01.464518: Pseudo dice [0.8796]
2023-08-21 20:53:01.467335: Epoch time: 726.27 s
2023-08-21 20:53:02.769547: 
2023-08-21 20:53:02.772501: Epoch 282
2023-08-21 20:53:02.775019: Current learning rate: 0.0004
2023-08-21 20:53:02.777918: start training, 250
================num of epochs: 250================
2023-08-21 20:59:55.090097: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 20:59:55.418784: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 20:59:55.423144: The split file contains 1 splits.
2023-08-21 20:59:55.425996: Desired fold for training: 0
2023-08-21 20:59:55.429997: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 21:03:35.118354: dsc: 88.20%
2023-08-21 21:03:35.122224: miou: 78.90%
2023-08-21 21:03:35.124646: acc: 96.17%, sen: 85.57%, spe: 98.30%
2023-08-21 21:03:35.127934: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:03:35.130444: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:03:35.132915: finished real validation
2023-08-21 21:04:01.054462: train_loss -0.9406
2023-08-21 21:04:02.605292: val_loss -0.6897
2023-08-21 21:04:02.608308: Pseudo dice [0.8831]
2023-08-21 21:04:02.610838: Epoch time: 658.29 s
2023-08-21 21:04:03.927196: 
2023-08-21 21:04:03.930203: Epoch 283
2023-08-21 21:04:03.932711: Current learning rate: 0.00038
2023-08-21 21:04:03.935656: start training, 250
================num of epochs: 250================
2023-08-21 21:11:01.680787: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 21:11:02.003713: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 21:11:02.007915: The split file contains 1 splits.
2023-08-21 21:11:02.010455: Desired fold for training: 0
2023-08-21 21:11:02.013135: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 21:15:46.565346: dsc: 88.27%
2023-08-21 21:15:46.568585: miou: 79.00%
2023-08-21 21:15:46.571049: acc: 96.17%, sen: 86.04%, spe: 98.21%
2023-08-21 21:15:46.574366: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:15:46.576849: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:15:46.579240: finished real validation
2023-08-21 21:16:12.439718: train_loss -0.9418
2023-08-21 21:16:12.442872: val_loss -0.7011
2023-08-21 21:16:12.445778: Pseudo dice [0.8863]
2023-08-21 21:16:12.448342: Epoch time: 728.52 s
2023-08-21 21:16:12.450805: Yayy! New best EMA pseudo Dice: 0.882
2023-08-21 21:16:15.771483: 
2023-08-21 21:16:15.774536: Epoch 284
2023-08-21 21:16:15.777086: Current learning rate: 0.00036
2023-08-21 21:16:15.779863: start training, 250
================num of epochs: 250================
2023-08-21 21:23:11.366428: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 21:23:11.694319: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 21:23:11.699638: The split file contains 1 splits.
2023-08-21 21:23:11.702590: Desired fold for training: 0
2023-08-21 21:23:11.705282: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 21:26:49.884769: dsc: 88.11%
2023-08-21 21:26:49.887775: miou: 78.75%
2023-08-21 21:26:49.890138: acc: 96.17%, sen: 84.79%, spe: 98.46%
2023-08-21 21:26:49.893698: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:26:49.896230: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:26:49.898770: finished real validation
2023-08-21 21:27:15.904524: train_loss -0.9417
2023-08-21 21:27:15.907787: val_loss -0.6385
2023-08-21 21:27:15.910917: Pseudo dice [0.8709]
2023-08-21 21:27:15.913497: Epoch time: 660.14 s
2023-08-21 21:27:17.257715: 
2023-08-21 21:27:17.260728: Epoch 285
2023-08-21 21:27:17.263296: Current learning rate: 0.00034
2023-08-21 21:27:17.266140: start training, 250
================num of epochs: 250================
2023-08-21 21:34:12.812726: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 21:34:13.136655: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 21:34:13.143094: The split file contains 1 splits.
2023-08-21 21:34:13.146780: Desired fold for training: 0
2023-08-21 21:34:13.150066: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 21:37:57.898356: dsc: 88.38%
2023-08-21 21:37:57.901889: miou: 79.18%
2023-08-21 21:37:57.904341: acc: 96.22%, sen: 85.90%, spe: 98.29%
2023-08-21 21:37:57.907676: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:37:57.910254: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:37:57.912644: finished real validation
2023-08-21 21:38:24.047293: train_loss -0.9415
2023-08-21 21:38:24.050751: val_loss -0.6814
2023-08-21 21:38:24.053861: Pseudo dice [0.8828]
2023-08-21 21:38:24.056595: Epoch time: 666.79 s
2023-08-21 21:38:25.398762: 
2023-08-21 21:38:25.401691: Epoch 286
2023-08-21 21:38:25.404312: Current learning rate: 0.00032
2023-08-21 21:38:25.407273: start training, 250
================num of epochs: 250================
2023-08-21 21:45:21.434489: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 21:45:21.753651: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 21:45:21.758041: The split file contains 1 splits.
2023-08-21 21:45:21.760665: Desired fold for training: 0
2023-08-21 21:45:21.763438: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 21:54:18.347255: dsc: 88.35%
2023-08-21 21:54:18.352806: miou: 79.12%
2023-08-21 21:54:18.356842: acc: 96.21%, sen: 85.82%, spe: 98.30%
2023-08-21 21:54:18.365036: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:54:18.367499: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 21:54:18.369972: finished real validation
2023-08-21 21:54:44.263326: train_loss -0.9419
2023-08-21 21:54:44.266593: val_loss -0.6719
2023-08-21 21:54:44.269519: Pseudo dice [0.8764]
2023-08-21 21:54:44.272089: Epoch time: 978.87 s
2023-08-21 21:54:45.616940: 
2023-08-21 21:54:45.619929: Epoch 287
2023-08-21 21:54:45.622486: Current learning rate: 0.0003
2023-08-21 21:54:45.625282: start training, 250
================num of epochs: 250================
2023-08-21 22:01:41.932358: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 22:01:42.346038: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 22:01:42.350246: The split file contains 1 splits.
2023-08-21 22:01:42.352988: Desired fold for training: 0
2023-08-21 22:01:42.355548: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 22:10:24.709797: dsc: 88.15%
2023-08-21 22:10:24.740367: miou: 78.81%
2023-08-21 22:10:24.744522: acc: 96.16%, sen: 85.18%, spe: 98.37%
2023-08-21 22:10:24.748082: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:10:24.752388: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:10:24.755618: finished real validation
2023-08-21 22:10:50.729423: train_loss -0.9429
2023-08-21 22:10:50.740825: val_loss -0.673
2023-08-21 22:10:50.759399: Pseudo dice [0.8809]
2023-08-21 22:10:50.777383: Epoch time: 965.12 s
2023-08-21 22:10:52.155638: 
2023-08-21 22:10:52.169847: Epoch 288
2023-08-21 22:10:52.188972: Current learning rate: 0.00028
2023-08-21 22:10:52.207379: start training, 250
================num of epochs: 250================
2023-08-21 22:17:47.064263: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 22:17:47.386604: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 22:17:47.393094: The split file contains 1 splits.
2023-08-21 22:17:47.396142: Desired fold for training: 0
2023-08-21 22:17:47.402956: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 22:21:28.276155: dsc: 88.19%
2023-08-21 22:21:28.279969: miou: 78.87%
2023-08-21 22:21:28.282919: acc: 96.16%, sen: 85.46%, spe: 98.32%
2023-08-21 22:21:28.286692: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:21:28.290702: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:21:28.294046: finished real validation
2023-08-21 22:21:54.318487: train_loss -0.9426
2023-08-21 22:21:54.321888: val_loss -0.6733
2023-08-21 22:21:54.325000: Pseudo dice [0.883]
2023-08-21 22:21:54.327521: Epoch time: 662.17 s
2023-08-21 22:21:55.672792: 
2023-08-21 22:21:55.675680: Epoch 289
2023-08-21 22:21:55.677966: Current learning rate: 0.00026
2023-08-21 22:21:55.680778: start training, 250
================num of epochs: 250================
2023-08-21 22:28:50.827505: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 22:28:51.148218: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 22:28:51.154026: The split file contains 1 splits.
2023-08-21 22:28:51.158130: Desired fold for training: 0
2023-08-21 22:28:51.161559: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 22:32:32.218105: dsc: 88.27%
2023-08-21 22:32:32.223032: miou: 79.00%
2023-08-21 22:32:32.225794: acc: 96.18%, sen: 85.77%, spe: 98.27%
2023-08-21 22:32:32.230472: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:32:32.234452: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:32:32.236849: finished real validation
2023-08-21 22:32:58.131790: train_loss -0.942
2023-08-21 22:32:58.136478: val_loss -0.6544
2023-08-21 22:32:58.142963: Pseudo dice [0.8773]
2023-08-21 22:32:58.145709: Epoch time: 662.46 s
2023-08-21 22:32:59.498717: 
2023-08-21 22:32:59.502636: Epoch 290
2023-08-21 22:32:59.505660: Current learning rate: 0.00023
2023-08-21 22:32:59.510156: start training, 250
================num of epochs: 250================
2023-08-21 22:39:53.359979: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 22:39:53.693312: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 22:39:53.698397: The split file contains 1 splits.
2023-08-21 22:39:53.701611: Desired fold for training: 0
2023-08-21 22:39:53.704884: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 22:43:34.084874: dsc: 88.27%
2023-08-21 22:43:34.088837: miou: 79.01%
2023-08-21 22:43:34.091267: acc: 96.18%, sen: 85.73%, spe: 98.29%
2023-08-21 22:43:34.094774: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:43:34.097399: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:43:34.100087: finished real validation
2023-08-21 22:43:59.737280: train_loss -0.9427
2023-08-21 22:43:59.740466: val_loss -0.6781
2023-08-21 22:43:59.743396: Pseudo dice [0.8862]
2023-08-21 22:43:59.745892: Epoch time: 660.24 s
2023-08-21 22:44:01.036151: 
2023-08-21 22:44:01.039535: Epoch 291
2023-08-21 22:44:01.041951: Current learning rate: 0.00021
2023-08-21 22:44:01.044808: start training, 250
================num of epochs: 250================
2023-08-21 22:50:54.442399: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 22:50:54.766658: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 22:50:54.771580: The split file contains 1 splits.
2023-08-21 22:50:54.774993: Desired fold for training: 0
2023-08-21 22:50:54.778602: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 22:54:34.589566: dsc: 88.20%
2023-08-21 22:54:34.593693: miou: 78.89%
2023-08-21 22:54:34.596211: acc: 96.18%, sen: 85.23%, spe: 98.38%
2023-08-21 22:54:34.599694: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:54:34.602475: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 22:54:34.605093: finished real validation
2023-08-21 22:55:00.264502: train_loss -0.9413
2023-08-21 22:55:00.267792: val_loss -0.6652
2023-08-21 22:55:00.271271: Pseudo dice [0.8773]
2023-08-21 22:55:00.274333: Epoch time: 659.23 s
2023-08-21 22:55:01.582697: 
2023-08-21 22:55:01.586130: Epoch 292
2023-08-21 22:55:01.588910: Current learning rate: 0.00019
2023-08-21 22:55:01.592030: start training, 250
================num of epochs: 250================
2023-08-21 23:01:54.345456: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 23:01:54.661457: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 23:01:54.666829: The split file contains 1 splits.
2023-08-21 23:01:54.670240: Desired fold for training: 0
2023-08-21 23:01:54.673137: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 23:05:36.015232: dsc: 88.22%
2023-08-21 23:05:36.019066: miou: 78.92%
2023-08-21 23:05:36.021604: acc: 96.16%, sen: 85.74%, spe: 98.26%
2023-08-21 23:05:36.024964: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:05:36.027399: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:05:36.029865: finished real validation
2023-08-21 23:06:01.783080: train_loss -0.9425
2023-08-21 23:06:01.786740: val_loss -0.6983
2023-08-21 23:06:01.790022: Pseudo dice [0.8896]
2023-08-21 23:06:01.792948: Epoch time: 660.2 s
2023-08-21 23:06:03.118606: 
2023-08-21 23:06:03.122190: Epoch 293
2023-08-21 23:06:03.124906: Current learning rate: 0.00017
2023-08-21 23:06:03.127935: start training, 250
================num of epochs: 250================
2023-08-21 23:12:55.224049: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 23:12:55.549305: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 23:12:55.553993: The split file contains 1 splits.
2023-08-21 23:12:55.556733: Desired fold for training: 0
2023-08-21 23:12:55.559608: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 23:16:39.125209: dsc: 88.11%
2023-08-21 23:16:39.128773: miou: 78.75%
2023-08-21 23:16:39.131398: acc: 96.15%, sen: 85.11%, spe: 98.38%
2023-08-21 23:16:39.134917: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:16:39.137549: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:16:39.139981: finished real validation
2023-08-21 23:17:06.174244: train_loss -0.9435
2023-08-21 23:17:06.177815: val_loss -0.6579
2023-08-21 23:17:06.181067: Pseudo dice [0.874]
2023-08-21 23:17:06.184052: Epoch time: 663.06 s
2023-08-21 23:17:07.507246: 
2023-08-21 23:17:07.510507: Epoch 294
2023-08-21 23:17:07.513261: Current learning rate: 0.00015
2023-08-21 23:17:07.516413: start training, 250
================num of epochs: 250================
2023-08-21 23:24:02.550122: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 23:24:02.865871: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 23:24:02.870989: The split file contains 1 splits.
2023-08-21 23:24:02.875031: Desired fold for training: 0
2023-08-21 23:24:02.877991: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 23:27:44.586599: dsc: 88.22%
2023-08-21 23:27:44.590235: miou: 78.92%
2023-08-21 23:27:44.592777: acc: 96.19%, sen: 85.29%, spe: 98.38%
2023-08-21 23:27:44.596399: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:27:44.599003: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:27:44.601449: finished real validation
2023-08-21 23:28:10.385297: train_loss -0.9424
2023-08-21 23:28:10.388671: val_loss -0.6795
2023-08-21 23:28:10.391616: Pseudo dice [0.8816]
2023-08-21 23:28:10.394335: Epoch time: 662.88 s
2023-08-21 23:28:11.720631: 
2023-08-21 23:28:11.724023: Epoch 295
2023-08-21 23:28:11.726677: Current learning rate: 0.00013
2023-08-21 23:28:11.729670: start training, 250
================num of epochs: 250================
2023-08-21 23:35:04.691189: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 23:35:05.017555: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 23:35:05.022604: The split file contains 1 splits.
2023-08-21 23:35:05.025553: Desired fold for training: 0
2023-08-21 23:35:05.029353: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 23:38:46.654328: dsc: 88.33%
2023-08-21 23:38:46.657840: miou: 79.09%
2023-08-21 23:38:46.660457: acc: 96.20%, sen: 85.90%, spe: 98.27%
2023-08-21 23:38:46.663875: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:38:46.666314: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:38:46.668746: finished real validation
2023-08-21 23:39:12.442338: train_loss -0.9414
2023-08-21 23:39:12.446262: val_loss -0.671
2023-08-21 23:39:12.449438: Pseudo dice [0.878]
2023-08-21 23:39:12.452054: Epoch time: 660.72 s
2023-08-21 23:39:13.764009: 
2023-08-21 23:39:13.768061: Epoch 296
2023-08-21 23:39:13.770761: Current learning rate: 0.0001
2023-08-21 23:39:13.773686: start training, 250
================num of epochs: 250================
2023-08-21 23:46:05.695951: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 23:46:06.017198: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 23:46:06.021497: The split file contains 1 splits.
2023-08-21 23:46:06.024408: Desired fold for training: 0
2023-08-21 23:46:06.028333: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-21 23:49:46.441976: dsc: 88.12%
2023-08-21 23:49:46.445601: miou: 78.76%
2023-08-21 23:49:46.448010: acc: 96.16%, sen: 84.94%, spe: 98.42%
2023-08-21 23:49:46.451420: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:49:46.453968: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-21 23:49:46.456421: finished real validation
2023-08-21 23:50:12.085468: train_loss -0.9435
2023-08-21 23:50:12.088768: val_loss -0.7027
2023-08-21 23:50:12.092160: Pseudo dice [0.8871]
2023-08-21 23:50:12.094778: Epoch time: 658.32 s
2023-08-21 23:50:13.394874: 
2023-08-21 23:50:13.397672: Epoch 297
2023-08-21 23:50:13.400347: Current learning rate: 8e-05
2023-08-21 23:50:13.403134: start training, 250
================num of epochs: 250================
2023-08-21 23:57:05.581263: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-21 23:57:05.885231: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-21 23:57:05.890794: The split file contains 1 splits.
2023-08-21 23:57:05.893763: Desired fold for training: 0
2023-08-21 23:57:05.897372: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-22 00:00:45.752311: dsc: 88.07%
2023-08-22 00:00:45.756116: miou: 78.68%
2023-08-22 00:00:45.758885: acc: 96.14%, sen: 85.09%, spe: 98.36%
2023-08-22 00:00:45.763301: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-22 00:00:45.766148: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-22 00:00:45.768938: finished real validation
2023-08-22 00:01:11.414355: train_loss -0.9431
2023-08-22 00:01:11.419936: val_loss -0.6443
2023-08-22 00:01:11.423902: Pseudo dice [0.8721]
2023-08-22 00:01:11.426669: Epoch time: 658.02 s
2023-08-22 00:01:12.734530: 
2023-08-22 00:01:12.738601: Epoch 298
2023-08-22 00:01:12.741154: Current learning rate: 6e-05
2023-08-22 00:01:12.745460: start training, 250
================num of epochs: 250================
2023-08-22 00:08:05.805863: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-22 00:08:06.132550: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-22 00:08:06.137808: The split file contains 1 splits.
2023-08-22 00:08:06.142886: Desired fold for training: 0
2023-08-22 00:08:06.147111: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-22 00:11:48.415905: dsc: 88.20%
2023-08-22 00:11:48.419848: miou: 78.89%
2023-08-22 00:11:48.422488: acc: 96.18%, sen: 85.36%, spe: 98.35%
2023-08-22 00:11:48.427078: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-22 00:11:48.430969: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-22 00:11:48.433666: finished real validation
2023-08-22 00:12:13.877734: train_loss -0.9442
2023-08-22 00:12:13.881983: val_loss -0.6777
2023-08-22 00:12:13.885426: Pseudo dice [0.8845]
2023-08-22 00:12:13.888719: Epoch time: 661.15 s
2023-08-22 00:12:15.210020: 
2023-08-22 00:12:15.213901: Epoch 299
2023-08-22 00:12:15.216675: Current learning rate: 3e-05
2023-08-22 00:12:15.219708: start training, 250
================num of epochs: 250================
2023-08-22 00:19:08.430372: finish training
/afs/crc.nd.edu/user/y/ypeng4/.conda/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2023-08-22 00:19:08.764094: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-22 00:19:08.768939: The split file contains 1 splits.
2023-08-22 00:19:08.771814: Desired fold for training: 0
2023-08-22 00:19:08.774478: This split has 1500 training and 650 validation cases.
start computing score....
2023-08-22 00:22:48.018933: dsc: 88.26%
2023-08-22 00:22:48.022545: miou: 78.98%
2023-08-22 00:22:48.024896: acc: 96.19%, sen: 85.48%, spe: 98.35%
2023-08-22 00:22:48.028747: current best miou: 0.7952371091590432 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-22 00:22:48.031512: current best dsc: 0.8859410326378138 at epoch: 201, (201, 0.7952371091590432, 0.8859410326378138)
2023-08-22 00:22:48.034732: finished real validation
2023-08-22 00:23:13.642477: train_loss -0.9439
2023-08-22 00:23:13.646244: val_loss -0.6585
2023-08-22 00:23:13.649273: Pseudo dice [0.8751]
2023-08-22 00:23:13.651922: Epoch time: 658.44 s
2023-08-22 00:23:18.428663: Training done.
2023-08-22 00:23:18.794594: Using splits from existing split file: /tmp/ypeng4/preprocessed_data/Dataset122_ISIC2017/splits_final.json
2023-08-22 00:23:18.826400: The split file contains 1 splits.
2023-08-22 00:23:18.829953: Desired fold for training: 0
2023-08-22 00:23:18.833488: This split has 1500 training and 650 validation cases.
2023-08-22 00:23:18.858551: predicting ISIC_0001769
2023-08-22 00:23:19.074393: predicting ISIC_0001852
2023-08-22 00:23:19.259656: predicting ISIC_0001871
2023-08-22 00:23:19.445187: predicting ISIC_0003462
2023-08-22 00:23:19.635567: predicting ISIC_0003539
2023-08-22 00:23:19.820897: predicting ISIC_0003582
2023-08-22 00:23:20.006022: predicting ISIC_0003657
2023-08-22 00:23:20.192886: predicting ISIC_0003805
2023-08-22 00:23:20.386427: predicting ISIC_0004337
2023-08-22 00:23:20.573354: predicting ISIC_0006651
2023-08-22 00:23:20.757282: predicting ISIC_0006671
2023-08-22 00:23:20.942811: predicting ISIC_0006815
2023-08-22 00:23:21.131252: predicting ISIC_0006914
2023-08-22 00:23:21.317663: predicting ISIC_0007141
2023-08-22 00:23:21.501505: predicting ISIC_0007156
2023-08-22 00:23:21.688174: predicting ISIC_0007235
2023-08-22 00:23:21.873266: predicting ISIC_0007241
2023-08-22 00:23:22.058696: predicting ISIC_0007332
2023-08-22 00:23:22.244667: predicting ISIC_0007344
2023-08-22 00:23:22.428816: predicting ISIC_0007528
2023-08-22 00:23:22.613534: predicting ISIC_0007796
2023-08-22 00:23:22.797681: predicting ISIC_0008025
2023-08-22 00:23:22.981748: predicting ISIC_0008524
2023-08-22 00:23:23.166083: predicting ISIC_0009995
2023-08-22 00:23:25.459094: predicting ISIC_0010459
2023-08-22 00:23:25.654821: predicting ISIC_0012099
2023-08-22 00:23:25.883862: predicting ISIC_0012109
2023-08-22 00:23:26.052057: predicting ISIC_0012126
2023-08-22 00:23:26.218189: predicting ISIC_0012127
2023-08-22 00:23:26.758035: predicting ISIC_0012143
2023-08-22 00:23:26.923980: predicting ISIC_0012151
2023-08-22 00:23:27.097142: predicting ISIC_0012159
2023-08-22 00:23:27.270929: predicting ISIC_0012160
2023-08-22 00:23:27.443043: predicting ISIC_0012191
2023-08-22 00:23:27.611676: predicting ISIC_0012201
2023-08-22 00:23:27.781143: predicting ISIC_0012204
2023-08-22 00:23:27.948527: predicting ISIC_0012206
2023-08-22 00:23:28.115614: predicting ISIC_0012210
2023-08-22 00:23:28.283370: predicting ISIC_0012221
2023-08-22 00:23:28.476439: predicting ISIC_0012222
2023-08-22 00:23:28.665353: predicting ISIC_0012254
2023-08-22 00:23:28.856830: predicting ISIC_0012256
2023-08-22 00:23:29.022776: predicting ISIC_0012288
2023-08-22 00:23:29.189691: predicting ISIC_0012306
2023-08-22 00:23:29.354792: predicting ISIC_0012313
2023-08-22 00:23:29.522763: predicting ISIC_0012316
2023-08-22 00:23:29.694062: predicting ISIC_0012335
2023-08-22 00:23:29.864233: predicting ISIC_0012380
2023-08-22 00:23:30.033552: predicting ISIC_0012383
2023-08-22 00:23:30.203304: predicting ISIC_0012400
2023-08-22 00:23:30.369413: predicting ISIC_0012417
2023-08-22 00:23:30.534710: predicting ISIC_0012434
2023-08-22 00:23:30.700335: predicting ISIC_0012492
2023-08-22 00:23:30.865735: predicting ISIC_0012513
2023-08-22 00:23:31.033092: predicting ISIC_0012538
2023-08-22 00:23:31.201827: predicting ISIC_0012547
2023-08-22 00:23:31.370939: predicting ISIC_0012660
2023-08-22 00:23:31.540461: predicting ISIC_0012684
2023-08-22 00:23:31.709505: predicting ISIC_0012720
2023-08-22 00:23:31.876706: predicting ISIC_0012746
2023-08-22 00:23:32.047425: predicting ISIC_0012876
2023-08-22 00:23:32.215170: predicting ISIC_0012927
2023-08-22 00:23:32.384077: predicting ISIC_0012956
2023-08-22 00:23:32.553916: predicting ISIC_0012959
2023-08-22 00:23:32.717820: predicting ISIC_0012965
2023-08-22 00:23:32.883817: predicting ISIC_0013010
2023-08-22 00:23:33.049882: predicting ISIC_0013082
2023-08-22 00:23:33.214725: predicting ISIC_0013104
2023-08-22 00:23:33.379013: predicting ISIC_0013127
2023-08-22 00:23:33.543967: predicting ISIC_0013128
2023-08-22 00:23:33.714406: predicting ISIC_0013132
2023-08-22 00:23:33.878978: predicting ISIC_0013188
2023-08-22 00:23:34.045978: predicting ISIC_0013215
2023-08-22 00:23:34.213511: predicting ISIC_0013232
2023-08-22 00:23:34.378155: predicting ISIC_0013421
2023-08-22 00:23:34.542519: predicting ISIC_0013474
2023-08-22 00:23:34.705326: predicting ISIC_0013480
2023-08-22 00:23:34.869687: predicting ISIC_0013486
2023-08-22 00:23:35.038382: predicting ISIC_0013487
2023-08-22 00:23:35.203842: predicting ISIC_0013488
2023-08-22 00:23:35.372702: predicting ISIC_0013489
2023-08-22 00:23:35.563945: predicting ISIC_0013490
2023-08-22 00:23:35.734866: predicting ISIC_0013491
2023-08-22 00:23:35.902314: predicting ISIC_0013492
2023-08-22 00:23:36.070945: predicting ISIC_0013493
2023-08-22 00:23:36.250331: predicting ISIC_0013494
2023-08-22 00:23:36.438269: predicting ISIC_0013495
2023-08-22 00:23:36.645082: predicting ISIC_0013497
2023-08-22 00:23:36.811609: predicting ISIC_0013498
2023-08-22 00:23:36.976442: predicting ISIC_0013499
2023-08-22 00:23:37.140666: predicting ISIC_0013500
2023-08-22 00:23:37.308151: predicting ISIC_0013501
2023-08-22 00:23:37.474774: predicting ISIC_0013516
2023-08-22 00:23:37.638816: predicting ISIC_0013517
2023-08-22 00:23:37.812393: predicting ISIC_0013518
2023-08-22 00:23:37.977448: predicting ISIC_0013523
2023-08-22 00:23:38.145681: predicting ISIC_0013525
2023-08-22 00:23:38.315501: predicting ISIC_0013526
2023-08-22 00:23:38.481523: predicting ISIC_0013527
2023-08-22 00:23:38.645044: predicting ISIC_0013530
2023-08-22 00:23:38.810712: predicting ISIC_0013549
2023-08-22 00:23:38.974446: predicting ISIC_0013552
2023-08-22 00:23:39.140815: predicting ISIC_0013553
2023-08-22 00:23:39.305814: predicting ISIC_0013554
2023-08-22 00:23:39.471243: predicting ISIC_0013558
2023-08-22 00:23:39.637128: predicting ISIC_0013559
2023-08-22 00:23:39.805743: predicting ISIC_0013561
2023-08-22 00:23:39.974274: predicting ISIC_0013562
2023-08-22 00:23:40.140170: predicting ISIC_0013567
2023-08-22 00:23:40.306846: predicting ISIC_0013568
2023-08-22 00:23:40.475685: predicting ISIC_0013572
2023-08-22 00:23:40.644713: predicting ISIC_0013573
2023-08-22 00:23:40.812439: predicting ISIC_0013578
2023-08-22 00:23:40.981649: predicting ISIC_0013579
2023-08-22 00:23:41.148318: predicting ISIC_0013580
2023-08-22 00:23:41.317025: predicting ISIC_0013581
2023-08-22 00:23:41.483880: predicting ISIC_0013584
2023-08-22 00:23:41.650551: predicting ISIC_0013585
2023-08-22 00:23:41.818698: predicting ISIC_0013592
2023-08-22 00:23:41.991292: predicting ISIC_0013594
2023-08-22 00:23:42.158855: predicting ISIC_0013595
2023-08-22 00:23:42.325450: predicting ISIC_0013596
2023-08-22 00:23:42.494472: predicting ISIC_0013597
2023-08-22 00:23:42.660944: predicting ISIC_0013599
2023-08-22 00:23:42.825697: predicting ISIC_0013601
2023-08-22 00:23:42.991684: predicting ISIC_0013603
2023-08-22 00:23:43.160583: predicting ISIC_0013606
2023-08-22 00:23:43.327288: predicting ISIC_0013610
2023-08-22 00:23:43.497310: predicting ISIC_0013618
2023-08-22 00:23:43.664438: predicting ISIC_0013621
2023-08-22 00:23:43.832333: predicting ISIC_0013625
2023-08-22 00:23:44.004427: predicting ISIC_0013626
2023-08-22 00:23:44.171944: predicting ISIC_0013632
2023-08-22 00:23:44.340759: predicting ISIC_0013634
2023-08-22 00:23:44.512137: predicting ISIC_0013635
2023-08-22 00:23:44.681454: predicting ISIC_0013637
2023-08-22 00:23:44.849842: predicting ISIC_0013639
2023-08-22 00:23:45.020751: predicting ISIC_0013643
2023-08-22 00:23:45.188898: predicting ISIC_0013644
2023-08-22 00:23:45.356537: predicting ISIC_0013651
2023-08-22 00:23:45.525515: predicting ISIC_0013652
2023-08-22 00:23:45.695332: predicting ISIC_0013656
2023-08-22 00:23:45.862202: predicting ISIC_0013663
2023-08-22 00:23:46.032411: predicting ISIC_0013664
2023-08-22 00:23:46.201530: predicting ISIC_0013667
2023-08-22 00:23:46.370978: predicting ISIC_0013670
2023-08-22 00:23:46.540169: predicting ISIC_0013671
2023-08-22 00:23:46.708260: predicting ISIC_0013672
2023-08-22 00:23:46.876804: predicting ISIC_0013674
2023-08-22 00:23:47.044674: predicting ISIC_0013675
2023-08-22 00:23:47.213067: predicting ISIC_0013676
2023-08-22 00:23:47.385571: predicting ISIC_0013680
2023-08-22 00:23:47.558965: predicting ISIC_0013682
2023-08-22 00:23:47.733412: predicting ISIC_0013684
2023-08-22 00:23:47.902176: predicting ISIC_0013685
2023-08-22 00:23:48.077296: predicting ISIC_0013687
2023-08-22 00:23:48.250885: predicting ISIC_0013688
2023-08-22 00:23:48.426721: predicting ISIC_0013689
2023-08-22 00:23:48.606686: predicting ISIC_0013690
2023-08-22 00:23:48.779279: predicting ISIC_0013691
2023-08-22 00:23:48.949243: predicting ISIC_0013695
2023-08-22 00:23:49.121943: predicting ISIC_0013702
2023-08-22 00:23:49.295619: predicting ISIC_0013706
2023-08-22 00:23:49.468680: predicting ISIC_0013707
2023-08-22 00:23:49.647508: predicting ISIC_0013709
2023-08-22 00:23:49.817244: predicting ISIC_0013712
2023-08-22 00:23:49.984021: predicting ISIC_0013713
2023-08-22 00:23:50.153638: predicting ISIC_0013719
2023-08-22 00:23:50.322923: predicting ISIC_0013721
2023-08-22 00:23:50.493396: predicting ISIC_0013725
2023-08-22 00:23:50.662710: predicting ISIC_0013731
2023-08-22 00:23:50.830163: predicting ISIC_0013736
2023-08-22 00:23:50.996156: predicting ISIC_0013737
2023-08-22 00:23:51.163337: predicting ISIC_0013740
2023-08-22 00:23:51.334450: predicting ISIC_0013742
2023-08-22 00:23:51.502996: predicting ISIC_0013747
2023-08-22 00:23:51.670504: predicting ISIC_0013748
2023-08-22 00:23:51.840698: predicting ISIC_0013749
2023-08-22 00:23:52.006670: predicting ISIC_0013758
2023-08-22 00:23:52.178673: predicting ISIC_0013762
2023-08-22 00:23:52.346357: predicting ISIC_0013765
2023-08-22 00:23:52.514253: predicting ISIC_0013772
2023-08-22 00:23:52.682832: predicting ISIC_0013775
2023-08-22 00:23:52.849587: predicting ISIC_0013777
2023-08-22 00:23:53.017478: predicting ISIC_0013778
2023-08-22 00:23:53.187543: predicting ISIC_0013782
2023-08-22 00:23:53.358372: predicting ISIC_0013783
2023-08-22 00:23:53.533374: predicting ISIC_0013789
2023-08-22 00:23:53.704131: predicting ISIC_0013792
2023-08-22 00:23:53.873112: predicting ISIC_0013793
2023-08-22 00:23:54.041759: predicting ISIC_0013795
2023-08-22 00:23:54.213233: predicting ISIC_0013796
2023-08-22 00:23:54.382488: predicting ISIC_0013797
2023-08-22 00:23:54.554194: predicting ISIC_0013798
2023-08-22 00:23:54.720127: predicting ISIC_0013799
2023-08-22 00:23:54.888055: predicting ISIC_0013800
2023-08-22 00:23:55.052751: predicting ISIC_0013801
2023-08-22 00:23:55.218208: predicting ISIC_0013802
2023-08-22 00:23:55.383538: predicting ISIC_0013803
2023-08-22 00:23:55.548734: predicting ISIC_0013804
2023-08-22 00:23:55.716281: predicting ISIC_0013805
2023-08-22 00:23:55.882022: predicting ISIC_0013806
2023-08-22 00:23:56.049316: predicting ISIC_0013807
2023-08-22 00:23:56.216732: predicting ISIC_0013808
2023-08-22 00:23:56.390850: predicting ISIC_0013815
2023-08-22 00:23:56.557698: predicting ISIC_0013816
2023-08-22 00:23:56.726076: predicting ISIC_0013817
2023-08-22 00:23:56.893161: predicting ISIC_0013819
2023-08-22 00:23:57.059439: predicting ISIC_0013828
2023-08-22 00:23:57.226559: predicting ISIC_0013830
2023-08-22 00:23:57.392463: predicting ISIC_0013831
2023-08-22 00:23:57.557869: predicting ISIC_0013832
2023-08-22 00:23:57.725654: predicting ISIC_0013835
2023-08-22 00:23:57.892069: predicting ISIC_0013836
2023-08-22 00:23:58.059918: predicting ISIC_0013837
2023-08-22 00:23:58.226146: predicting ISIC_0013839
2023-08-22 00:23:58.401299: predicting ISIC_0013840
2023-08-22 00:23:58.576968: predicting ISIC_0013841
2023-08-22 00:23:58.745837: predicting ISIC_0013843
2023-08-22 00:23:58.913620: predicting ISIC_0013844
2023-08-22 00:23:59.081664: predicting ISIC_0013845
2023-08-22 00:23:59.247749: predicting ISIC_0013861
2023-08-22 00:23:59.417377: predicting ISIC_0013862
2023-08-22 00:23:59.584519: predicting ISIC_0013863
2023-08-22 00:23:59.752893: predicting ISIC_0013864
2023-08-22 00:23:59.920130: predicting ISIC_0013865
2023-08-22 00:24:00.087051: predicting ISIC_0013874
2023-08-22 00:24:00.254887: predicting ISIC_0013876
2023-08-22 00:24:00.426642: predicting ISIC_0013879
2023-08-22 00:24:00.594514: predicting ISIC_0013886
2023-08-22 00:24:00.762749: predicting ISIC_0013888
2023-08-22 00:24:00.927147: predicting ISIC_0013890
2023-08-22 00:24:01.094053: predicting ISIC_0013896
2023-08-22 00:24:01.259283: predicting ISIC_0013898
2023-08-22 00:24:01.426219: predicting ISIC_0013910
2023-08-22 00:24:01.592921: predicting ISIC_0013918
2023-08-22 00:24:01.760869: predicting ISIC_0013921
2023-08-22 00:24:01.928517: predicting ISIC_0013922
2023-08-22 00:24:02.092944: predicting ISIC_0013927
2023-08-22 00:24:02.257432: predicting ISIC_0013929
2023-08-22 00:24:02.434984: predicting ISIC_0013933
2023-08-22 00:24:02.605927: predicting ISIC_0013935
2023-08-22 00:24:02.772300: predicting ISIC_0013936
2023-08-22 00:24:02.942123: predicting ISIC_0013942
2023-08-22 00:24:03.106448: predicting ISIC_0013945
2023-08-22 00:24:03.270189: predicting ISIC_0013946
2023-08-22 00:24:03.433863: predicting ISIC_0013958
2023-08-22 00:24:03.600581: predicting ISIC_0013961
2023-08-22 00:24:03.766611: predicting ISIC_0013962
2023-08-22 00:24:03.931238: predicting ISIC_0013965
2023-08-22 00:24:04.097626: predicting ISIC_0013967
2023-08-22 00:24:04.262675: predicting ISIC_0013969
2023-08-22 00:24:04.431378: predicting ISIC_0013970
2023-08-22 00:24:04.607944: predicting ISIC_0013971
2023-08-22 00:24:04.780401: predicting ISIC_0013972
2023-08-22 00:24:04.947420: predicting ISIC_0013975
2023-08-22 00:24:05.115845: predicting ISIC_0013980
2023-08-22 00:24:05.284170: predicting ISIC_0013981
2023-08-22 00:24:05.452345: predicting ISIC_0013982
2023-08-22 00:24:05.621138: predicting ISIC_0013983
2023-08-22 00:24:05.789715: predicting ISIC_0013984
2023-08-22 00:24:05.959745: predicting ISIC_0013986
2023-08-22 00:24:06.128955: predicting ISIC_0013995
2023-08-22 00:24:06.300583: predicting ISIC_0013996
2023-08-22 00:24:06.471555: predicting ISIC_0013997
2023-08-22 00:24:06.648416: predicting ISIC_0014001
2023-08-22 00:24:06.822341: predicting ISIC_0014004
2023-08-22 00:24:06.990800: predicting ISIC_0014013
2023-08-22 00:24:07.157483: predicting ISIC_0014026
2023-08-22 00:24:07.323275: predicting ISIC_0014028
2023-08-22 00:24:07.490484: predicting ISIC_0014029
2023-08-22 00:24:07.657961: predicting ISIC_0014031
2023-08-22 00:24:07.825777: predicting ISIC_0014032
2023-08-22 00:24:07.993910: predicting ISIC_0014037
2023-08-22 00:24:08.161717: predicting ISIC_0014038
2023-08-22 00:24:08.326349: predicting ISIC_0014044
2023-08-22 00:24:08.492760: predicting ISIC_0014045
2023-08-22 00:24:08.666405: predicting ISIC_0014046
2023-08-22 00:24:08.834967: predicting ISIC_0014049
2023-08-22 00:24:09.003088: predicting ISIC_0014055
2023-08-22 00:24:09.169032: predicting ISIC_0014061
2023-08-22 00:24:09.340547: predicting ISIC_0014062
2023-08-22 00:24:09.506845: predicting ISIC_0014066
2023-08-22 00:24:09.675642: predicting ISIC_0014069
2023-08-22 00:24:09.840866: predicting ISIC_0014072
2023-08-22 00:24:10.009548: predicting ISIC_0014073
2023-08-22 00:24:10.177513: predicting ISIC_0014074
2023-08-22 00:24:10.349025: predicting ISIC_0014076
2023-08-22 00:24:10.521236: predicting ISIC_0014079
2023-08-22 00:24:10.751752: predicting ISIC_0014080
2023-08-22 00:24:10.922457: predicting ISIC_0014081
2023-08-22 00:24:11.091411: predicting ISIC_0014082
2023-08-22 00:24:11.267961: predicting ISIC_0014083
2023-08-22 00:24:11.446726: predicting ISIC_0014088
2023-08-22 00:24:11.615100: predicting ISIC_0014089
2023-08-22 00:24:11.785985: predicting ISIC_0014092
2023-08-22 00:24:11.953660: predicting ISIC_0014093
2023-08-22 00:24:12.120880: predicting ISIC_0014094
2023-08-22 00:24:12.288665: predicting ISIC_0014099
2023-08-22 00:24:12.455462: predicting ISIC_0014108
2023-08-22 00:24:12.622974: predicting ISIC_0014114
2023-08-22 00:24:12.792971: predicting ISIC_0014120
2023-08-22 00:24:12.960057: predicting ISIC_0014127
2023-08-22 00:24:13.129038: predicting ISIC_0014131
2023-08-22 00:24:13.295867: predicting ISIC_0014132
2023-08-22 00:24:13.464976: predicting ISIC_0014133
2023-08-22 00:24:13.634059: predicting ISIC_0014136
2023-08-22 00:24:13.801241: predicting ISIC_0014139
2023-08-22 00:24:13.969574: predicting ISIC_0014144
2023-08-22 00:24:14.137401: predicting ISIC_0014149
2023-08-22 00:24:14.303653: predicting ISIC_0014150
2023-08-22 00:24:14.471016: predicting ISIC_0014151
2023-08-22 00:24:14.638875: predicting ISIC_0014156
2023-08-22 00:24:14.811874: predicting ISIC_0014157
2023-08-22 00:24:14.976874: predicting ISIC_0014158
2023-08-22 00:24:15.144378: predicting ISIC_0014162
2023-08-22 00:24:15.311735: predicting ISIC_0014163
2023-08-22 00:24:15.478608: predicting ISIC_0014164
2023-08-22 00:24:15.645478: predicting ISIC_0014166
2023-08-22 00:24:15.813161: predicting ISIC_0014169
2023-08-22 00:24:15.982671: predicting ISIC_0014171
2023-08-22 00:24:16.154402: predicting ISIC_0014173
2023-08-22 00:24:16.320245: predicting ISIC_0014174
2023-08-22 00:24:16.485843: predicting ISIC_0014178
2023-08-22 00:24:16.653748: predicting ISIC_0014179
2023-08-22 00:24:16.820708: predicting ISIC_0014183
2023-08-22 00:24:16.996135: predicting ISIC_0014185
2023-08-22 00:24:17.164292: predicting ISIC_0014187
2023-08-22 00:24:17.332901: predicting ISIC_0014189
2023-08-22 00:24:17.500911: predicting ISIC_0014190
2023-08-22 00:24:17.670249: predicting ISIC_0014191
2023-08-22 00:24:17.838989: predicting ISIC_0014195
2023-08-22 00:24:18.008941: predicting ISIC_0014197
2023-08-22 00:24:18.175693: predicting ISIC_0014211
2023-08-22 00:24:18.343033: predicting ISIC_0014212
2023-08-22 00:24:18.508977: predicting ISIC_0014216
2023-08-22 00:24:18.678107: predicting ISIC_0014217
2023-08-22 00:24:18.843602: predicting ISIC_0014222
2023-08-22 00:24:19.014303: predicting ISIC_0014225
2023-08-22 00:24:19.180350: predicting ISIC_0014229
2023-08-22 00:24:19.346968: predicting ISIC_0014238
2023-08-22 00:24:19.511730: predicting ISIC_0014248
2023-08-22 00:24:19.677620: predicting ISIC_0014249
2023-08-22 00:24:19.842379: predicting ISIC_0014253
2023-08-22 00:24:20.009149: predicting ISIC_0014263
2023-08-22 00:24:20.177809: predicting ISIC_0014264
2023-08-22 00:24:20.341677: predicting ISIC_0014272
2023-08-22 00:24:20.506169: predicting ISIC_0014273
2023-08-22 00:24:20.670521: predicting ISIC_0014274
2023-08-22 00:24:20.835924: predicting ISIC_0014286
2023-08-22 00:24:21.003711: predicting ISIC_0014289
2023-08-22 00:24:21.169327: predicting ISIC_0014290
2023-08-22 00:24:21.337067: predicting ISIC_0014291
2023-08-22 00:24:21.504624: predicting ISIC_0014299
2023-08-22 00:24:21.672837: predicting ISIC_0014302
2023-08-22 00:24:21.840222: predicting ISIC_0014308
2023-08-22 00:24:22.005710: predicting ISIC_0014310
2023-08-22 00:24:22.182941: predicting ISIC_0014311
2023-08-22 00:24:22.532322: predicting ISIC_0014316
2023-08-22 00:24:22.701222: predicting ISIC_0014317
2023-08-22 00:24:22.871964: predicting ISIC_0014324
2023-08-22 00:24:23.042132: predicting ISIC_0014325
2023-08-22 00:24:23.212010: predicting ISIC_0014327
2023-08-22 00:24:23.384747: predicting ISIC_0014328
2023-08-22 00:24:23.556691: predicting ISIC_0014331
2023-08-22 00:24:23.721683: predicting ISIC_0014337
2023-08-22 00:24:23.891352: predicting ISIC_0014341
2023-08-22 00:24:24.061832: predicting ISIC_0014346
2023-08-22 00:24:24.233789: predicting ISIC_0014347
2023-08-22 00:24:24.402557: predicting ISIC_0014353
2023-08-22 00:24:24.570147: predicting ISIC_0014357
2023-08-22 00:24:24.737035: predicting ISIC_0014360
2023-08-22 00:24:24.904445: predicting ISIC_0014361
2023-08-22 00:24:25.071801: predicting ISIC_0014365
2023-08-22 00:24:25.249226: predicting ISIC_0014366
2023-08-22 00:24:25.421042: predicting ISIC_0014372
2023-08-22 00:24:25.588449: predicting ISIC_0014382
2023-08-22 00:24:25.756659: predicting ISIC_0014385
2023-08-22 00:24:25.924062: predicting ISIC_0014393
2023-08-22 00:24:26.092112: predicting ISIC_0014394
2023-08-22 00:24:26.259116: predicting ISIC_0014395
2023-08-22 00:24:26.430326: predicting ISIC_0014397
2023-08-22 00:24:26.603623: predicting ISIC_0014410
2023-08-22 00:24:26.772784: predicting ISIC_0014422
2023-08-22 00:24:26.940818: predicting ISIC_0014428
2023-08-22 00:24:27.113452: predicting ISIC_0014430
2023-08-22 00:24:27.285976: predicting ISIC_0014431
2023-08-22 00:24:27.454107: predicting ISIC_0014432
2023-08-22 00:24:27.622528: predicting ISIC_0014433
2023-08-22 00:24:27.789724: predicting ISIC_0014438
2023-08-22 00:24:27.960958: predicting ISIC_0014440
2023-08-22 00:24:28.128738: predicting ISIC_0014441
2023-08-22 00:24:28.301546: predicting ISIC_0014453
2023-08-22 00:24:28.469724: predicting ISIC_0014458
2023-08-22 00:24:28.640352: predicting ISIC_0014469
2023-08-22 00:24:28.809116: predicting ISIC_0014473
2023-08-22 00:24:28.978086: predicting ISIC_0014475
2023-08-22 00:24:29.144804: predicting ISIC_0014476
2023-08-22 00:24:29.319418: predicting ISIC_0014480
2023-08-22 00:24:29.486670: predicting ISIC_0014486
2023-08-22 00:24:29.654680: predicting ISIC_0014490
2023-08-22 00:24:29.822051: predicting ISIC_0014498
2023-08-22 00:24:29.991686: predicting ISIC_0014501
2023-08-22 00:24:30.156910: predicting ISIC_0014502
2023-08-22 00:24:30.325560: predicting ISIC_0014504
2023-08-22 00:24:30.495740: predicting ISIC_0014507
2023-08-22 00:24:30.666833: predicting ISIC_0014511
2023-08-22 00:24:30.831866: predicting ISIC_0014515
2023-08-22 00:24:31.001615: predicting ISIC_0014516
2023-08-22 00:24:31.176610: predicting ISIC_0014518
2023-08-22 00:24:31.348861: predicting ISIC_0014522
2023-08-22 00:24:31.517099: predicting ISIC_0014525
2023-08-22 00:24:31.683910: predicting ISIC_0014526
2023-08-22 00:24:31.850802: predicting ISIC_0014527
2023-08-22 00:24:32.018228: predicting ISIC_0014529
2023-08-22 00:24:32.185348: predicting ISIC_0014535
2023-08-22 00:24:32.356609: predicting ISIC_0014537
2023-08-22 00:24:32.527789: predicting ISIC_0014543
2023-08-22 00:24:32.693814: predicting ISIC_0014545
2023-08-22 00:24:32.857637: predicting ISIC_0014547
2023-08-22 00:24:33.026052: predicting ISIC_0014554
2023-08-22 00:24:33.194757: predicting ISIC_0014557
2023-08-22 00:24:33.364563: predicting ISIC_0014558
2023-08-22 00:24:33.533187: predicting ISIC_0014568
2023-08-22 00:24:33.703105: predicting ISIC_0014569
2023-08-22 00:24:33.877074: predicting ISIC_0014570
2023-08-22 00:24:34.048135: predicting ISIC_0014571
2023-08-22 00:24:34.215630: predicting ISIC_0014572
2023-08-22 00:24:34.382483: predicting ISIC_0014573
2023-08-22 00:24:34.549688: predicting ISIC_0014576
2023-08-22 00:24:34.720402: predicting ISIC_0014577
2023-08-22 00:24:34.888065: predicting ISIC_0014578
2023-08-22 00:24:35.056658: predicting ISIC_0014579
2023-08-22 00:24:35.226513: predicting ISIC_0014580
2023-08-22 00:24:35.400985: predicting ISIC_0014581
2023-08-22 00:24:35.566041: predicting ISIC_0014582
2023-08-22 00:24:35.732176: predicting ISIC_0014583
2023-08-22 00:24:35.896636: predicting ISIC_0014585
2023-08-22 00:24:36.068961: predicting ISIC_0014589
2023-08-22 00:24:36.238059: predicting ISIC_0014591
2023-08-22 00:24:36.410232: predicting ISIC_0014592
2023-08-22 00:24:36.579469: predicting ISIC_0014593
2023-08-22 00:24:36.748367: predicting ISIC_0014594
2023-08-22 00:24:36.914297: predicting ISIC_0014595
2023-08-22 00:24:37.082035: predicting ISIC_0014596
2023-08-22 00:24:37.250505: predicting ISIC_0014597
2023-08-22 00:24:37.420660: predicting ISIC_0014598
2023-08-22 00:24:37.591882: predicting ISIC_0014599
2023-08-22 00:24:37.766135: predicting ISIC_0014601
2023-08-22 00:24:37.933856: predicting ISIC_0014602
2023-08-22 00:24:38.106594: predicting ISIC_0014603
2023-08-22 00:24:38.274710: predicting ISIC_0014605
2023-08-22 00:24:38.446668: predicting ISIC_0014606
2023-08-22 00:24:38.617090: predicting ISIC_0014607
2023-08-22 00:24:38.784287: predicting ISIC_0014608
2023-08-22 00:24:38.951543: predicting ISIC_0014609
2023-08-22 00:24:39.118755: predicting ISIC_0014610
2023-08-22 00:24:39.285950: predicting ISIC_0014611
2023-08-22 00:24:39.458601: predicting ISIC_0014612
2023-08-22 00:24:39.634402: predicting ISIC_0014613
2023-08-22 00:24:39.812949: predicting ISIC_0014615
2023-08-22 00:24:39.978333: predicting ISIC_0014616
2023-08-22 00:24:40.145572: predicting ISIC_0014617
2023-08-22 00:24:40.311121: predicting ISIC_0014618
2023-08-22 00:24:40.477938: predicting ISIC_0014620
2023-08-22 00:24:40.645393: predicting ISIC_0014621
2023-08-22 00:24:40.813496: predicting ISIC_0014622
2023-08-22 00:24:40.981258: predicting ISIC_0014623
2023-08-22 00:24:41.562902: predicting ISIC_0014624
2023-08-22 00:24:41.742859: predicting ISIC_0014625
2023-08-22 00:24:41.911436: predicting ISIC_0014628
2023-08-22 00:24:42.081081: predicting ISIC_0014630
2023-08-22 00:24:42.248818: predicting ISIC_0014632
2023-08-22 00:24:42.413672: predicting ISIC_0014633
2023-08-22 00:24:42.592656: predicting ISIC_0014635
2023-08-22 00:24:42.759507: predicting ISIC_0014636
2023-08-22 00:24:42.925711: predicting ISIC_0014637
2023-08-22 00:24:43.091012: predicting ISIC_0014638
2023-08-22 00:24:43.256634: predicting ISIC_0014639
2023-08-22 00:24:43.424745: predicting ISIC_0014640
2023-08-22 00:24:43.597859: predicting ISIC_0014641
2023-08-22 00:24:43.774481: predicting ISIC_0014642
2023-08-22 00:24:43.941501: predicting ISIC_0014646
2023-08-22 00:24:44.107824: predicting ISIC_0014650
2023-08-22 00:24:44.275630: predicting ISIC_0014651
2023-08-22 00:24:44.441105: predicting ISIC_0014654
2023-08-22 00:24:44.615731: predicting ISIC_0014657
2023-08-22 00:24:44.784327: predicting ISIC_0014658
2023-08-22 00:24:44.953641: predicting ISIC_0014661
2023-08-22 00:24:45.121711: predicting ISIC_0014664
2023-08-22 00:24:45.288985: predicting ISIC_0014665
2023-08-22 00:24:45.456198: predicting ISIC_0014667
2023-08-22 00:24:45.623221: predicting ISIC_0014680
2023-08-22 00:24:45.794916: predicting ISIC_0014682
2023-08-22 00:24:45.964849: predicting ISIC_0014683
2023-08-22 00:24:46.131878: predicting ISIC_0014684
2023-08-22 00:24:46.299163: predicting ISIC_0014685
2023-08-22 00:24:46.466395: predicting ISIC_0014688
2023-08-22 00:24:46.633571: predicting ISIC_0014692
2023-08-22 00:24:46.804791: predicting ISIC_0014694
2023-08-22 00:24:46.972318: predicting ISIC_0014696
2023-08-22 00:24:47.136575: predicting ISIC_0014699
2023-08-22 00:24:47.303153: predicting ISIC_0014702
2023-08-22 00:24:47.466940: predicting ISIC_0014707
2023-08-22 00:24:47.635892: predicting ISIC_0014708
2023-08-22 00:24:47.805818: predicting ISIC_0014711
2023-08-22 00:24:47.970215: predicting ISIC_0014712
2023-08-22 00:24:48.134395: predicting ISIC_0014713
2023-08-22 00:24:48.299549: predicting ISIC_0014714
2023-08-22 00:24:48.464524: predicting ISIC_0014715
2023-08-22 00:24:48.631308: predicting ISIC_0014716
2023-08-22 00:24:48.801936: predicting ISIC_0014722
2023-08-22 00:24:48.972388: predicting ISIC_0014723
2023-08-22 00:24:49.138580: predicting ISIC_0014724
2023-08-22 00:24:49.302733: predicting ISIC_0014726
2023-08-22 00:24:49.467088: predicting ISIC_0014730
2023-08-22 00:24:49.633171: predicting ISIC_0014731
2023-08-22 00:24:49.804806: predicting ISIC_0014735
2023-08-22 00:24:49.973234: predicting ISIC_0014739
2023-08-22 00:24:50.139680: predicting ISIC_0014742
2023-08-22 00:24:50.304298: predicting ISIC_0014745
2023-08-22 00:24:50.468656: predicting ISIC_0014748
2023-08-22 00:24:50.632800: predicting ISIC_0014754
2023-08-22 00:24:50.799108: predicting ISIC_0014760
2023-08-22 00:24:50.962852: predicting ISIC_0014762
2023-08-22 00:24:51.127895: predicting ISIC_0014763
2023-08-22 00:24:51.293842: predicting ISIC_0014769
2023-08-22 00:24:51.461875: predicting ISIC_0014770
2023-08-22 00:24:51.629033: predicting ISIC_0014771
2023-08-22 00:24:51.796419: predicting ISIC_0014775
2023-08-22 00:24:51.962252: predicting ISIC_0014778
2023-08-22 00:24:52.127862: predicting ISIC_0014779
2023-08-22 00:24:52.292169: predicting ISIC_0014782
2023-08-22 00:24:52.458900: predicting ISIC_0014783
2023-08-22 00:24:52.631433: predicting ISIC_0014785
2023-08-22 00:24:52.802042: predicting ISIC_0014788
2023-08-22 00:24:52.971442: predicting ISIC_0014791
2023-08-22 00:24:53.140244: predicting ISIC_0014793
2023-08-22 00:24:53.308221: predicting ISIC_0014794
2023-08-22 00:24:53.476922: predicting ISIC_0014795
2023-08-22 00:24:53.649106: predicting ISIC_0014797
2023-08-22 00:24:53.817457: predicting ISIC_0014802
2023-08-22 00:24:53.992426: predicting ISIC_0014803
2023-08-22 00:24:54.163735: predicting ISIC_0014804
2023-08-22 00:24:54.332840: predicting ISIC_0014805
2023-08-22 00:24:54.500758: predicting ISIC_0014806
2023-08-22 00:24:54.669862: predicting ISIC_0014808
2023-08-22 00:24:54.838336: predicting ISIC_0014809
2023-08-22 00:24:55.005711: predicting ISIC_0014811
2023-08-22 00:24:55.174246: predicting ISIC_0014812
2023-08-22 00:24:55.342304: predicting ISIC_0014818
2023-08-22 00:24:55.510370: predicting ISIC_0014819
2023-08-22 00:24:55.677716: predicting ISIC_0014821
2023-08-22 00:24:55.846599: predicting ISIC_0014823
2023-08-22 00:24:56.017972: predicting ISIC_0014825
2023-08-22 00:24:56.185885: predicting ISIC_0014829
2023-08-22 00:24:56.352685: predicting ISIC_0014830
2023-08-22 00:24:56.519459: predicting ISIC_0014831
2023-08-22 00:24:56.688572: predicting ISIC_0014832
2023-08-22 00:24:56.859578: predicting ISIC_0014834
2023-08-22 00:24:57.029135: predicting ISIC_0014836
2023-08-22 00:24:57.195768: predicting ISIC_0014838
2023-08-22 00:24:57.364436: predicting ISIC_0014839
2023-08-22 00:24:57.531470: predicting ISIC_0014843
2023-08-22 00:24:57.699438: predicting ISIC_0014845
2023-08-22 00:24:57.866822: predicting ISIC_0014846
2023-08-22 00:24:58.037606: predicting ISIC_0014848
2023-08-22 00:24:58.207728: predicting ISIC_0014849
2023-08-22 00:24:58.378557: predicting ISIC_0014850
2023-08-22 00:24:58.545452: predicting ISIC_0014851
2023-08-22 00:24:58.716360: predicting ISIC_0014855
2023-08-22 00:24:58.885755: predicting ISIC_0014857
2023-08-22 00:24:59.054291: predicting ISIC_0014860
2023-08-22 00:24:59.223785: predicting ISIC_0014866
2023-08-22 00:24:59.392015: predicting ISIC_0014869
2023-08-22 00:24:59.559738: predicting ISIC_0014890
2023-08-22 00:24:59.728650: predicting ISIC_0014891
2023-08-22 00:24:59.895814: predicting ISIC_0014897
2023-08-22 00:25:00.063875: predicting ISIC_0014898
2023-08-22 00:25:00.235784: predicting ISIC_0014903
2023-08-22 00:25:00.402380: predicting ISIC_0014904
2023-08-22 00:25:00.570055: predicting ISIC_0014908
2023-08-22 00:25:00.735611: predicting ISIC_0014911
2023-08-22 00:25:00.902144: predicting ISIC_0014913
2023-08-22 00:25:01.070668: predicting ISIC_0014915
2023-08-22 00:25:01.236870: predicting ISIC_0014919
2023-08-22 00:25:01.405671: predicting ISIC_0014920
2023-08-22 00:25:01.573336: predicting ISIC_0014922
2023-08-22 00:25:01.739123: predicting ISIC_0014923
2023-08-22 00:25:01.905487: predicting ISIC_0014925
2023-08-22 00:25:02.077759: predicting ISIC_0014926
2023-08-22 00:25:02.252165: predicting ISIC_0014929
2023-08-22 00:25:02.421877: predicting ISIC_0014930
2023-08-22 00:25:02.588823: predicting ISIC_0014931
2023-08-22 00:25:02.756032: predicting ISIC_0014933
2023-08-22 00:25:02.922536: predicting ISIC_0014937
2023-08-22 00:25:03.092285: predicting ISIC_0014945
2023-08-22 00:25:03.260081: predicting ISIC_0014946
2023-08-22 00:25:03.428488: predicting ISIC_0014951
2023-08-22 00:25:03.595149: predicting ISIC_0014975
2023-08-22 00:25:03.764977: predicting ISIC_0014979
2023-08-22 00:25:03.932800: predicting ISIC_0014983
2023-08-22 00:25:04.103215: predicting ISIC_0014985
2023-08-22 00:25:04.275305: predicting ISIC_0014987
2023-08-22 00:25:04.442439: predicting ISIC_0014989
2023-08-22 00:25:04.610427: predicting ISIC_0015005
2023-08-22 00:25:04.778769: predicting ISIC_0015006
2023-08-22 00:25:04.950863: predicting ISIC_0015032
2023-08-22 00:25:05.118270: predicting ISIC_0015038
2023-08-22 00:25:05.286494: predicting ISIC_0015043
2023-08-22 00:25:05.452760: predicting ISIC_0015044
2023-08-22 00:25:05.619938: predicting ISIC_0015045
2023-08-22 00:25:05.786771: predicting ISIC_0015062
2023-08-22 00:25:05.955147: predicting ISIC_0015079
2023-08-22 00:25:06.126999: predicting ISIC_0015082
2023-08-22 00:25:06.299076: predicting ISIC_0015108
2023-08-22 00:25:06.468554: predicting ISIC_0015109
2023-08-22 00:25:06.638416: predicting ISIC_0015110
2023-08-22 00:25:06.808765: predicting ISIC_0015112
2023-08-22 00:25:06.974298: predicting ISIC_0015113
2023-08-22 00:25:07.144309: predicting ISIC_0015124
2023-08-22 00:25:07.312193: predicting ISIC_0015144
2023-08-22 00:25:07.480782: predicting ISIC_0015153
2023-08-22 00:25:07.655223: predicting ISIC_0015158
2023-08-22 00:25:07.826461: predicting ISIC_0015166
2023-08-22 00:25:07.992935: predicting ISIC_0015168
2023-08-22 00:25:08.164008: predicting ISIC_0015170
2023-08-22 00:25:08.336977: predicting ISIC_0015181
2023-08-22 00:25:08.505723: predicting ISIC_0015182
2023-08-22 00:25:08.676286: predicting ISIC_0015189
2023-08-22 00:25:08.845842: predicting ISIC_0015190
2023-08-22 00:25:09.014372: predicting ISIC_0015200
2023-08-22 00:25:09.180808: predicting ISIC_0015204
2023-08-22 00:25:09.349355: predicting ISIC_0015211
2023-08-22 00:25:09.521541: predicting ISIC_0015219
2023-08-22 00:25:09.690946: predicting ISIC_0015220
2023-08-22 00:25:09.859745: predicting ISIC_0015233
2023-08-22 00:25:10.027444: predicting ISIC_0015243
2023-08-22 00:25:10.198898: predicting ISIC_0015256
2023-08-22 00:25:10.369559: predicting ISIC_0015260
2023-08-22 00:25:10.545792: predicting ISIC_0015284
2023-08-22 00:25:10.715678: predicting ISIC_0015295
2023-08-22 00:25:10.885079: predicting ISIC_0015313
2023-08-22 00:25:11.053309: predicting ISIC_0015372
2023-08-22 00:25:11.220685: predicting ISIC_0015401
2023-08-22 00:25:11.387212: predicting ISIC_0015443
2023-08-22 00:25:11.556908: predicting ISIC_0015445
2023-08-22 00:25:11.727774: predicting ISIC_0015483
2023-08-22 00:25:11.897542: predicting ISIC_0015496
2023-08-22 00:25:12.064007: predicting ISIC_0015627
2023-08-22 00:25:19.721725: Validation complete
2023-08-22 00:25:19.725427: Mean Validation Dice:  0.870262679047011
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: / 0.602 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: - 0.602 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: \ 0.602 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: | 0.602 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: / 0.602 MB of 0.602 MB uploaded (0.000 MB deduped)wandb: - 0.602 MB of 0.602 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb: loss/train_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   loss/val_loss ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñá
wandb:        test/acc ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test/dsc ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:       test/miou ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:        test/sen ‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ
wandb:        test/spe ‚ñÜ‚ñá‚ñÜ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÅ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÇ‚ñà‚ñá‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb: loss/train_loss -0.94387
wandb:   loss/val_loss -0.65848
wandb:        test/acc 0.96191
wandb:        test/dsc 0.88258
wandb:       test/miou 0.78984
wandb:        test/sen 0.85478
wandb:        test/spe 0.98346
wandb: 
wandb: Synced nnunet_808641_0_lr_0.005: https://wandb.ai/ianben/isic2017_2_0/runs/ff348brl
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /tmp/808641.1.gpu/wandb/run-20230819_162054-ff348brl/logs
